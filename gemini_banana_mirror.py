"""
Gemini Banana é•œåƒç«™èŠ‚ç‚¹
æ”¯æŒè‡ªå®šä¹‰APIåœ°å€ï¼Œé€‚é…å›½å†…é•œåƒç«™å’Œä»£ç†æœåŠ¡
"""

import torch
import numpy as np
from PIL import Image
import io
import base64
import requests
import json
import time
import random
from typing import Optional, Tuple, Dict, Any
import re
import os
import urllib3

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
try:
    from server import PromptServer
except ImportError:
    # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼ŒPromptServerå¯èƒ½ä¸å¯ç”¨
    class PromptServer:
        @staticmethod
        def instance():
            return None
import sys
from io import BytesIO
from PIL import Image, ImageFilter

# ğŸš€ AIæ”¾å¤§æ¨¡å‹é›†æˆ
def detect_available_upscale_models():
    """
    è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„AIæ”¾å¤§æ¨¡å‹
    """
    available_models = []
    
    # æ£€æµ‹ Real-ESRGAN
    try:
        import realesrgan
        available_models.append("Real-ESRGAN")
        print(f"âœ… æ£€æµ‹åˆ° Real-ESRGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ Real-ESRGAN æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ ESRGAN
    try:
        import esrgan
        available_models.append("ESRGAN")
        print(f"âœ… æ£€æµ‹åˆ° ESRGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ ESRGAN æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ Waifu2x
    try:
        import waifu2x
        available_models.append("Waifu2x")
        print(f"âœ… æ£€æµ‹åˆ° Waifu2x æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ Waifu2x æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ GFPGAN
    try:
        import gfpgan
        available_models.append("GFPGAN")
        print(f"âœ… æ£€æµ‹åˆ° GFPGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ GFPGAN æ¨¡å‹æœªå®‰è£…")
    
    print(f"ğŸ” å¯ç”¨AIæ”¾å¤§æ¨¡å‹: {available_models}")
    return available_models

def ai_upscale_with_realesrgan(image, target_width, target_height):
    """
    ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰ï¼Œä¼˜å…ˆ2xï¼Œå¤±è´¥å›é€€LANCZOSã€‚
    """
    try:
        try:
            from .banana_upscale import smart_upscale as _smart
        except ImportError:
            from banana_upscale import smart_upscale as _smart
        res = _smart(image, target_width, target_height)
        if res is not None:
            return res
        print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
        return image.resize((target_width, target_height), Image.Resampling.LANCZOS)
    except Exception as e:
        print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥ï¼Œå›é€€é‡é‡‡æ ·: {e}")
        return image.resize((target_width, target_height), Image.Resampling.LANCZOS)
def ai_upscale_with_esrgan(image, target_width, target_height):
    """
    ä½¿ç”¨ ESRGAN è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§
    """
    try:
        print(f"ğŸš€ ä½¿ç”¨ ESRGAN è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§...")
        
        # ESRGAN å®ç°ä»£ç 
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ESRGANå®ç°æ¥ç¼–å†™
        
        print(f"âœ… ESRGAN AIæ”¾å¤§å®Œæˆ")
        return image  # ä¸´æ—¶è¿”å›åŸå›¾
        
    except Exception as e:
        print(f"âŒ ESRGAN æ”¾å¤§å¤±è´¥: {e}")
        raise e

def ai_upscale_with_waifu2x(image, target_width, target_height):
    """
    ä½¿ç”¨ Waifu2x è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§
    """
    try:
        print(f"ğŸš€ ä½¿ç”¨ Waifu2x è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§...")
        
        # Waifu2x å®ç°ä»£ç 
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„Waifu2xå®ç°æ¥ç¼–å†™
        
        print(f"âœ… Waifu2x AIæ”¾å¤§å®Œæˆ")
        return image  # ä¸´æ—¶è¿”å›åŸå›¾
        
    except Exception as e:
        print(f"âŒ Waifu2x æ”¾å¤§å¤±è´¥: {e}")
        raise e

def smart_ai_upscale(image, target_width, target_height):
	"""
	ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰
	"""
	try:
		try:
			from .banana_upscale import smart_upscale as _smart
		except ImportError:
			from banana_upscale import smart_upscale as _smart
		return _smart(image, target_width, target_height)
	except Exception as e:
		print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥: {e}")
		return None


def detect_image_foreground_subject(image):
    """
    ğŸ¯ æ™ºèƒ½æ£€æµ‹å›¾åƒå‰æ™¯ä¸»ä½“ï¼ˆäººç‰©ã€ç‰©ä½“ç­‰ï¼‰
    è¿”å›ä¸»ä½“è¾¹ç•Œæ¡† (x, y, width, height) å’Œä¸­å¿ƒç‚¹
    """
    try:
        import cv2
        import numpy as np

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if hasattr(image, 'convert'):
            if image.mode != 'RGB':
                image = image.convert('RGB')
            img_array = np.array(image)
        else:
            img_array = image

        # è½¬æ¢ä¸ºBGRæ ¼å¼ï¼ˆOpenCVä½¿ç”¨BGRï¼‰
        if len(img_array.shape) == 3:
            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        else:
            img_bgr = img_array

        height, width = img_bgr.shape[:2]
        print(f"ğŸ” å›¾åƒå°ºå¯¸: {width}x{height}")

        # ğŸ¯ æ–¹æ³•1ï¼šå…¨å›¾äººè„¸æ£€æµ‹ï¼ˆæœ€ä¼˜å…ˆï¼‰
        print(f"ğŸ” [DEBUG] å¼€å§‹å°è¯•äººè„¸æ£€æµ‹æ–¹æ³•...")
        try:
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)

            if len(faces) > 0:
                # æ‰¾åˆ°æœ€å¤§çš„äººè„¸
                largest_face = max(faces, key=lambda f: f[2] * f[3])
                x, y, w, h = largest_face

                # ğŸš€ å…³é”®ä¿®å¤ï¼šä»¥äººè„¸ä¸ºä¸­å¿ƒï¼Œæ‰©å±•åˆ°åˆç†çš„äººç‰©åŒºåŸŸ
                # æ ¹æ®äººè„¸å¤§å°åŠ¨æ€è°ƒæ•´æ‰©å±•ç³»æ•°
                face_ratio = (w * h) / (width * height)
                print(f"ğŸ” äººè„¸å æ¯”: {face_ratio:.1%}, äººè„¸å°ºå¯¸: {w}x{h}")

                if face_ratio > 0.05:  # å¦‚æœäººè„¸è¶…è¿‡5%ï¼Œä½¿ç”¨æå°æ‰©å±•
                    estimated_person_height = h * 2.5  # æå°æ‰©å±•ï¼š2.5å€ï¼ˆæ˜¾ç¤ºä¸ŠåŠèº«ï¼‰
                    estimated_person_width = w * 1.8   # æå°æ‰©å±•ï¼š1.8å€
                    print(f"ğŸ” æ£€æµ‹åˆ°å¤§äººè„¸ï¼Œä½¿ç”¨æå°æ‰©å±•ç³»æ•°")
                else:
                    estimated_person_height = h * 3.0  # å°æ‰©å±•ï¼š3.0å€ï¼ˆæ˜¾ç¤ºåŠèº«å¤šä¸€ç‚¹ï¼‰
                    estimated_person_width = w * 2.0   # å°æ‰©å±•ï¼š2.0å€

                # è®¡ç®—äººç‰©åŒºåŸŸï¼ˆä»¥äººè„¸ä¸­å¿ƒä¸ºåŸºå‡†ï¼‰
                face_center_x = x + w // 2
                face_center_y = y + h // 2

                # äººç‰©åŒºåŸŸçš„å·¦ä¸Šè§’ï¼ˆäººè„¸é€šå¸¸åœ¨äººç‰©ä¸Šéƒ¨1/6å¤„ï¼Œæ˜¾ç¤ºå…¨èº«ï¼‰
                person_x = max(0, face_center_x - estimated_person_width // 2)
                person_y = max(0, face_center_y - estimated_person_height // 6)

                # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
                person_w = min(estimated_person_width, width - person_x)
                person_h = min(estimated_person_height, height - person_y)

                # ğŸš€ æ–°å¢ï¼šä¸»ä½“å°ºå¯¸éªŒè¯ï¼Œç¡®ä¿åˆç†èŒƒå›´
                person_ratio = (person_w * person_h) / (width * height)
                if person_ratio > 0.15:  # è¶…è¿‡15%å°±è°ƒæ•´ï¼ˆé€‚ä¸­æ§åˆ¶ï¼‰
                    print(f"âš ï¸ äººè„¸æ£€æµ‹ä¸»ä½“è¿‡å¤§(å æ¯”{person_ratio:.1%})ï¼Œè°ƒæ•´å°ºå¯¸...")
                    scale_factor = (0.12 / person_ratio) ** 0.5  # è°ƒæ•´åˆ°12%
                    person_w = int(person_w * scale_factor)
                    person_h = int(person_h * scale_factor)
                    # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒä¸­å¿ƒä¸å˜
                    person_x = max(0, face_center_x - person_w // 2)
                    person_y = max(0, face_center_y - person_h // 6)  # ä¿®æ­£Yè½´ä½ç½®
                elif person_ratio < 0.03:  # å¦‚æœä¸»ä½“å¤ªå°ï¼ˆ<3%ï¼‰ï¼Œè¿›è¡Œæ”¾å¤§
                    print(f"âš ï¸ äººè„¸æ£€æµ‹ä¸»ä½“è¿‡å°(å æ¯”{person_ratio:.1%})ï¼Œæ”¾å¤§å°ºå¯¸...")
                    scale_factor = (0.08 / person_ratio) ** 0.5  # æ”¾å¤§åˆ°8%
                    person_w = int(person_w * scale_factor)
                    person_h = int(person_h * scale_factor)
                    # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒä¸­å¿ƒä¸å˜
                    person_x = max(0, face_center_x - person_w // 2)
                    person_y = max(0, face_center_y - person_h // 6)  # ä¿®æ­£Yè½´ä½ç½®
                    # ç¡®ä¿ä¸è¶…å‡ºè¾¹ç•Œ
                    person_x = min(person_x, width - person_w)
                    person_y = min(person_y, height - person_h)

                subject_center_x = person_x + person_w // 2
                subject_center_y = person_y + person_h // 2

                print(f"ğŸ¯ äººè„¸æ£€æµ‹è¯†åˆ«åˆ°ä¸»ä½“: äººè„¸({x}, {y}, {w}x{h}), äººç‰©åŒºåŸŸ({person_x}, {person_y}, {person_w}x{person_h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                print(f"ğŸ” ä¸»ä½“å æ¯”: {(person_w * person_h) / (width * height):.1%}")
                return (person_x, person_y, person_w, person_h), (subject_center_x, subject_center_y)
        except Exception as e:
            print(f"âš ï¸ äººè„¸æ£€æµ‹å¤±è´¥: {e}")

        # ğŸ¯ æ–¹æ³•2ï¼šå…¨å›¾è‚¤è‰²æ£€æµ‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        print(f"ğŸ” [DEBUG] å¼€å§‹å°è¯•è‚¤è‰²æ£€æµ‹æ–¹æ³•...")
        try:
            hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)

            # æ‰©å±•è‚¤è‰²èŒƒå›´ï¼ŒåŒ…å«æ›´å¤šè‚¤è‰²ç±»å‹
            lower_skin1 = np.array([0, 20, 70])    # åçº¢è‚¤è‰²
            upper_skin1 = np.array([20, 255, 255])

            lower_skin2 = np.array([0, 10, 60])    # è¾ƒæµ…è‚¤è‰²
            upper_skin2 = np.array([25, 255, 255])

            # åˆ›å»ºè‚¤è‰²æ©ç 
            skin_mask1 = cv2.inRange(hsv, lower_skin1, upper_skin1)
            skin_mask2 = cv2.inRange(hsv, lower_skin2, upper_skin2)
            skin_mask = cv2.bitwise_or(skin_mask1, skin_mask2)

            # å½¢æ€å­¦æ“ä½œ
            kernel = np.ones((7,7), np.uint8)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)

            # æ‰¾åˆ°è‚¤è‰²åŒºåŸŸ
            contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            if contours:
                # è¿‡æ»¤å¤ªå°çš„åŒºåŸŸ
                min_area = (width * height) * 0.005  # æœ€å°é¢ç§¯é˜ˆå€¼
                valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]

                if valid_contours:
                    # æ‰¾åˆ°æœ€å¤§çš„è‚¤è‰²åŒºåŸŸ
                    largest_contour = max(valid_contours, key=cv2.contourArea)
                    x, y, w, h = cv2.boundingRect(largest_contour)

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šæä¿å®ˆæ‰©å±•è¾¹ç•Œæ¡†ï¼Œé¿å…ä¸»ä½“è¿‡å¤§
                    # è‚¤è‰²åŒºåŸŸé€šå¸¸åªæ˜¯äººç‰©çš„ä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨æœ€å°æ‰©å±•ç³»æ•°
                    expand_factor_w = 1.1  # è¿›ä¸€æ­¥å‡å°‘å®½åº¦æ‰©å±•ç³»æ•°
                    expand_factor_h = 1.2  # è¿›ä¸€æ­¥å‡å°‘é«˜åº¦æ‰©å±•ç³»æ•°

                    expanded_w = int(w * expand_factor_w)
                    expanded_h = int(h * expand_factor_h)

                    # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒä¸­å¿ƒä¸å˜
                    expanded_x = max(0, x - (expanded_w - w) // 2)
                    expanded_y = max(0, y - (expanded_h - h) // 4)  # å‘ä¸Šæ‰©å±•æ›´å¤š

                    # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
                    expanded_w = min(expanded_w, width - expanded_x)
                    expanded_h = min(expanded_h, height - expanded_y)

                    # ğŸš€ æ–°å¢ï¼šä¸»ä½“å°ºå¯¸éªŒè¯ï¼Œç¡®ä¿ä¸ä¼šè¿‡å¤§
                    expanded_ratio = (expanded_w * expanded_h) / (width * height)
                    if expanded_ratio > 0.35:  # å¦‚æœä¸»ä½“è¶…è¿‡35%ï¼Œè¿›è¡Œè°ƒæ•´
                        print(f"âš ï¸ è‚¤è‰²æ£€æµ‹ä¸»ä½“è¿‡å¤§(å æ¯”{expanded_ratio:.1%})ï¼Œè°ƒæ•´å°ºå¯¸...")
                        scale_factor = (0.35 / expanded_ratio) ** 0.5
                        expanded_w = int(expanded_w * scale_factor)
                        expanded_h = int(expanded_h * scale_factor)
                        # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒä¸­å¿ƒä¸å˜
                        expanded_x = max(0, x + w//2 - expanded_w // 2)
                        expanded_y = max(0, y + h//2 - expanded_h // 2)

                    subject_center_x = expanded_x + expanded_w // 2
                    subject_center_y = expanded_y + expanded_h // 2

                    print(f"ğŸ¯ è‚¤è‰²æ£€æµ‹è¯†åˆ«åˆ°ä¸»ä½“: åŸå§‹({x}, {y}, {w}x{h}), æ‰©å±•å({expanded_x}, {expanded_y}, {expanded_w}x{expanded_h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                    print(f"ğŸ” ä¸»ä½“å æ¯”: {(expanded_w * expanded_h) / (width * height):.1%}")
                    return (expanded_x, expanded_y, expanded_w, expanded_h), (subject_center_x, subject_center_y)

        except Exception as e:
            print(f"âš ï¸ è‚¤è‰²æ£€æµ‹å¤±è´¥: {e}")
        
        # ğŸ¯ æ–¹æ³•3ï¼šæ”¹è¿›çš„è¾¹ç¼˜æ£€æµ‹ï¼ˆä¸“æ³¨äºäººç‰©è½®å»“ï¼‰
        print(f"ğŸ” [DEBUG] å¼€å§‹å°è¯•è¾¹ç¼˜æ£€æµ‹æ–¹æ³•...")
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

            # é«˜æ–¯æ¨¡ç³Šå‡å°‘å™ªå£°
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)

            # ä½¿ç”¨æ›´ä¿å®ˆçš„è¾¹ç¼˜æ£€æµ‹å‚æ•°
            edges = cv2.Canny(blurred, 30, 100)

            # å½¢æ€å­¦æ“ä½œè¿æ¥è¾¹ç¼˜
            kernel = np.ones((5,5), np.uint8)
            edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)

            # æ‰¾åˆ°è½®å»“
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            if contours:
                # è¿‡æ»¤å°è½®å»“ï¼Œæ‰¾åˆ°ä¸»è¦ç‰©ä½“
                min_area = (width * height) * 0.02  # æé«˜æœ€å°é¢ç§¯é˜ˆå€¼
                valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]

                if valid_contours:
                    # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
                    largest_contour = max(valid_contours, key=cv2.contourArea)
                    x, y, w, h = cv2.boundingRect(largest_contour)

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ£€æµ‹åˆ°çš„æ˜¯åˆç†çš„äººç‰©åŒºåŸŸ
                    aspect_ratio = w / h
                    if 0.3 <= aspect_ratio <= 2.0:  # äººç‰©çš„å®½é«˜æ¯”é€šå¸¸åœ¨è¿™ä¸ªèŒƒå›´å†…
                        # ğŸš€ æ–°å¢ï¼šä¸»ä½“å°ºå¯¸éªŒè¯ï¼Œç¡®ä¿ä¸ä¼šè¿‡å¤§
                        edge_ratio = (w * h) / (width * height)
                        if edge_ratio > 0.35:  # å¦‚æœä¸»ä½“è¶…è¿‡35%ï¼Œè¿›è¡Œè°ƒæ•´
                            print(f"âš ï¸ è¾¹ç¼˜æ£€æµ‹ä¸»ä½“è¿‡å¤§(å æ¯”{edge_ratio:.1%})ï¼Œè°ƒæ•´å°ºå¯¸...")
                            scale_factor = (0.35 / edge_ratio) ** 0.5
                            new_w = int(w * scale_factor)
                            new_h = int(h * scale_factor)
                            # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒä¸­å¿ƒä¸å˜
                            x = max(0, x + w//2 - new_w // 2)
                            y = max(0, y + h//2 - new_h // 2)
                            w, h = new_w, new_h

                        subject_center_x = x + w // 2
                        subject_center_y = y + h // 2

                        print(f"ğŸ¯ è¾¹ç¼˜æ£€æµ‹è¯†åˆ«åˆ°å‰æ™¯ä¸»ä½“: ä½ç½®({x}, {y}), å°ºå¯¸({w}x{h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                        print(f"ğŸ” ä¸»ä½“å æ¯”: {(w * h) / (width * height):.1%}")
                        return (x, y, w, h), (subject_center_x, subject_center_y)

        except Exception as e:
            print(f"âš ï¸ è¾¹ç¼˜æ£€æµ‹ç®—æ³•å¤±è´¥: {e}")

        # ğŸ¯ æ–¹æ³•4ï¼šå®‰å…¨çš„ä¿å®ˆç­–ç•¥ï¼ˆç¡®ä¿ä¸»ä½“ä¸ä¸¢å¤±ï¼‰
        print(f"ğŸ” [DEBUG] å¼€å§‹å°è¯•æ–¹å·®åˆ†ææ–¹æ³•...")
        try:
            print(f"ğŸ” ä½¿ç”¨å®‰å…¨ä¿å®ˆç­–ç•¥...")

            # ğŸš€ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„ä¸»ä½“ä½ç½®ä¼°è®¡
            # åŸºäºå›¾åƒçš„é»„é‡‘åˆ†å‰²ç‚¹å’Œå¸¸è§äººç‰©ä½ç½®

            # åˆ†æå›¾åƒçš„äº®åº¦åˆ†å¸ƒï¼Œæ‰¾åˆ°å¯èƒ½çš„ä¸»ä½“åŒºåŸŸ
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

            # å°†å›¾åƒåˆ†ä¸º9ä¸ªåŒºåŸŸï¼ˆ3x3ç½‘æ ¼ï¼‰
            h_step = height // 3
            w_step = width // 3

            max_variance = 0
            best_region = None

            for i in range(3):
                for j in range(3):
                    region_y = i * h_step
                    region_x = j * w_step
                    region = gray[region_y:region_y+h_step, region_x:region_x+w_step]

                    # è®¡ç®—åŒºåŸŸçš„æ–¹å·®ï¼ˆé«˜æ–¹å·®é€šå¸¸è¡¨ç¤ºæœ‰æ›´å¤šç»†èŠ‚ï¼Œå¯èƒ½æ˜¯ä¸»ä½“ï¼‰
                    variance = np.var(region)
                    if variance > max_variance:
                        max_variance = variance
                        best_region = (region_x, region_y, w_step, h_step)

            if best_region:
                x, y, w, h = best_region

                # ğŸš€ å…³é”®ä¿®å¤ï¼šæ–¹å·®åˆ†æè¿”å›çš„æ˜¯1/3å°ºå¯¸ï¼Œéœ€è¦è°ƒæ•´ä¸ºæ›´åˆç†çš„ä¸»ä½“å°ºå¯¸
                # åŸå§‹çš„w_stepå’Œh_stepæ˜¯width//3å’Œheight//3ï¼Œå¤ªå¤§äº†
                if width > height:
                    # æ¨ªå‘å›¾åƒï¼šä½¿ç”¨æ›´å°çš„ä¸»ä½“å°ºå¯¸
                    w = int(width * 0.2)   # 20%å®½åº¦
                    h = int(height * 0.3)  # 30%é«˜åº¦
                else:
                    # çºµå‘å›¾åƒï¼šä½¿ç”¨æ›´å°çš„ä¸»ä½“å°ºå¯¸
                    w = int(width * 0.3)   # 30%å®½åº¦
                    h = int(height * 0.2)  # 20%é«˜åº¦

                # ç¡®ä¿ä¸»ä½“ä¸ä¼šå¤ªå°
                w = max(w, width // 10)
                h = max(h, height // 10)

                # ç¡®ä¿ä¸»ä½“ä¸ä¼šå¤ªå¤§ï¼ˆä¸è¶…è¿‡35%ï¼‰
                w = min(w, int(width * 0.35))
                h = min(h, int(height * 0.35))

                # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒåœ¨æœ€ä½³æ–¹å·®åŒºåŸŸçš„ä¸­å¿ƒ
                original_center_x = x + best_region[2] // 2
                original_center_y = y + best_region[3] // 2

                x = max(0, original_center_x - w // 2)
                y = max(0, original_center_y - h // 2)

                # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
                x = min(x, width - w)
                y = min(y, height - h)

                subject_center_x = x + w // 2
                subject_center_y = y + h // 2

                print(f"ğŸ¯ æ–¹å·®åˆ†æè¯†åˆ«åˆ°ä¸»ä½“åŒºåŸŸ: ä½ç½®({x}, {y}), å°ºå¯¸({w}x{h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                print(f"ğŸ” ä¸»ä½“å æ¯”: {(w * h) / (width * height):.1%}")
                return (x, y, w, h), (subject_center_x, subject_center_y)

        except Exception as e:
            print(f"âš ï¸ æ–¹å·®åˆ†æå¤±è´¥: {e}")

        # ğŸ¯ æœ€ç»ˆå®‰å…¨ç­–ç•¥ï¼šåŸºäºå›¾åƒä¸­å¿ƒçš„ä¿å®ˆä¼°è®¡
        print(f"ğŸ” [DEBUG] ä½¿ç”¨æœ€ç»ˆå®‰å…¨ç­–ç•¥ï¼šå›¾åƒä¸­å¿ƒåŒºåŸŸ...")
        print(f"ğŸ” ä½¿ç”¨æœ€ç»ˆå®‰å…¨ç­–ç•¥ï¼šå›¾åƒä¸­å¿ƒåŒºåŸŸ...")

        # ğŸš€ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æä¿å®ˆçš„ä¸»ä½“å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å®Œæ•´æ˜¾ç¤º
        # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸»ä½“ä¸ä¼šå®Œå…¨ä¸¢å¤±ï¼ŒåŒæ—¶é¿å…ä¸»ä½“è¿‡å¤§
        if width > height:
            safe_w = int(width * 0.2)   # æ¨ªå‘å›¾åƒï¼š20%å®½åº¦
            safe_h = int(height * 0.3)  # 30%é«˜åº¦
        else:
            safe_w = int(width * 0.3)   # çºµå‘å›¾åƒï¼š30%å®½åº¦
            safe_h = int(height * 0.2)  # 20%é«˜åº¦

        # ç¡®ä¿ä¸»ä½“ä¸ä¼šå¤ªå°
        safe_w = max(safe_w, width // 10)
        safe_h = max(safe_h, height // 10)

        # ç¡®ä¿ä¸»ä½“ä¸ä¼šå¤ªå¤§ï¼ˆä¸è¶…è¿‡35%ï¼‰
        safe_w = min(safe_w, int(width * 0.35))
        safe_h = min(safe_h, int(height * 0.35))

        safe_x = (width - safe_w) // 2   # å±…ä¸­
        safe_y = (height - safe_h) // 2  # å±…ä¸­

        safe_center_x = safe_x + safe_w // 2  # å›¾åƒä¸­å¿ƒ
        safe_center_y = safe_y + safe_h // 2  # å›¾åƒä¸­å¿ƒ

        print(f"ğŸ¯ å®‰å…¨ç­–ç•¥ä¸»ä½“ä½ç½®: ({safe_x}, {safe_y}), å°ºå¯¸({safe_w}x{safe_h}), ä¸­å¿ƒ({safe_center_x}, {safe_center_y})")
        return (safe_x, safe_y, safe_w, safe_h), (safe_center_x, safe_center_y)
        
    except ImportError:
        print("âš ï¸ OpenCVæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½ä¸»ä½“æ£€æµ‹")
        # ğŸš€ å…³é”®ä¿®å¤ï¼šè¿”å›å®‰å…¨çš„å›¾åƒä¸­å¿ƒä½ç½®
        width, height = image.size
        safe_center_x = width // 2   # å›¾åƒä¸­å¿ƒ
        safe_center_y = height // 2  # å›¾åƒä¸­å¿ƒ

        # ğŸš€ ä¿®å¤ï¼šä½¿ç”¨æä¿å®ˆçš„ä¸»ä½“å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å®Œæ•´æ˜¾ç¤º
        if width > height:
            safe_w = int(width * 0.2)
            safe_h = int(height * 0.3)
        else:
            safe_w = int(width * 0.3)
            safe_h = int(height * 0.2)

        safe_w = max(safe_w, width // 10)
        safe_h = max(safe_h, height // 10)
        safe_w = min(safe_w, int(width * 0.35))
        safe_h = min(safe_h, int(height * 0.35))

        safe_x = safe_center_x - safe_w // 2
        safe_y = safe_center_y - safe_h // 2

        print(f"ğŸ¯ é»˜è®¤å®‰å…¨ä½ç½®: ({safe_x}, {safe_y}), å°ºå¯¸({safe_w}x{safe_h}), ä¸­å¿ƒ({safe_center_x}, {safe_center_y})")
        print(f"ğŸ” ä¸»ä½“å æ¯”: å®½åº¦{safe_w/width:.1%}, é«˜åº¦{safe_h/height:.1%}")
        return (safe_x, safe_y, safe_w, safe_h), (safe_center_x, safe_center_y)
    except Exception as e:
        print(f"âŒ æ™ºèƒ½ä¸»ä½“æ£€æµ‹å¤±è´¥: {e}")
        # ğŸš€ å…³é”®ä¿®å¤ï¼šè¿”å›å®‰å…¨çš„å›¾åƒä¸­å¿ƒä½ç½®
        width, height = image.size
        safe_center_x = width // 2   # å›¾åƒä¸­å¿ƒ
        safe_center_y = height // 2  # å›¾åƒä¸­å¿ƒ

        # ğŸš€ ä¿®å¤ï¼šä½¿ç”¨æä¿å®ˆçš„ä¸»ä½“å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å®Œæ•´æ˜¾ç¤º
        if width > height:
            safe_w = int(width * 0.2)
            safe_h = int(height * 0.3)
        else:
            safe_w = int(width * 0.3)
            safe_h = int(height * 0.2)

        safe_w = max(safe_w, width // 10)
        safe_h = max(safe_h, height // 10)
        safe_w = min(safe_w, int(width * 0.35))
        safe_h = min(safe_h, int(height * 0.35))

        safe_x = safe_center_x - safe_w // 2
        safe_y = safe_center_y - safe_h // 2

        print(f"ğŸ¯ å¼‚å¸¸å¤„ç†å®‰å…¨ä½ç½®: ({safe_x}, {safe_y}), å°ºå¯¸({safe_w}x{safe_h}), ä¸­å¿ƒ({safe_center_x}, {safe_center_y})")
        print(f"ğŸ” ä¸»ä½“å æ¯”: å®½åº¦{safe_w/width:.1%}, é«˜åº¦{safe_h/height:.1%}")
        return (safe_x, safe_y, safe_w, safe_h), (safe_center_x, safe_center_y)

def _log_info(message):
    try:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] INFO: {repr(message)}")

def _log_warning(message):
    try:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] WARNING: {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] WARNING: {repr(message)}")

def _log_error(message):
    try:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] ERROR: {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] ERROR: {repr(message)}")

def tensor_to_pil(tensor):
    """Convert tensor to PIL Image"""
    if len(tensor.shape) == 4:
        tensor = tensor.squeeze(0)
    if tensor.shape[0] == 3:
        tensor = tensor.permute(1, 2, 0)
    tensor = (tensor * 255).clamp(0, 255).byte()
    return Image.fromarray(tensor.cpu().numpy())

def pil_to_tensor(image):
    """Convert PIL Image to tensor"""
    if image.mode != 'RGB':
        image = image.convert('RGB')
    image_array = np.array(image).astype(np.float32) / 255.0
    tensor = torch.from_numpy(image_array).unsqueeze(0)
    return tensor

def enhance_image_quality(image: Image.Image, quality: str = "hd", adaptive_mode: str = "disabled") -> Image.Image:
    """
    ğŸš€ å…¨é¢AIç”»è´¨å¢å¼ºç³»ç»Ÿ
    æ™ºèƒ½è¯†åˆ«å›¾åƒç±»å‹å¹¶åº”ç”¨æœ€é€‚åˆçš„å¢å¼ºæŠ€æœ¯
    """
    print(f"ğŸ¨ å¼€å§‹å…¨é¢AIç”»è´¨å¢å¼ºï¼Œè´¨é‡çº§åˆ«: {quality}")

    # ä¿å­˜åŸå›¾ç”¨äºè¿‡åº¦å¢å¼ºæ£€æµ‹
    original_image = image.copy()

    # ğŸš€ ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½å›¾åƒç±»å‹è¯†åˆ«
    image_type = _analyze_image_type(image)
    print(f"ğŸ” å›¾åƒç±»å‹è¯†åˆ«: {image_type}")

    # ğŸš€ ç¬¬äºŒæ­¥ï¼šæ ¹æ®å›¾åƒç±»å‹é€‰æ‹©æœ€ä½³å¢å¼ºç­–ç•¥
    if image_type == "portrait":
        # äººåƒå›¾åƒï¼šä¼˜å…ˆäººè„¸ä¿®å¤
        enhanced_image = _apply_ai_face_restoration(image)
        if enhanced_image:
            print(f"âœ… AIäººè„¸ä¿®å¤å®Œæˆ")
            image = enhanced_image

    elif image_type == "landscape":
        # é£æ™¯å›¾åƒï¼šä½¿ç”¨é£æ™¯ä¸“ç”¨å¢å¼º
        enhanced_image = _apply_landscape_enhancement(image, quality)
        if enhanced_image:
            print(f"âœ… é£æ™¯ä¸“ç”¨å¢å¼ºå®Œæˆ")
            image = enhanced_image

    elif image_type == "architecture":
        # å»ºç­‘å›¾åƒï¼šä½¿ç”¨å»ºç­‘ä¸“ç”¨å¢å¼º
        enhanced_image = _apply_architecture_enhancement(image, quality)
        if enhanced_image:
            print(f"âœ… å»ºç­‘ä¸“ç”¨å¢å¼ºå®Œæˆ")
            image = enhanced_image

    elif image_type == "artwork":
        # è‰ºæœ¯ä½œå“ï¼šä½¿ç”¨è‰ºæœ¯ä¸“ç”¨å¢å¼º
        enhanced_image = _apply_artwork_enhancement(image, quality)
        if enhanced_image:
            print(f"âœ… è‰ºæœ¯ä¸“ç”¨å¢å¼ºå®Œæˆ")
            image = enhanced_image

    else:
        # é€šç”¨å›¾åƒï¼šä½¿ç”¨é€šç”¨AIå¢å¼º
        print(f"ğŸ” ä½¿ç”¨é€šç”¨AIå¢å¼ºç­–ç•¥")

    # ğŸš€ ç¬¬ä¸‰æ­¥ï¼šé€šç”¨AIè¶…åˆ†è¾¨ç‡å¢å¼ºï¼ˆé€‚ç”¨äºæ‰€æœ‰ç±»å‹ï¼‰
    if quality == "ultra_hd":
        sr_enhanced = _apply_ai_super_resolution(image, image_type)
        if sr_enhanced:
            print(f"âœ… AIè¶…åˆ†è¾¨ç‡å¢å¼ºå®Œæˆ")
            image = sr_enhanced
        else:
            print(f"ğŸ” AIè¶…åˆ†è¾¨ç‡ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿå¢å¼º")

    # ğŸš€ ç¬¬å››æ­¥ï¼šè‡ªé€‚åº”ä¼ ç»Ÿå¢å¼ºï¼ˆæ ¹æ®å›¾åƒç±»å‹è°ƒæ•´å‚æ•°ï¼‰
    if quality in ["hd", "ultra_hd"]:
        image = _apply_adaptive_traditional_enhancement(image, quality, image_type)

    # ğŸš€ ç¬¬äº”æ­¥ï¼šæ™ºèƒ½è¿‡åº¦å¢å¼ºæ£€æµ‹å’Œä¿®æ­£
    final_image = _check_and_correct_over_enhancement(original_image, image)  # ä¼ å…¥åŸå›¾å’Œå¢å¼ºåçš„å›¾
    if final_image != image:
        print(f"ğŸ”§ æ£€æµ‹åˆ°è¿‡åº¦å¢å¼ºï¼Œå·²è‡ªåŠ¨ä¿®æ­£")
        image = final_image

    print(f"ğŸ¨ å…¨é¢ç”»è´¨å¢å¼ºæµç¨‹å®Œæˆ")
    return image


def _apply_ai_face_restoration(image: Image.Image) -> Optional[Image.Image]:
    """
    ğŸš€ AIäººè„¸ä¿®å¤ï¼šä¼˜å…ˆä¸“ä¸šæ¨¡å‹ï¼Œå›é€€åˆ°æ™ºèƒ½å¢å¼º
    """
    try:
        # é¦–å…ˆæ£€æµ‹æ˜¯å¦æœ‰äººè„¸
        faces = _detect_faces_with_locations(image)
        if not faces:
            return None

        print(f"ğŸ¯ æ£€æµ‹åˆ°{len(faces)}ä¸ªäººè„¸ï¼Œå¼€å§‹AIä¿®å¤...")

        # å°è¯•CodeFormerï¼ˆæœ€æ–°æŠ€æœ¯ï¼‰
        restored = _try_codeformer_restoration(image)
        if restored:
            print(f"âœ… CodeFormeräººè„¸ä¿®å¤æˆåŠŸ")
            return restored

        # å›é€€åˆ°GFPGAN
        restored = _try_gfpgan_restoration(image)
        if restored:
            print(f"âœ… GFPGANäººè„¸ä¿®å¤æˆåŠŸ")
            return restored

        # ğŸš€ å®ç”¨çš„å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼ºäººè„¸åŒºåŸŸ
        enhanced = _enhance_face_regions_with_ai_upscale(image, faces)
        if enhanced:
            print(f"âœ… AIæ”¾å¤§äººè„¸å¢å¼ºæˆåŠŸ")
            return enhanced

        print(f"âš ï¸ æ‰€æœ‰AIäººè„¸ä¿®å¤æ–¹æ³•ä¸å¯ç”¨")
        return None

    except Exception as e:
        print(f"âŒ AIäººè„¸ä¿®å¤å¤±è´¥: {e}")
        return None


def _apply_ai_super_resolution(image: Image.Image, image_type: str = "general") -> Optional[Image.Image]:
    """
    ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºï¼šæ ¹æ®å›¾åƒç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥
    """
    try:
        # ğŸš€ æ ¹æ®å›¾åƒç±»å‹è°ƒæ•´æ”¾å¤§ç­–ç•¥
        current_size = max(image.size)

        if image_type == "portrait":
            # äººåƒï¼šéå¸¸ä¿å®ˆï¼Œä¿æŒè‡ªç„¶
            target_scale = 1.15 if current_size < 1024 else 1.08
        elif image_type == "landscape":
            # é£æ™¯ï¼šæ¸©å’Œæ”¾å¤§ï¼Œä¿æŒçœŸå®æ„Ÿ
            target_scale = 1.25 if current_size < 1024 else 1.12
        elif image_type == "architecture":
            # å»ºç­‘ï¼šé€‚åº¦æ”¾å¤§ï¼Œä¿æŒæè´¨è´¨æ„Ÿ
            target_scale = 1.2 if current_size < 1024 else 1.1
        elif image_type == "artwork":
            # è‰ºæœ¯ä½œå“ï¼šæœ€å°æ”¾å¤§ï¼Œä¿æŒåŸæœ‰é£æ ¼
            target_scale = 1.1 if current_size < 1024 else 1.05
        else:
            # é€šç”¨ï¼šä¿å®ˆç­–ç•¥
            target_scale = 1.2 if current_size < 1024 else 1.1

        target_w = int(image.width * target_scale)
        target_h = int(image.height * target_scale)

        print(f"ğŸš€ å°è¯•AIè¶…åˆ†è¾¨ç‡å¢å¼º: {image.size} -> ({target_w}, {target_h})")

        # ä½¿ç”¨ç°æœ‰çš„æ™ºèƒ½æ”¾å¤§ç³»ç»Ÿ
        enhanced = smart_ai_upscale(image, target_w, target_h)
        if enhanced:
            # å¦‚æœæ”¾å¤§æˆåŠŸï¼Œç¼©å›åŸå°ºå¯¸ä»¥ä¿æŒç»†èŠ‚æå‡
            final = enhanced.resize(image.size, Image.Resampling.LANCZOS)
            print(f"âœ… AIè¶…åˆ†è¾¨ç‡å¢å¼ºæˆåŠŸ")
            return final

        return None

    except Exception as e:
        print(f"âŒ AIè¶…åˆ†è¾¨ç‡å¢å¼ºå¤±è´¥: {e}")
        return None


def _analyze_image_type(image: Image.Image) -> str:
    """
    ğŸš€ æ™ºèƒ½å›¾åƒç±»å‹è¯†åˆ«
    åˆ†æå›¾åƒå†…å®¹å¹¶è¿”å›æœ€é€‚åˆçš„å¢å¼ºç­–ç•¥ç±»å‹
    """
    try:
        import cv2
        import numpy as np

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        img_array = np.array(image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)

        # æ£€æµ‹äººè„¸
        faces = _detect_faces_with_locations(image)
        if faces:
            face_area = sum(w * h for x, y, w, h in faces)
            total_area = image.width * image.height
            face_ratio = face_area / total_area

            if face_ratio > 0.05:  # äººè„¸å æ¯”è¶…è¿‡5%
                return "portrait"

        # åˆ†æé¢œè‰²åˆ†å¸ƒå’Œçº¹ç†ç‰¹å¾
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)

        # æ£€æµ‹å¤©ç©ºï¼ˆè“è‰²åŒºåŸŸï¼‰
        blue_mask = cv2.inRange(hsv, np.array([100, 50, 50]), np.array([130, 255, 255]))
        blue_ratio = np.sum(blue_mask > 0) / (image.width * image.height)

        # æ£€æµ‹ç»¿è‰²æ¤è¢«
        green_mask = cv2.inRange(hsv, np.array([40, 40, 40]), np.array([80, 255, 255]))
        green_ratio = np.sum(green_mask > 0) / (image.width * image.height)

        # æ£€æµ‹å»ºç­‘ç‰¹å¾ï¼ˆç›´çº¿å’Œè¾¹ç¼˜ï¼‰
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)

        # æ£€æµ‹ç›´çº¿ï¼ˆå»ºç­‘ç‰¹å¾ï¼‰
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)
        line_count = len(lines) if lines is not None else 0

        # æ ¹æ®ç‰¹å¾åˆ¤æ–­å›¾åƒç±»å‹
        if blue_ratio > 0.15 and green_ratio > 0.2:
            return "landscape"  # é£æ™¯ï¼šæœ‰å¤©ç©ºå’Œæ¤è¢«
        elif line_count > 20:
            return "architecture"  # å»ºç­‘ï¼šæœ‰å¾ˆå¤šç›´çº¿
        elif blue_ratio < 0.05 and green_ratio < 0.1 and line_count < 10:
            # æ£€æµ‹è‰ºæœ¯ä½œå“ç‰¹å¾ï¼ˆè‰²å½©ä¸°å¯Œåº¦ï¼‰
            colors = img_array.reshape(-1, 3)
            unique_colors = len(np.unique(colors.view(np.dtype((np.void, colors.dtype.itemsize*colors.shape[1])))))
            color_diversity = unique_colors / (image.width * image.height)

            if color_diversity > 0.3:
                return "artwork"  # è‰ºæœ¯ä½œå“ï¼šè‰²å½©ä¸°å¯Œ

        return "general"  # é€šç”¨å›¾åƒ

    except Exception as e:
        print(f"âš ï¸ å›¾åƒç±»å‹è¯†åˆ«å¤±è´¥: {e}")
        return "general"


def _apply_landscape_enhancement(image: Image.Image, quality: str) -> Optional[Image.Image]:
    """
    ğŸš€ è‡ªç„¶é£æ™¯ä¸“ç”¨å¢å¼º - ä¿æŒçœŸå®æ„Ÿ
    """
    try:
        print(f"ğŸŒ„ åº”ç”¨è‡ªç„¶é£æ™¯å¢å¼º...")
        from PIL import ImageEnhance, ImageFilter
        import numpy as np

        enhanced = image.copy()

        # ğŸš€ è‡ªç„¶é£æ™¯å¢å¼ºç­–ç•¥ï¼ˆä¿æŒçœŸå®æ„Ÿï¼‰ï¼š
        # 1. è½»å¾®å¢å¼ºå¯¹æ¯”åº¦ï¼ˆé¿å…è¿‡åº¦HDRæ•ˆæœï¼‰
        # 2. æ¸©å’Œæå‡é¥±å’Œåº¦ï¼ˆä¿æŒè‡ªç„¶è‰²å½©ï¼‰
        # 3. ç»†èŠ‚å¢å¼ºä½†ä¸è¿‡åº¦é”åŒ–

        # è½»å¾®å¯¹æ¯”åº¦å¢å¼ºï¼ˆé¿å…å‡HDRæ•ˆæœï¼‰
        contrast_enhancer = ImageEnhance.Contrast(enhanced)
        enhanced = contrast_enhancer.enhance(1.06)  # é™ä½åˆ°æ›´è‡ªç„¶çš„æ°´å¹³

        # æ¸©å’Œé¥±å’Œåº¦å¢å¼ºï¼ˆä¿æŒè‡ªç„¶ï¼‰
        color_enhancer = ImageEnhance.Color(enhanced)
        enhanced = color_enhancer.enhance(1.04)  # æ›´æ¸©å’Œçš„é¥±å’Œåº¦

        # è‡ªç„¶çš„ç»†èŠ‚å¢å¼º
        if quality == "ultra_hd":
            # ä½¿ç”¨éå¸¸æ¸©å’Œçš„é”åŒ–
            enhanced = enhanced.filter(ImageFilter.UnsharpMask(radius=0.8, percent=80, threshold=5))
        else:
            sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = sharpness_enhancer.enhance(1.08)  # æ¸©å’Œé”åŒ–

        # ä¿æŒè‡ªç„¶äº®åº¦
        brightness_enhancer = ImageEnhance.Brightness(enhanced)
        enhanced = brightness_enhancer.enhance(1.01)  # å¾®è°ƒå³å¯

        print(f"âœ… è‡ªç„¶é£æ™¯å¢å¼ºå®Œæˆ")
        return enhanced

    except Exception as e:
        print(f"âŒ é£æ™¯å¢å¼ºå¤±è´¥: {e}")
        return None


def _apply_architecture_enhancement(image: Image.Image, quality: str) -> Optional[Image.Image]:
    """
    ğŸš€ å»ºç­‘å›¾åƒä¸“ç”¨å¢å¼º - ä¿æŒçœŸå®æ„Ÿ
    """
    try:
        print(f"ğŸ¢ åº”ç”¨å»ºç­‘ä¸“ç”¨å¢å¼º...")
        from PIL import ImageEnhance, ImageFilter

        enhanced = image.copy()

        # ğŸš€ å»ºç­‘å¢å¼ºç­–ç•¥ï¼ˆä¿æŒçœŸå®æ„Ÿï¼‰ï¼š
        # 1. é€‚åº¦å¼ºåŒ–è¾¹ç¼˜ï¼ˆé¿å…è¿‡åº¦é”åŒ–ï¼‰
        # 2. æ¸©å’Œå¢å¼ºå¯¹æ¯”åº¦ï¼ˆä¿æŒè‡ªç„¶å…‰å½±ï¼‰
        # 3. ä¿æŒæè´¨è´¨æ„Ÿ

        # é€‚åº¦è¾¹ç¼˜å¢å¼ºï¼ˆé¿å…è¿‡åº¦é”åŒ–ï¼‰
        if quality == "ultra_hd":
            # ä½¿ç”¨æ¸©å’Œçš„é”åŒ–å‚æ•°
            enhanced = enhanced.filter(ImageFilter.UnsharpMask(radius=1.2, percent=100, threshold=3))
        else:
            sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = sharpness_enhancer.enhance(1.15)  # é™ä½é”åŒ–å¼ºåº¦

        # æ¸©å’Œå¯¹æ¯”åº¦å¢å¼ºï¼ˆä¿æŒè‡ªç„¶å…‰å½±ï¼‰
        contrast_enhancer = ImageEnhance.Contrast(enhanced)
        enhanced = contrast_enhancer.enhance(1.08)  # é™ä½å¯¹æ¯”åº¦å¢å¼º

        # ä¿æŒè‡ªç„¶é¥±å’Œåº¦
        color_enhancer = ImageEnhance.Color(enhanced)
        enhanced = color_enhancer.enhance(1.03)  # æ›´æ¸©å’Œçš„é¥±å’Œåº¦

        # ä¿æŒè‡ªç„¶äº®åº¦
        brightness_enhancer = ImageEnhance.Brightness(enhanced)
        enhanced = brightness_enhancer.enhance(1.01)  # å¾®è°ƒå³å¯

        print(f"âœ… å»ºç­‘ä¸“ç”¨å¢å¼ºå®Œæˆ")
        return enhanced

    except Exception as e:
        print(f"âŒ å»ºç­‘å¢å¼ºå¤±è´¥: {e}")
        return None


def _apply_artwork_enhancement(image: Image.Image, quality: str) -> Optional[Image.Image]:
    """
    ğŸš€ è‰ºæœ¯ä½œå“ä¸“ç”¨å¢å¼º - ä¿æŒåŸæœ‰é£æ ¼
    """
    try:
        print(f"ğŸ¨ åº”ç”¨è‰ºæœ¯ä¸“ç”¨å¢å¼º...")
        from PIL import ImageEnhance, ImageFilter

        enhanced = image.copy()

        # ğŸš€ è‰ºæœ¯ä½œå“å¢å¼ºç­–ç•¥ï¼ˆä¿æŒåŸæœ‰é£æ ¼ï¼‰ï¼š
        # 1. æœ€å°åŒ–è‰²å½©æ”¹åŠ¨
        # 2. è½»å¾®ç»†èŠ‚å¢å¼º
        # 3. å®Œå…¨ä¿æŒè‰ºæœ¯é£æ ¼

        # éå¸¸æ¸©å’Œçš„è‰²å½©å¢å¼º
        color_enhancer = ImageEnhance.Color(enhanced)
        enhanced = color_enhancer.enhance(1.03)  # é™ä½åˆ°æœ€å°

        # è½»å¾®çš„å¯¹æ¯”åº¦å¢å¼º
        contrast_enhancer = ImageEnhance.Contrast(enhanced)
        enhanced = contrast_enhancer.enhance(1.04)  # é™ä½å¯¹æ¯”åº¦å¢å¼º

        # æœ€æ¸©å’Œçš„é”åŒ–
        if quality == "ultra_hd":
            # ä½¿ç”¨ææ¸©å’Œçš„é”åŒ–å‚æ•°
            enhanced = enhanced.filter(ImageFilter.UnsharpMask(radius=0.6, percent=60, threshold=8))
        else:
            sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = sharpness_enhancer.enhance(1.06)  # ææ¸©å’Œé”åŒ–

        # ä¿æŒåŸæœ‰äº®åº¦
        brightness_enhancer = ImageEnhance.Brightness(enhanced)
        enhanced = brightness_enhancer.enhance(1.005)  # å‡ ä¹ä¸æ”¹å˜

        print(f"âœ… è‰ºæœ¯ä¸“ç”¨å¢å¼ºå®Œæˆ")
        return enhanced

    except Exception as e:
        print(f"âŒ è‰ºæœ¯å¢å¼ºå¤±è´¥: {e}")
        return None


def _apply_adaptive_traditional_enhancement(image: Image.Image, quality: str, image_type: str) -> Image.Image:
    """
    ğŸš€ è‡ªé€‚åº”ä¼ ç»Ÿå¢å¼ºï¼šæ ¹æ®å›¾åƒç±»å‹è°ƒæ•´å‚æ•°
    """
    try:
        print(f"ğŸ¨ åº”ç”¨è‡ªé€‚åº”ä¼ ç»Ÿå¢å¼º (ç±»å‹: {image_type}, è´¨é‡: {quality})...")

        # å¦‚æœå·²ç»åº”ç”¨äº†ä¸“ç”¨å¢å¼ºï¼Œä½¿ç”¨æ›´æ¸©å’Œçš„å‚æ•°
        if image_type in ["landscape", "architecture", "artwork"]:
            return _apply_gentle_traditional_enhancement(image, quality)
        else:
            # å¯¹äºäººåƒå’Œé€šç”¨å›¾åƒï¼Œä½¿ç”¨æ ‡å‡†å¢å¼º
            return _apply_advanced_traditional_enhancement(image, quality)

    except Exception as e:
        print(f"âŒ è‡ªé€‚åº”ä¼ ç»Ÿå¢å¼ºå¤±è´¥: {e}")
        return image


def _apply_gentle_traditional_enhancement(image: Image.Image, quality: str) -> Image.Image:
    """
    ğŸš€ ææ¸©å’Œçš„ä¼ ç»Ÿå¢å¼ºï¼šç”¨äºå·²ç»åº”ç”¨ä¸“ç”¨å¢å¼ºçš„å›¾åƒ
    """
    try:
        from PIL import ImageEnhance

        print(f"ğŸ¨ åº”ç”¨ææ¸©å’Œä¼ ç»Ÿå¢å¼º...")

        # ææ¸©å’Œçš„é”åŒ–
        enhancer = ImageEnhance.Sharpness(image)
        sharpness_factor = 1.02 if quality == "hd" else 1.04  # é™ä½åˆ°ææ¸©å’Œ
        image = enhancer.enhance(sharpness_factor)

        # æè½»å¾®çš„å¯¹æ¯”åº¦è°ƒæ•´
        enhancer = ImageEnhance.Contrast(image)
        contrast_factor = 1.015 if quality == "hd" else 1.025  # é™ä½åˆ°ææ¸©å’Œ
        image = enhancer.enhance(contrast_factor)

        # æå¾®è°ƒé¥±å’Œåº¦
        enhancer = ImageEnhance.Color(image)
        color_factor = 1.01 if quality == "hd" else 1.015  # é™ä½åˆ°ææ¸©å’Œ
        image = enhancer.enhance(color_factor)

        # äº®åº¦å‡ ä¹ä¸å˜
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.003)  # å‡ ä¹ä¸æ”¹å˜

        print(f"âœ… ææ¸©å’Œä¼ ç»Ÿå¢å¼ºå®Œæˆ")
        return image

    except Exception as e:
        print(f"âŒ æ¸©å’Œä¼ ç»Ÿå¢å¼ºå¤±è´¥: {e}")
        return image


def _check_and_correct_over_enhancement(original: Image.Image, enhanced: Image.Image) -> Image.Image:
    """
    ğŸš€ æ™ºèƒ½æ£€æµ‹è¿‡åº¦å¢å¼ºå¹¶è‡ªåŠ¨ä¿®æ­£
    """
    try:
        import numpy as np
        from PIL import ImageStat

        print(f"ğŸ” æ£€æµ‹è¿‡åº¦å¢å¼º...")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œåˆ†æ
        orig_array = np.array(original)
        enh_array = np.array(enhanced)

        # æ£€æµ‹è¿‡åº¦é¥±å’Œ
        orig_sat = np.std(orig_array)
        enh_sat = np.std(enhanced)
        saturation_increase = enh_sat / orig_sat if orig_sat > 0 else 1.0

        # æ£€æµ‹è¿‡åº¦å¯¹æ¯”åº¦
        orig_contrast = np.std(orig_array.astype(float))
        enh_contrast = np.std(enh_array.astype(float))
        contrast_increase = enh_contrast / orig_contrast if orig_contrast > 0 else 1.0

        # æ£€æµ‹è¿‡åº¦é”åŒ–ï¼ˆè¾¹ç¼˜æ£€æµ‹ï¼‰
        try:
            import cv2
            orig_gray = cv2.cvtColor(orig_array, cv2.COLOR_RGB2GRAY)
            enh_gray = cv2.cvtColor(enh_array, cv2.COLOR_RGB2GRAY)

            orig_edges = cv2.Canny(orig_gray, 50, 150)
            enh_edges = cv2.Canny(enh_gray, 50, 150)

            orig_edge_count = np.sum(orig_edges > 0)
            enh_edge_count = np.sum(enh_edges > 0)
            edge_increase = enh_edge_count / orig_edge_count if orig_edge_count > 0 else 1.0
        except:
            edge_increase = 1.0

        print(f"ğŸ” å¢å¼ºåˆ†æ: é¥±å’Œåº¦x{saturation_increase:.2f}, å¯¹æ¯”åº¦x{contrast_increase:.2f}, è¾¹ç¼˜x{edge_increase:.2f}")

        # åˆ¤æ–­æ˜¯å¦è¿‡åº¦å¢å¼º
        over_enhanced = False
        blend_ratio = 1.0  # 1.0 = å®Œå…¨ä½¿ç”¨å¢å¼ºå›¾åƒï¼Œ0.0 = å®Œå…¨ä½¿ç”¨åŸå›¾

        if saturation_increase > 1.15:  # é¥±å’Œåº¦å¢åŠ è¶…è¿‡15%
            over_enhanced = True
            blend_ratio *= 0.7
            print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦é¥±å’Œ")

        if contrast_increase > 1.2:  # å¯¹æ¯”åº¦å¢åŠ è¶…è¿‡20%
            over_enhanced = True
            blend_ratio *= 0.8
            print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦å¯¹æ¯”")

        if edge_increase > 1.5:  # è¾¹ç¼˜å¢åŠ è¶…è¿‡50%
            over_enhanced = True
            blend_ratio *= 0.6
            print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦é”åŒ–")

        if over_enhanced:
            # æ··åˆåŸå›¾å’Œå¢å¼ºå›¾åƒä»¥å‡å°‘è¿‡åº¦å¢å¼º
            print(f"ğŸ”§ åº”ç”¨ä¿®æ­£æ··åˆ (æ¯”ä¾‹: {blend_ratio:.2f})")

            orig_array = orig_array.astype(float)
            enh_array = enh_array.astype(float)

            corrected_array = (enh_array * blend_ratio + orig_array * (1 - blend_ratio)).astype(np.uint8)
            corrected_image = Image.fromarray(corrected_array)

            print(f"âœ… è¿‡åº¦å¢å¼ºä¿®æ­£å®Œæˆ")
            return corrected_image
        else:
            print(f"âœ… å¢å¼ºæ•ˆæœè‡ªç„¶ï¼Œæ— éœ€ä¿®æ­£")
            return enhanced

    except Exception as e:
        print(f"âŒ è¿‡åº¦å¢å¼ºæ£€æµ‹å¤±è´¥: {e}")
        return enhanced


def _detect_faces_in_image(image: Image.Image) -> bool:
    """
    å¿«é€Ÿæ£€æµ‹å›¾åƒä¸­æ˜¯å¦æœ‰äººè„¸
    """
    faces = _detect_faces_with_locations(image)
    return len(faces) > 0


def _detect_faces_with_locations(image: Image.Image) -> list:
    """
    æ£€æµ‹å›¾åƒä¸­çš„äººè„¸å¹¶è¿”å›ä½ç½®ä¿¡æ¯
    è¿”å›æ ¼å¼: [(x, y, w, h), ...]
    """
    try:
        import cv2
        import numpy as np

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        img_array = np.array(image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

        # ä½¿ç”¨Haarçº§è”åˆ†ç±»å™¨æ£€æµ‹äººè„¸
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(30, 30))

        return faces.tolist() if len(faces) > 0 else []

    except Exception as e:
        print(f"âš ï¸ äººè„¸æ£€æµ‹å¤±è´¥: {e}")
        return []


def _enhance_face_regions_with_ai_upscale(image: Image.Image, faces: list) -> Optional[Image.Image]:
    """
    ğŸš€ ä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼ºäººè„¸åŒºåŸŸ
    è¿™æ˜¯ä¸€ä¸ªå®ç”¨çš„äººè„¸å¢å¼ºæ–¹æ¡ˆï¼Œåˆ©ç”¨ç°æœ‰çš„AIæ”¾å¤§æŠ€æœ¯
    """
    try:
        if not faces:
            return None

        print(f"ğŸ¯ ä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼º{len(faces)}ä¸ªäººè„¸åŒºåŸŸ...")

        # åˆ›å»ºå›¾åƒå‰¯æœ¬
        enhanced_image = image.copy()

        for i, (x, y, w, h) in enumerate(faces):
            try:
                # æ‰©å±•äººè„¸åŒºåŸŸï¼ˆåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
                padding = max(w, h) // 4
                expanded_x = max(0, x - padding)
                expanded_y = max(0, y - padding)
                expanded_w = min(image.width - expanded_x, w + 2 * padding)
                expanded_h = min(image.height - expanded_y, h + 2 * padding)

                # æå–äººè„¸åŒºåŸŸ
                face_region = image.crop((expanded_x, expanded_y, expanded_x + expanded_w, expanded_y + expanded_h))

                # ä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼ºäººè„¸åŒºåŸŸ
                # è®¡ç®—åˆé€‚çš„æ”¾å¤§å€æ•°
                face_size = max(expanded_w, expanded_h)
                if face_size < 128:
                    scale_factor = 2.0  # å°äººè„¸éœ€è¦æ›´å¤šå¢å¼º
                elif face_size < 256:
                    scale_factor = 1.5  # ä¸­ç­‰äººè„¸é€‚åº¦å¢å¼º
                else:
                    scale_factor = 1.2  # å¤§äººè„¸è½»å¾®å¢å¼º

                target_w = int(expanded_w * scale_factor)
                target_h = int(expanded_h * scale_factor)

                print(f"ğŸš€ å¢å¼ºäººè„¸{i+1}: {expanded_w}x{expanded_h} -> {target_w}x{target_h}")

                # ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§
                enhanced_face = smart_ai_upscale(face_region, target_w, target_h)

                if enhanced_face:
                    # ç¼©å›åŸå°ºå¯¸ï¼Œä¿ç•™å¢å¼ºæ•ˆæœ
                    enhanced_face_resized = enhanced_face.resize((expanded_w, expanded_h), Image.Resampling.LANCZOS)

                    # å°†å¢å¼ºåçš„äººè„¸åŒºåŸŸç²˜è´´å›åŸå›¾
                    enhanced_image.paste(enhanced_face_resized, (expanded_x, expanded_y))
                    print(f"âœ… äººè„¸{i+1}å¢å¼ºå®Œæˆ")
                else:
                    print(f"âš ï¸ äººè„¸{i+1}AIæ”¾å¤§å¤±è´¥ï¼Œè·³è¿‡")

            except Exception as e:
                print(f"âŒ äººè„¸{i+1}å¢å¼ºå¤±è´¥: {e}")
                continue

        print(f"âœ… æ‰€æœ‰äººè„¸åŒºåŸŸAIå¢å¼ºå®Œæˆ")
        return enhanced_image

    except Exception as e:
        print(f"âŒ AIäººè„¸åŒºåŸŸå¢å¼ºå¤±è´¥: {e}")
        return None


def _try_codeformer_restoration(image: Image.Image) -> Optional[Image.Image]:
    """
    å°è¯•ä½¿ç”¨CodeFormerè¿›è¡Œäººè„¸ä¿®å¤
    """
    try:
        # ğŸš€ å°è¯•ä½¿ç”¨ComfyUIçš„FaceRestoreèŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ComfyUIçš„äººè„¸ä¿®å¤èŠ‚ç‚¹
            import comfy.model_management
            from comfy.utils import load_torch_file

            # æŸ¥æ‰¾CodeFormeræ¨¡å‹
            models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'models')
            possible_paths = [
                os.path.join(models_dir, 'facerestore_models', 'codeformer-v0.1.0.pth'),
                os.path.join(models_dir, 'face_restore', 'codeformer-v0.1.0.pth'),
                os.path.join(models_dir, 'upscale_models', 'codeformer-v0.1.0.pth'),
            ]

            codeformer_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    codeformer_path = path
                    break

            if codeformer_path:
                print(f"ğŸš€ æ‰¾åˆ°CodeFormeræ¨¡å‹: {codeformer_path}")

                # å°è¯•å®é™…è°ƒç”¨CodeFormer
                try:
                    import cv2
                    from basicsr.utils import imwrite
                    from codeformer import CodeFormer
                    import torch

                    print(f"âœ… CodeFormerä¾èµ–æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹äººè„¸ä¿®å¤...")

                    # è½¬æ¢PILå›¾åƒä¸ºOpenCVæ ¼å¼
                    img_array = np.array(image)
                    if img_array.shape[2] == 3:  # RGB
                        img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
                    else:  # RGBA
                        img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)

                    # åˆå§‹åŒ–CodeFormer
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    codeformer = CodeFormer(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9,
                                          connect_list=['32', '64', '128', '256']).to(device)

                    # åŠ è½½æ¨¡å‹æƒé‡
                    checkpoint = torch.load(codeformer_path, map_location=device)
                    codeformer.load_state_dict(checkpoint['params_ema'])
                    codeformer.eval()

                    # é¢„å¤„ç†å›¾åƒ
                    img_tensor = torch.from_numpy(img_cv).float().permute(2, 0, 1).unsqueeze(0) / 255.0
                    img_tensor = img_tensor.to(device)

                    # æ‰§è¡Œä¿®å¤
                    with torch.no_grad():
                        output = codeformer(img_tensor, w=0.5, adain=True)[0]
                        output = output.squeeze(0).permute(1, 2, 0).cpu().numpy()
                        output = (output * 255).astype(np.uint8)

                    # è½¬æ¢å›PILæ ¼å¼
                    output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB)
                    result_image = Image.fromarray(output_rgb)

                    print(f"âœ… CodeFormeräººè„¸ä¿®å¤å®Œæˆ")
                    return result_image

                except ImportError as e:
                    print(f"ğŸ” CodeFormerä¾èµ–ç¼ºå¤±: {e}")
                    print(f"ğŸ’¡ è¯·å®‰è£…: pip install -r requirements_face_restore.txt")
                    return None
                except Exception as e:
                    print(f"âŒ CodeFormeræ‰§è¡Œå¤±è´¥: {e}")
                    return None
            else:
                print(f"ğŸ” CodeFormeræ¨¡å‹æœªæ‰¾åˆ°")
                return None

        except ImportError:
            print(f"ğŸ” ComfyUIäººè„¸ä¿®å¤æ¨¡å—ä¸å¯ç”¨")
            return None

    except Exception as e:
        print(f"âŒ CodeFormerä¿®å¤å¤±è´¥: {e}")
        return None


def _try_gfpgan_restoration(image: Image.Image) -> Optional[Image.Image]:
    """
    å°è¯•ä½¿ç”¨GFPGANè¿›è¡Œäººè„¸ä¿®å¤
    """
    try:
        # ğŸš€ å°è¯•ä½¿ç”¨ComfyUIçš„FaceRestoreèŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import comfy.model_management
            from comfy.utils import load_torch_file

            # æŸ¥æ‰¾GFPGANæ¨¡å‹
            models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'models')
            possible_paths = [
                os.path.join(models_dir, 'facerestore_models', 'GFPGANv1.4.pth'),
                os.path.join(models_dir, 'face_restore', 'GFPGANv1.4.pth'),
                os.path.join(models_dir, 'upscale_models', 'GFPGANv1.4.pth'),
            ]

            gfpgan_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    gfpgan_path = path
                    break

            if gfpgan_path:
                print(f"ğŸš€ æ‰¾åˆ°GFPGANæ¨¡å‹: {gfpgan_path}")

                # å°è¯•å®é™…è°ƒç”¨GFPGAN
                try:
                    import cv2
                    from gfpgan import GFPGANer
                    import torch

                    print(f"âœ… GFPGANä¾èµ–æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹äººè„¸ä¿®å¤...")

                    # è½¬æ¢PILå›¾åƒä¸ºOpenCVæ ¼å¼
                    img_array = np.array(image)
                    if img_array.shape[2] == 3:  # RGB
                        img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
                    else:  # RGBA
                        img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)

                    # åˆå§‹åŒ–GFPGAN
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

                    # åˆ›å»ºGFPGANå®ä¾‹
                    restorer = GFPGANer(
                        model_path=gfpgan_path,
                        upscale=1,  # ä¸æ”¾å¤§ï¼Œåªä¿®å¤
                        arch='clean',
                        channel_multiplier=2,
                        bg_upsampler=None,
                        device=device
                    )

                    # æ‰§è¡Œä¿®å¤
                    cropped_faces, restored_faces, restored_img = restorer.enhance(
                        img_cv,
                        has_aligned=False,
                        only_center_face=False,
                        paste_back=True,
                        weight=0.5
                    )

                    # è½¬æ¢å›PILæ ¼å¼
                    if restored_img is not None:
                        output_rgb = cv2.cvtColor(restored_img, cv2.COLOR_BGR2RGB)
                        result_image = Image.fromarray(output_rgb)
                        print(f"âœ… GFPGANäººè„¸ä¿®å¤å®Œæˆ")
                        return result_image
                    else:
                        print(f"âš ï¸ GFPGANæœªæ£€æµ‹åˆ°äººè„¸æˆ–ä¿®å¤å¤±è´¥")
                        return None

                except ImportError as e:
                    print(f"ğŸ” GFPGANä¾èµ–ç¼ºå¤±: {e}")
                    print(f"ğŸ’¡ è¯·å®‰è£…: pip install -r requirements_face_restore.txt")
                    return None
                except Exception as e:
                    print(f"âŒ GFPGANæ‰§è¡Œå¤±è´¥: {e}")
                    return None
            else:
                print(f"ğŸ” GFPGANæ¨¡å‹æœªæ‰¾åˆ°")
                return None

        except ImportError:
            print(f"ğŸ” ComfyUIäººè„¸ä¿®å¤æ¨¡å—ä¸å¯ç”¨")
            return None

    except Exception as e:
        print(f"âŒ GFPGANä¿®å¤å¤±è´¥: {e}")
        return None


def _apply_advanced_traditional_enhancement(image: Image.Image, quality: str) -> Image.Image:
    """
    ğŸš€ è‡ªç„¶ä¼ ç»Ÿç”»è´¨å¢å¼ºç®—æ³• - ä¿æŒçœŸå®æ„Ÿ
    """
    try:
        from PIL import ImageEnhance, ImageFilter
        import numpy as np

        print(f"ğŸ¨ åº”ç”¨è‡ªç„¶ä¼ ç»Ÿå¢å¼ºç®—æ³•...")

        # ğŸš€ ç¬¬ä¸€æ­¥ï¼šè½»å¾®å»å™ªï¼ˆä»…åœ¨ultra_hdæ¨¡å¼ä¸‹ï¼‰
        if quality == "ultra_hd":
            # éå¸¸è½»å¾®çš„å»å™ª
            denoised = image.filter(ImageFilter.GaussianBlur(radius=0.3))
            # ä¸åŸå›¾æ··åˆï¼ˆæ›´ä¿å®ˆçš„æ¯”ä¾‹ï¼‰
            original_array = np.array(image)
            denoised_array = np.array(denoised)
            mixed_array = (original_array * 0.9 + denoised_array * 0.1).astype(np.uint8)
            image = Image.fromarray(mixed_array)
            print(f"âœ… è½»å¾®å»å™ªå®Œæˆ")

        # ğŸš€ ç¬¬äºŒæ­¥ï¼šè‡ªç„¶é”åŒ–ï¼ˆé™ä½å¼ºåº¦ï¼‰
        enhancer = ImageEnhance.Sharpness(image)
        if quality == "hd":
            sharpness_factor = 1.06  # éå¸¸æ¸©å’Œçš„é”åŒ–
        else:  # ultra_hd
            sharpness_factor = 1.1   # æ¸©å’Œé”åŒ–
        image = enhancer.enhance(sharpness_factor)
        print(f"âœ… è‡ªç„¶é”åŒ–å®Œæˆ (factor: {sharpness_factor})")

        # ğŸš€ ç¬¬ä¸‰æ­¥ï¼šæ¸©å’Œå¯¹æ¯”åº¦å¢å¼º
        enhancer = ImageEnhance.Contrast(image)
        if quality == "hd":
            contrast_factor = 1.04  # éå¸¸æ¸©å’Œ
        else:  # ultra_hd
            contrast_factor = 1.06  # æ¸©å’Œå¯¹æ¯”åº¦
        image = enhancer.enhance(contrast_factor)
        print(f"âœ… æ¸©å’Œå¯¹æ¯”åº¦å¢å¼ºå®Œæˆ (factor: {contrast_factor})")

        # ğŸš€ ç¬¬å››æ­¥ï¼šè‡ªç„¶è‰²å½©ä¼˜åŒ–
        enhancer = ImageEnhance.Color(image)
        if quality == "hd":
            color_factor = 1.02  # éå¸¸æ¸©å’Œçš„é¥±å’Œåº¦
        else:  # ultra_hd
            color_factor = 1.03  # æ¸©å’Œé¥±å’Œåº¦
        image = enhancer.enhance(color_factor)
        print(f"âœ… è‡ªç„¶è‰²å½©ä¼˜åŒ–å®Œæˆ (factor: {color_factor})")

        # ğŸš€ ç¬¬äº”æ­¥ï¼šäº®åº¦ä¿æŒï¼ˆå‡ ä¹ä¸å˜ï¼‰
        enhancer = ImageEnhance.Brightness(image)
        brightness_factor = 1.005  # å‡ ä¹ä¸æ”¹å˜äº®åº¦
        image = enhancer.enhance(brightness_factor)
        print(f"âœ… äº®åº¦ä¿æŒå®Œæˆ (factor: {brightness_factor})")

        print(f"ğŸ¨ è‡ªç„¶ä¼ ç»Ÿå¢å¼ºç®—æ³•å®Œæˆ")
        return image

    except Exception as e:
        print(f"âŒ ä¼ ç»Ÿå¢å¼ºå¤±è´¥: {e}")
        return image

def _apply_full_enhancements(base_image: Image.Image, target_size_str: str, quality: str, enhance_quality: bool, smart_resize_enabled: bool) -> Image.Image:
    """å°ºå¯¸é€‚é… + ä¸»ä½“æ£€æµ‹è£å‰ª + ç”»è´¨å¢å¼ºï¼Œä¾›nano-bananaå¤ç”¨"""
    print(f"ğŸ”§ å¼€å§‹å›¾åƒå¢å¼ºå¤„ç†...")
    print(f"ğŸ”§ è¾“å…¥å‚æ•°: target_size={target_size_str}, quality={quality}, enhance_quality={enhance_quality}, smart_resize={smart_resize_enabled}")
    print(f"ğŸ”§ åŸå§‹å›¾åƒå°ºå¯¸: {base_image.size}")

    image = base_image
    try:
        if 'x' in target_size_str and target_size_str != "Original size":
            target_width, target_height = map(int, target_size_str.split('x'))
            print(f"ğŸ”§ ç›®æ ‡å°ºå¯¸: {target_width}x{target_height} (æ¥è‡ªå‚æ•°: {target_size_str})")
        elif target_size_str == "Original size":
            target_width, target_height = image.size
            print(f"ğŸ”§ ç›®æ ‡å°ºå¯¸: {target_width}x{target_height} (ä¿æŒåŸå§‹å°ºå¯¸)")
        else:
            # ğŸš€ ä¿®å¤ï¼šå¦‚æœå‚æ•°æ— æ³•è§£æï¼Œå°è¯•ä»å¸¸è§é¢„è®¾ä¸­åŒ¹é…
            print(f"âš ï¸ æ— æ³•è§£æç›®æ ‡å°ºå¯¸å‚æ•°: {target_size_str}")

            # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„å°ºå¯¸é¢„è®¾
            common_sizes = {
                "512x512": (512, 512),
                "768x768": (768, 768),
                "1024x1024": (1024, 1024),
                "1024x1792": (1024, 1792),
                "1792x1024": (1792, 1024),
                "1920x1080": (1920, 1080),
                "2560x1440": (2560, 1440),
                "3840x2160": (3840, 2160)
            }

            if target_size_str in common_sizes:
                target_width, target_height = common_sizes[target_size_str]
                print(f"ğŸ”§ ç›®æ ‡å°ºå¯¸: {target_width}x{target_height} (ä»é¢„è®¾åŒ¹é…: {target_size_str})")
            else:
                target_width, target_height = image.size
                print(f"ğŸ”§ ç›®æ ‡å°ºå¯¸: {target_width}x{target_height} (å›é€€åˆ°åŸå§‹å°ºå¯¸)")
    except Exception as e:
        target_width, target_height = image.size
        print(f"âš ï¸ è§£æç›®æ ‡å°ºå¯¸å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å°ºå¯¸: {e}")

    try:
        if smart_resize_enabled and (image.size != (target_width, target_height)):
            print(f"ğŸ¯ å¯ç”¨æ™ºèƒ½è°ƒæ•´å°ºå¯¸...")

            # ğŸš€ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥AIæ¨¡å‹æ˜¯å¦å·²ç»ç”Ÿæˆäº†åˆé€‚çš„å°ºå¯¸
            current_aspect = image.size[0] / image.size[1]
            target_aspect = target_width / target_height
            aspect_diff = abs(current_aspect - target_aspect) / target_aspect

            print(f"ğŸ” å®½é«˜æ¯”åˆ†æ: å½“å‰={current_aspect:.3f}, ç›®æ ‡={target_aspect:.3f}, å·®å¼‚={aspect_diff:.3f}")

            # å¦‚æœå®½é«˜æ¯”å·®å¼‚å¾ˆå°ï¼ˆ<10%ï¼‰ï¼Œä½¿ç”¨æ¸©å’Œçš„è°ƒæ•´ç­–ç•¥
            if aspect_diff < 0.1:
                print(f"ğŸ¯ å®½é«˜æ¯”æ¥è¿‘ç›®æ ‡ï¼Œä½¿ç”¨æ¸©å’Œè°ƒæ•´ç­–ç•¥...")
                # ç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸ï¼Œä¸è¿›è¡Œæ¿€è¿›è£å‰ª
                image = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
                print(f"âœ… æ¸©å’Œè°ƒæ•´å®Œæˆ: {image.size}")
            elif image.size[0] / image.size[1] != target_width / target_height:
                print(f"ğŸ¯ æ£€æµ‹åˆ°å®½é«˜æ¯”ä¸åŒ¹é…ï¼Œå¯ç”¨æ™ºèƒ½ä¸»ä½“æ£€æµ‹...")

                # ğŸš€ æ™ºèƒ½åˆ†ææ”¾å¤§éœ€æ±‚
                original_width, original_height = image.size
                width_ratio = target_width / original_width
                height_ratio = target_height / original_height

                print(f"ğŸ“Š åŸå§‹å°ºå¯¸: {original_width}x{original_height}")
                print(f"ğŸ“Š ç›®æ ‡å°ºå¯¸: {target_width}x{target_height}")
                print(f"ğŸ“Š å®½åº¦æ¯”ä¾‹: {width_ratio:.3f}, é«˜åº¦æ¯”ä¾‹: {height_ratio:.3f}")

                scale = max(width_ratio, height_ratio)
                enlarged_w = max(1, int(original_width * scale))
                enlarged_h = max(1, int(original_height * scale))
                print(f"ğŸ¯ æ™ºèƒ½æ”¾å¤§åˆ°: {enlarged_w}x{enlarged_h} (æŒ‰{'é«˜åº¦' if height_ratio > width_ratio else 'å®½åº¦'}éœ€æ±‚)")

                try:
                    print(f"ğŸš€ å°è¯•AIæ™ºèƒ½æ”¾å¤§...")
                    ai_up = smart_ai_upscale(image, enlarged_w, enlarged_h)
                    enlarged = ai_up if ai_up is not None else image.resize((enlarged_w, enlarged_h), Image.Resampling.LANCZOS)
                    if ai_up is not None:
                        print(f"âœ… AIæ™ºèƒ½æ”¾å¤§æˆåŠŸ")
                    else:
                        print(f"âš ï¸ AIæ™ºèƒ½æ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ”¾å¤§")
                except Exception as e:
                    print(f"âŒ AIæ™ºèƒ½æ”¾å¤§å¼‚å¸¸: {e}")
                    enlarged = image.resize((enlarged_w, enlarged_h), Image.Resampling.LANCZOS)

                try:
                    print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½ä¸»ä½“æ£€æµ‹...")
                    subject_bbox, subject_center = detect_image_foreground_subject(enlarged)
                    cx, cy = subject_center
                    print(f"ğŸ¯ æ£€æµ‹åˆ°ä¸»ä½“ä¸­å¿ƒ: ({cx}, {cy})")

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿ä¸»ä½“ä¸ä¼šè¢«è£å‰ªæ‰
                    subject_x, subject_y, subject_w, subject_h = subject_bbox

                    # æ£€æŸ¥ä¸»ä½“æ˜¯å¦åˆç†ï¼ˆä¸èƒ½å¤ªå°æˆ–å¤ªå¤§ï¼‰
                    enlarged_area = enlarged.width * enlarged.height
                    subject_area = subject_w * subject_h
                    subject_ratio = subject_area / enlarged_area

                    print(f"ğŸ” ä¸»ä½“åŒºåŸŸæ£€æŸ¥: ä¸»ä½“({subject_x}, {subject_y}, {subject_w}x{subject_h}), å æ¯”{subject_ratio:.3f}")

                    # ğŸ“Š ä¸»ä½“ä¿¡æ¯è®°å½•ï¼ˆä»…ä¾›å‚è€ƒï¼Œä¸åšè°ƒæ•´ï¼‰
                    print(f"ğŸ“Š ä¸»ä½“ä¿¡æ¯: ä½ç½®({subject_x}, {subject_y}), å°ºå¯¸({subject_w}x{subject_h}), å æ¯”{subject_ratio:.3f}")

                    # ğŸ¯ ç®€åŒ–ç­–ç•¥ï¼šç›´æ¥ä½¿ç”¨ä¸­å¿ƒè£å‰ªï¼Œä¿æŒä¸»ä½“å±…ä¸­
                    print(f"ğŸ¯ ä½¿ç”¨ä¸­å¿ƒè£å‰ªç­–ç•¥ï¼Œä¿æŒä¸»ä½“å±…ä¸­")

                    # ä½¿ç”¨å›¾åƒä¸­å¿ƒè¿›è¡Œè£å‰ª
                    crop_x = max(0, (enlarged.width - target_width) // 2)
                    crop_y = max(0, (enlarged.height - target_height) // 2)
                    image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                    print(f"âœ… ä¸­å¿ƒè£å‰ªå®Œæˆ: ({crop_x}, {crop_y})")

                    # ä¿ç•™åŸæœ‰çš„å°ä¸»ä½“æ£€æµ‹é€»è¾‘
                    if subject_ratio > 0.2:
                        print(f"ğŸ“Š ä¸»ä½“å æ¯”ä¿¡æ¯: {subject_ratio:.3f} (ä»…ä¾›å‚è€ƒ)")
                    elif subject_ratio < 0.01:
                        print(f"âš ï¸ ä¸»ä½“å æ¯”è¿‡å°({subject_ratio:.3f})ï¼Œæ£€æµ‹å¯èƒ½æœ‰è¯¯ï¼Œä½¿ç”¨ä¸­å¿ƒè£å‰ª")
                        # ä½¿ç”¨å›¾åƒä¸­å¿ƒä½œä¸ºè£å‰ªä¸­å¿ƒ
                        crop_x = max(0, (enlarged.width - target_width) // 2)
                        crop_y = max(0, (enlarged.height - target_height) // 2)
                        image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                        print(f"âš ï¸ ä½¿ç”¨ä¸­å¿ƒè£å‰ª: ({crop_x}, {crop_y})")
                    else:
                        # æ­£å¸¸çš„æ™ºèƒ½ä¸»ä½“å±…ä¸­è£å‰ª
                        cx, cy = subject_center

                        # è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“åœ¨è£å‰ªåŒºåŸŸå†…
                        crop_x = int(cx - target_width / 2)
                        crop_y = int(cy - target_height / 2)

                        # ğŸš€ å…³é”®ä¿®å¤ï¼šç¡®ä¿è£å‰ªåŒºåŸŸåŒ…å«ä¸»ä½“
                        # æ£€æŸ¥ä¸»ä½“æ˜¯å¦ä¼šè¢«è£å‰ªæ‰
                        crop_right = crop_x + target_width
                        crop_bottom = crop_y + target_height
                        subject_right = subject_x + subject_w
                        subject_bottom = subject_y + subject_h

                        # å¦‚æœä¸»ä½“ä¼šè¢«è£å‰ªæ‰ï¼Œè°ƒæ•´è£å‰ªä½ç½®
                        if subject_x < crop_x:  # ä¸»ä½“å·¦è¾¹è¢«è£å‰ª
                            crop_x = max(0, subject_x - target_width // 10)  # ç•™10%è¾¹è·
                        if subject_y < crop_y:  # ä¸»ä½“ä¸Šè¾¹è¢«è£å‰ª
                            crop_y = max(0, subject_y - target_height // 10)
                        if subject_right > crop_right:  # ä¸»ä½“å³è¾¹è¢«è£å‰ª
                            crop_x = min(enlarged.width - target_width, subject_right - target_width + target_width // 10)
                        if subject_bottom > crop_bottom:  # ä¸»ä½“ä¸‹è¾¹è¢«è£å‰ª
                            crop_y = min(enlarged.height - target_height, subject_bottom - target_height + target_height // 10)

                        # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
                        crop_x = max(0, min(crop_x, enlarged.width - target_width))
                        crop_y = max(0, min(crop_y, enlarged.height - target_height))

                        print(f"ğŸ¯ æ™ºèƒ½è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
                        print(f"ğŸ” ä¸»ä½“ä¿æŠ¤æ£€æŸ¥: ä¸»ä½“({subject_x}, {subject_y}) -> ({subject_right}, {subject_bottom})")

                        image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                        print(f"âœ… æ™ºèƒ½ä¸»ä½“å±…ä¸­è£å‰ªå®Œæˆ")

                except Exception as e:
                    print(f"âŒ æ™ºèƒ½ä¸»ä½“æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å®‰å…¨ä¸­å¿ƒè£å‰ª: {e}")
                    import traceback
                    print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    # ğŸš€ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´å®‰å…¨çš„ä¸­å¿ƒè£å‰ª
                    crop_x = max(0, (enlarged.width - target_width) // 2)
                    crop_y = max(0, (enlarged.height - target_height) // 2)
                    image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                    print(f"âš ï¸ ä½¿ç”¨å®‰å…¨ä¸­å¿ƒè£å‰ª: ({crop_x}, {crop_y})")
            else:
                print(f"ğŸ¯ å®½é«˜æ¯”åŒ¹é…ï¼Œç›´æ¥ç¼©æ”¾")
                image = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        else:
            if not smart_resize_enabled:
                print(f"ğŸ”§ æ™ºèƒ½è°ƒæ•´å·²ç¦ç”¨ï¼Œä½¿ç”¨å¡«å……æ¨¡å¼")
            else:
                print(f"ğŸ”§ å°ºå¯¸å·²åŒ¹é…ï¼Œä½¿ç”¨å¡«å……æ¨¡å¼")
            image = smart_resize_with_padding(image, (target_width, target_height))
    except Exception as e:
        print(f"âŒ å°ºå¯¸è°ƒæ•´å¤±è´¥: {e}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    try:
        if enhance_quality and quality in ['hd', 'ultra_hd']:
            print(f"ğŸ¨ å¼€å§‹ç”»è´¨å¢å¼º (è´¨é‡çº§åˆ«: {quality})...")
            enhanced = enhance_image_quality(image, quality, "disabled")
            if enhanced:
                image = enhanced
                print(f"âœ… ç”»è´¨å¢å¼ºå®Œæˆ")
            else:
                print(f"âš ï¸ ç”»è´¨å¢å¼ºè¿”å›None")
        else:
            print(f"ğŸ”§ è·³è¿‡ç”»è´¨å¢å¼º (enhance_quality={enhance_quality}, quality={quality})")
    except Exception as e:
        print(f"âŒ ç”»è´¨å¢å¼ºå¤±è´¥: {e}")

    print(f"ğŸ”§ å›¾åƒå¢å¼ºå¤„ç†å®Œæˆï¼Œæœ€ç»ˆå°ºå¯¸: {image.size}")
    return image

def image_to_base64(image, format='JPEG'):
    """Convert PIL Image to base64 string"""
    buffer = io.BytesIO()
    if format.upper() == 'JPEG' and image.mode in ('RGBA', 'LA', 'P'):
        background = Image.new('RGB', image.size, (255, 255, 255))
        if image.mode == 'P':
            image = image.convert('RGBA')
        background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
        image = background
    image.save(buffer, format=format, quality=95)
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

def _make_chat_summary(response_text: str) -> str:
    try:
        import re
        url_pattern = r'https?://\S+\.(?:jpg|jpeg|png|gif|webp)'
        m = re.search(url_pattern, response_text or '')
        if m:
            return f"å·²ç”Ÿæˆå›¾åƒ: {m.group(0)}"
        # markdown image
        md_pat = r"!\[.*?\]\((.*?)\)"
        m2 = re.search(md_pat, response_text or '')
        if m2:
            return f"å·²ç”Ÿæˆå›¾åƒ: {m2.group(1)}"
        # base64
        if 'data:image/' in (response_text or ''):
            return "å·²ç”Ÿæˆå›¾åƒï¼ˆå†…åµŒbase64ï¼‰"
    except Exception:
        pass
    return "å·²ç”Ÿæˆå›¾åƒ"

def validate_api_key(api_key):
    """Validate API key format"""
    return api_key and len(api_key.strip()) > 10

def format_error_message(error):
    """Format error message"""
    return str(error)

def generate_with_official_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None):
    """ä½¿ç”¨å®˜æ–¹google.genaiåº“è°ƒç”¨API"""
    try:
        # å°è¯•å¯¼å…¥å®˜æ–¹åº“
        from google import genai
        from google.genai import types

        print(f"ğŸš€ ä½¿ç”¨å®˜æ–¹google.genaiåº“è°ƒç”¨æ¨¡å‹: {model}")

        # åˆ›å»ºå®¢æˆ·ç«¯
        client = genai.Client(api_key=api_key)

        # è½¬æ¢generation_configæ ¼å¼
        config_params = {
            'temperature': generation_config.get('temperature', 0.7),
            'top_p': generation_config.get('topP', 0.95),
            'top_k': generation_config.get('topK', 40),
            'max_output_tokens': generation_config.get('maxOutputTokens', 8192),
        }

        # å¤„ç†responseModalities
        if 'responseModalities' in generation_config:
            config_params['response_modalities'] = generation_config['responseModalities']
        else:
            config_params['response_modalities'] = ['Text', 'Image']

        # å¤„ç†imageConfigï¼ˆaspect_ratioï¼‰
        if 'imageConfig' in generation_config and 'aspectRatio' in generation_config['imageConfig']:
            config_params['image_config'] = types.ImageConfig(
                aspect_ratio=generation_config['imageConfig']['aspectRatio']
            )

        # å¤„ç†seed
        if 'seed' in generation_config and generation_config['seed'] > 0:
            config_params['seed'] = generation_config['seed']

        official_config = types.GenerateContentConfig(**config_params)

        # è½¬æ¢content_partsæ ¼å¼ï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œä¸gemini_bananaæ¨¡å—ä¿æŒä¸€è‡´ï¼‰
        official_parts = []
        for part in content_parts:
            if 'text' in part:
                official_parts.append({"text": part['text']})
            elif 'inline_data' in part:
                official_parts.append({
                    "inline_data": {
                        "mime_type": part['inline_data']['mime_type'],
                        "data": part['inline_data']['data']
                    }
                })

        # è°ƒç”¨APIï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
        response = client.models.generate_content(
            model=model,
            contents=[{"parts": official_parts}],
            config=official_config
        )

        # è½¬æ¢å“åº”æ ¼å¼ä¸ºREST APIå…¼å®¹æ ¼å¼
        result = {
            "candidates": [{
                "content": {
                    "parts": []
                }
            }]
        }

        # å¤„ç†å“åº”å†…å®¹
        if hasattr(response, 'candidates') and response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, 'content') and candidate.content and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # å¤„ç†æ–‡æœ¬å†…å®¹
                        if hasattr(part, 'text') and part.text:
                            result["candidates"][0]["content"]["parts"].append({
                                "text": part.text
                            })
                        # å¤„ç†å›¾åƒå†…å®¹
                        elif hasattr(part, 'inline_data') and part.inline_data:
                            try:
                                # ç¡®ä¿inline_dataä¸ä¸ºNoneä¸”æœ‰å¿…è¦çš„å±æ€§
                                if hasattr(part.inline_data, 'mime_type') and hasattr(part.inline_data, 'data'):
                                    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                                    data = part.inline_data.data
                                    if hasattr(data, 'decode'):
                                        # å¦‚æœæ˜¯bytesï¼Œè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
                                        import base64
                                        data = base64.b64encode(data).decode('utf-8')
                                    elif isinstance(data, str):
                                        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                                        data = data

                                    result["candidates"][0]["content"]["parts"].append({
                                        "inline_data": {
                                            "mime_type": part.inline_data.mime_type,
                                            "data": data
                                        }
                                    })
                            except Exception as e:
                                print(f"âš ï¸ å¤„ç†å›¾åƒå“åº”æ—¶å‡ºé”™: {e}")

        # å¦‚æœæ²¡æœ‰ä»candidatesä¸­è·å–åˆ°å†…å®¹ï¼Œå°è¯•ä½¿ç”¨response.text
        if not result["candidates"][0]["content"]["parts"] and hasattr(response, 'text') and response.text:
            result["candidates"][0]["content"]["parts"].append({
                "text": response.text
            })

        print("âœ… å®˜æ–¹APIè°ƒç”¨æˆåŠŸ")
        return result

    except ImportError:
        print("âš ï¸ google.genaiåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å®˜æ–¹API")
        return None
    except Exception as e:
        print(f"âŒ å®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
        # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def generate_with_rest_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None, base_url=None):
    """ä½¿ç”¨REST APIè°ƒç”¨Gemini"""
    import requests

    if not base_url:
        base_url = "https://generativelanguage.googleapis.com"

    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "contents": [{
            "parts": content_parts
        }],
        "generationConfig": generation_config
    }

    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key.strip()
    }

    # è®¾ç½®ä»£ç†
    proxies = None
    if proxy and proxy.strip() and "None" not in proxy:
        proxies = {
            'http': proxy.strip(),
            'https': proxy.strip()
        }

    for attempt in range(max_retries):
        try:
            print(f"ğŸŒ REST APIè°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries})")

            response = requests.post(
                f"{base_url}/v1beta/models/{model}:generateContent",
                headers=headers,
                json=request_data,
                timeout=120,
                proxies=proxies
            )

            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ REST APIé”™è¯¯ {response.status_code}: {response.text}")
                if attempt == max_retries - 1:
                    raise Exception(f"REST APIè°ƒç”¨å¤±è´¥: {response.status_code}")

        except Exception as e:
            print(f"âŒ REST APIè°ƒç”¨å¼‚å¸¸: {e}")
            if attempt == max_retries - 1:
                raise e

        # é‡è¯•å»¶è¿Ÿ
        import time
        time.sleep(2 ** attempt)

    return None

def generate_with_priority_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None, base_url=None):
    """ä¼˜å…ˆä½¿ç”¨å®˜æ–¹APIï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST API"""

    # é¦–å…ˆå°è¯•å®˜æ–¹API
    print("ğŸ¯ ä¼˜å…ˆå°è¯•å®˜æ–¹google.genai API")
    result = generate_with_official_api(api_key, model, content_parts, generation_config, max_retries, proxy)

    if result is not None:
        print("âœ… å®˜æ–¹APIè°ƒç”¨æˆåŠŸ")
        return result

    # å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°REST API
    print("ğŸ”„ å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°REST API")
    return generate_with_rest_api(api_key, model, content_parts, generation_config, max_retries, proxy, base_url)

def extract_text_from_response(response_json):
    """ä»å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹"""
    try:
        if not response_json or "candidates" not in response_json:
            return "Response received but no candidates"

        candidates = response_json["candidates"]
        if not candidates:
            return "Response received but no candidates"

        candidate = candidates[0]
        if "content" not in candidate:
            return "Response received but no content"

        content = candidate["content"]
        if "parts" not in content:
            return "Response received but no parts"

        parts = content["parts"]
        text_parts = []

        for part in parts:
            if "text" in part:
                text_parts.append(part["text"])

        if text_parts:
            return "\n".join(text_parts)
        else:
            return "Response received but no text content"

    except Exception as e:
        print(f"âŒ æå–æ–‡æœ¬å“åº”å¤±è´¥: {e}")
        return f"Error extracting text: {str(e)}"

def process_generated_image_from_response(response_json):
    """ä»å“åº”ä¸­æå–ç”Ÿæˆçš„å›¾åƒ"""
    try:
        if not response_json or "candidates" not in response_json:
            print(f"âš ï¸ å“åº”æ ¼å¼é”™è¯¯: response_json={'å­˜åœ¨' if response_json else 'ä¸å­˜åœ¨'}, candidates={'å­˜åœ¨' if response_json and 'candidates' in response_json else 'ä¸å­˜åœ¨'}")
            if response_json:
                print(f"ğŸ“‹ å“åº”ç»“æ„: {list(response_json.keys())}")
            return None

        candidates = response_json["candidates"]
        if not candidates:
            print("âš ï¸ candidatesä¸ºç©º")
            return None

        candidate = candidates[0]
        if "content" not in candidate:
            print(f"âš ï¸ candidateä¸­æ²¡æœ‰content, ç»“æ„: {list(candidate.keys())}")
            return None

        content = candidate["content"]
        if "parts" not in content:
            print(f"âš ï¸ contentä¸­æ²¡æœ‰parts, ç»“æ„: {list(content.keys())}")
            return None

        parts = content["parts"]
        print(f"ğŸ“‹ partsæ•°é‡: {len(parts)}")

        for i, part in enumerate(parts):
            print(f"ğŸ“‹ part[{i}]ç»“æ„: {list(part.keys())}")
            # æ”¯æŒä¸¤ç§å‘½åæ–¹å¼ï¼šinline_dataï¼ˆä¸‹åˆ’çº¿ï¼‰å’Œ inlineDataï¼ˆé©¼å³°ï¼‰
            inline_data = part.get("inline_data") or part.get("inlineData")
            if inline_data:
                print(f"ğŸ“‹ inline_dataç»“æ„: {list(inline_data.keys())}")
                if "data" in inline_data:
                    # è§£ç base64å›¾åƒæ•°æ®
                    import base64
                    import io
                    from PIL import Image

                    image_data = inline_data["data"]
                    print(f"ğŸ“‹ å›¾åƒæ•°æ®é•¿åº¦: {len(image_data)} å­—ç¬¦")
                    image_bytes = base64.b64decode(image_data)
                    image = Image.open(io.BytesIO(image_bytes))
                    print("âœ… æˆåŠŸæå–ç”Ÿæˆçš„å›¾åƒ")
                    return image

        print("âš ï¸ æ‰€æœ‰partsä¸­éƒ½æ²¡æœ‰æ‰¾åˆ°inline_data.dataæˆ–inlineData.data")
        return None

    except Exception as e:
        print(f"âŒ æå–å›¾åƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def _normalize_model_name(model: str) -> str:
    """Strip any trailing bracketed labels (e.g., ' [OpenRouter]', ' [Comflyâ€‘T8]'), robust to hyphen variants."""
    try:
        if not model:
            return model
        import re
        # Remove any ' [....]' suffix at end, including unicode hyphen variants inside
        return re.sub(r"\s*\[[^\]]+\]\s*$", "", model)
    except Exception:
        return model

def _is_comfly_base(url: str) -> bool:
    try:
        return isinstance(url, str) and ("ai.comfly.chat" in url or "comfly.chat" in url)
    except Exception:
        return False

def _parse_size_str(size_str: str, default: str = "1024x1024") -> str:
    try:
        s = size_str.strip().lower()
        if "x" in s:
            w, h = s.split("x", 1)
            w = int(w); h = int(h)
            return f"{w}x{h}"
    except Exception:
        pass
    return default

def _comfly_nano_banana_generate(api_url: str, api_key: str, model: str, prompt: str, size: str, temperature: float = 1.0, top_p: float = 0.95, max_tokens: int = 32768, seed: int = 0) -> dict:
    """Call images/generations endpoint for nano-banana image generation (supports both Comfly and T8 mirror sites)."""
    import requests, json, os, re, time

    # è°ƒè¯•ä¿¡æ¯ - å·²å…³é—­
    # print(f"[DEBUG] _comfly_nano_banana_generate called with:")
    # print(f"[DEBUG]   api_url: {api_url}")
    # print(f"[DEBUG]   model: {model}")
    # print(f"[DEBUG]   prompt: {prompt[:100]}...")
    # print(f"[DEBUG]   size: {size}")
    # print(f"[DEBUG]   seed: {seed}")
    # å¯¹äºå›¾åƒç”Ÿæˆï¼Œä½¿ç”¨ /v1/images/generations ç«¯ç‚¹
    if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
        # T8é•œåƒç«™çš„æ­£ç¡®URL
        url = "https://ai.t8star.cn/v1/images/generations"
    elif api_url.endswith('/v1/chat/completions'):
        url = api_url.replace('/v1/chat/completions', '/v1/images/generations')
    else:
        # Comflyæˆ–å…¶ä»–å…¼å®¹çš„é•œåƒç«™
        url = "https://ai.comfly.chat/v1/images/generations"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # æ„å»ºå›¾åƒç”Ÿæˆçš„è¯·æ±‚æ ¼å¼ï¼ˆå®Œå…¨æŒ‰ç…§Comflyé¡¹ç›®çš„æ ¼å¼ï¼‰
    payload = {
        "prompt": prompt,
        "model": str(model)
    }

    # æ·»åŠ å¯é€‰å‚æ•°ï¼ˆæŒ‰ç…§Comflyé¡¹ç›®çš„æ–¹å¼ï¼‰
    if size:
        # å¤„ç†ç‰¹æ®Šå°ºå¯¸æ ¼å¼ï¼ŒT8é•œåƒç«™éœ€è¦å…·ä½“çš„å°ºå¯¸æ ¼å¼
        if size == "Original size":
            # é»˜è®¤ä½¿ç”¨1024x1024ä½œä¸ºåŸå§‹å°ºå¯¸
            payload["size"] = "1024x1024"
            # print(f"[DEBUG] è½¬æ¢ 'Original size' -> '1024x1024'")
        else:
            payload["size"] = size

    # ä½¿ç”¨urlæ ¼å¼è€Œä¸æ˜¯b64_jsonï¼ˆæ ¹æ®ç”¨æˆ·åé¦ˆï¼‰
    payload["response_format"] = "url"

    if seed > 0:
        payload["seed"] = seed

    # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®é™…çš„è¯·æ±‚payload - å·²å…³é—­
    # print(f"[DEBUG] Request payload: {json.dumps(payload, indent=2)}")
    # print(f"[DEBUG] Target URL: {url}")

    # å‘é€å›¾åƒç”Ÿæˆè¯·æ±‚ï¼ˆéæµå¼ï¼‰
    session = requests.Session()
    max_retries = 1
    backoff_seconds = 1

    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            try:
                proxy = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy") or os.environ.get("HTTP_PROXY") or os.environ.get("http_proxy")
                if proxy:
                    print(f"Use Proxy: {proxy}")
            except Exception:
                pass

            print(f"[ComflyNanoBananaMirror] (generate) POST attempt {attempt}/{max_retries} â†’ {url}")

            response = session.post(
                url,
                headers=headers,
                json=payload,
                timeout=(20, 120),
                allow_redirects=True,
            )
            print(f"[ComflyNanoBananaMirror] HTTP status: {response.status_code}")
            try:
                print(f"[ComflyNanoBananaMirror] Content-Type: {response.headers.get('Content-Type','')}")
            except Exception:
                pass
            if response.status_code != 200:
                try:
                    # print(f"[ComflyNanoBananaMirror] Error body: {response.text[:500]}")  # æ³¨é‡Šæ‰å†—é•¿çš„è°ƒè¯•ä¿¡æ¯
                    print(f"[ComflyNanoBananaMirror] Error status: {response.status_code}")
                except Exception:
                    pass
            response.raise_for_status()

            # å¤„ç†å›¾åƒç”ŸæˆAPIçš„JSONå“åº”
            try:
                result = response.json()
                print(f"[ComflyNanoBananaMirror] Response received: {list(result.keys())}")

                # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒæ•°æ®
                if 'data' in result and result['data']:
                    # OpenAI Images APIæ ¼å¼
                    image_data = result['data'][0]

                    # ä¼˜å…ˆå¤„ç†URLæ ¼å¼ï¼ˆæ ¹æ®ç”¨æˆ·åé¦ˆï¼‰
                    if 'url' in image_data:
                        image_url = image_data['url']
                        # print(f"[DEBUG] ç”Ÿæˆçš„å›¾åƒURL: {image_url}")

                        # ä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸ºbase64
                        import base64
                        from io import BytesIO
                        img_response = session.get(image_url, timeout=30)
                        img_response.raise_for_status()
                        b64_data = base64.b64encode(img_response.content).decode('utf-8')

                        response_text = f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œæ¨¡å‹: {model}\nå›¾åƒURL: {image_url}"
                        # print(f"[DEBUG] å“åº”æ–‡æœ¬: {response_text}")

                        return {
                            "data": [{
                                "b64_json": b64_data,
                                "url": image_url,
                                "revised_prompt": image_data.get('revised_prompt', prompt)
                            }],
                            "response_text": response_text
                        }
                    elif 'b64_json' in image_data:
                        # å¤„ç†base64æ•°æ®ï¼Œå¯èƒ½åŒ…å«data URLå‰ç¼€
                        b64_data = image_data['b64_json']
                        if b64_data.startswith('data:image/'):
                            # æå–çº¯base64éƒ¨åˆ†
                            b64_data = b64_data.split(',', 1)[1] if ',' in b64_data else b64_data

                        return {
                            "data": [{
                                "b64_json": b64_data,
                                "url": "",
                                "revised_prompt": image_data.get('revised_prompt', prompt)
                            }],
                            "response_text": f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œæ¨¡å‹: {model}"
                        }

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾åƒæ•°æ®ï¼Œè¿”å›ç©ºç»“æœ
                print(f"[ComflyNanoBananaMirror] No image data found in response: {result}")
                return {
                    "data": [],
                    "response_text": f"å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œå“åº”: {str(result)[:200]}"
                }

            except json.JSONDecodeError as e:
                print(f"[ComflyNanoBananaMirror] JSON decode error: {e}")
                return {
                    "data": [],
                    "response_text": f"å“åº”è§£æå¤±è´¥: {response.text[:200]}"
                }

        except requests.exceptions.HTTPError as http_err:
            status = getattr(http_err.response, 'status_code', None)
            last_error = http_err
            if status in (408, 409, 429) or (status is not None and 500 <= status < 600):
                if attempt < max_retries:
                    time.sleep(backoff_seconds * attempt)
                    continue
            raise Exception(f"Error in nano-banana generation: {str(http_err)}")
        except requests.exceptions.Timeout as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception("Error in nano-banana generation: request timed out")
        except Exception as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception(f"Error in nano-banana generation: {str(e)}")

    raise Exception(f"Error in nano-banana generation: {str(last_error)}")

def _comfly_nano_banana_edit(api_url: str, api_key: str, model: str, prompt: str, pil_images: list, size: str, temperature: float = 1.0, top_p: float = 0.95, max_tokens: int = 32768, seed: int = 0) -> dict:
    """Call chat completions endpoint for nano-banana image editing with multiple images (supports both Comfly and T8 mirror sites)."""
    import requests, json, io, base64, re, time, os
    # ä½¿ç”¨ä¼ å…¥çš„api_urlï¼Œå¦‚æœå·²ç»æ˜¯å®Œæ•´URLåˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™æ„å»ºå®Œæ•´URL
    if api_url.endswith('/v1/chat/completions'):
        url = api_url
    elif "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
        url = f"{api_url.rstrip('/')}/v1/chat/completions"
    else:
        # Comflyæˆ–å…¶ä»–å…¼å®¹çš„é•œåƒç«™
        url = "https://ai.comfly.chat/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    # æ„å»ºcontentæ•°ç»„ï¼ŒåŒ…å«æ–‡æœ¬å’Œæ‰€æœ‰å›¾åƒ
    content = [{"type": "text", "text": prompt}]
    
    # æ·»åŠ æ‰€æœ‰è¾“å…¥å›¾åƒï¼ˆä½¿ç”¨åŸå›¾å°ºå¯¸ä¸æ— æŸPNGç¼–ç ï¼‰
    for pil_image in pil_images:
        if pil_image is not None:
            try:
                w, h = pil_image.size
                print(f"[ComflyNanoBananaMirror] Use original image size: {w}x{h}")
                buffered = io.BytesIO()
                pil_image.save(buffered, format="PNG")
                image_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{image_base64}"}
                })
            except Exception as e:
                try:
                    print(f"[ComflyNanoBananaMirror] Image encode error: {str(e)}")
                except Exception:
                    pass
    
    # æ„å»ºnano-bananaçš„å›¾åƒç¼–è¾‘è¯·æ±‚æ ¼å¼
    messages = [{
        "role": "user",
        "content": content
    }]
    
    payload = {
        "model": str(model),
        "messages": messages,
        "temperature": float(temperature),
        "top_p": float(top_p),
        "max_tokens": int(max_tokens),
        "stream": True
    }
    
    if seed > 0:
        payload["seed"] = seed
    
    # è°ƒè¯•ä¿¡æ¯
    try:
        print(f"[ComflyNanoBananaMirror] Building payload: model={model}, max_tokens={max_tokens}, seed={seed}")
        print(f"[ComflyNanoBananaMirror] Payload keys: {list(payload.keys())}")
        # print(f"[ComflyNanoBananaMirror] Messages structure: {payload.get('messages', 'MISSING')}")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
        messages = payload.get('messages', [])
        if messages:
            print(f"[ComflyNanoBananaMirror] Messages count: {len(messages)}")
            for i, msg in enumerate(messages):
                if 'content' in msg and isinstance(msg['content'], list):
                    content_types = [item.get('type', 'unknown') for item in msg['content']]
                    print(f"[ComflyNanoBananaMirror] Message {i+1} content types: {content_types}")
                else:
                    print(f"[ComflyNanoBananaMirror] Message {i+1} type: {type(msg.get('content', 'unknown'))}")
        else:
            print(f"[ComflyNanoBananaMirror] No messages found")
    except Exception:
        pass

    # å‘é€æµå¼è¯·æ±‚ï¼ˆå¯¹408ç­‰å¯é‡è¯•é”™è¯¯è¿›è¡Œé‡è¯•ï¼‰
    full_response = ""
    session = requests.Session()
    max_retries = 3
    backoff_seconds = 2

    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            # æ‰“å°ä»£ç†ä¿¡æ¯ï¼ˆè‹¥å­˜åœ¨ï¼‰
            try:
                proxy = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy") or os.environ.get("HTTP_PROXY") or os.environ.get("http_proxy")
                if proxy:
                    print(f"Use Proxy: {proxy}")
            except Exception:
                pass

            print(f"[ComflyNanoBananaMirror] POST attempt {attempt}/{max_retries} â†’ {url}")
            response = session.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=(20, 120),  # è¿æ¥/è¯»å–è¶…æ—¶åˆ†ç¦»
                allow_redirects=True,
            )
            print(f"[ComflyNanoBananaMirror] HTTP status: {response.status_code}")
            try:
                print(f"[ComflyNanoBananaMirror] Content-Type: {response.headers.get('Content-Type','')}")
            except Exception:
                pass
            # è‹¥é200ï¼Œæ‰“å°è¿”å›ä½“å¸®åŠ©å®šä½422åŸå› 
            if response.status_code != 200:
                try:
                    # print(f"[ComflyNanoBananaMirror] Error body: {response.text[:500]}")  # æ³¨é‡Šæ‰å†—é•¿çš„è°ƒè¯•ä¿¡æ¯
                    print(f"[ComflyNanoBananaMirror] Error status: {response.status_code}")
                except Exception:
                    pass
            response.raise_for_status()

            line_count = 0
            received_any = False
            ctype = (response.headers.get('Content-Type') or '').lower()
            if 'text/event-stream' in ctype or 'stream' in ctype:
                for line in response.iter_lines(chunk_size=1, decode_unicode=False):
                    if line is None or not line:
                        continue
                    received_any = True
                    try:
                        line_text = line.decode('utf-8', errors='ignore').strip()
                    except Exception:
                        line_text = str(line).strip()
                    line_count += 1
                    if not line_text.startswith('data: '):
                        continue
                    data = line_text[6:]
                    if data == '[DONE]':
                        print(f"[ComflyNanoBananaMirror] Stream finished. Total SSE lines: {line_count}")
                        break
                    try:
                        chunk = json.loads(data)
                    except json.JSONDecodeError:
                        continue

                    if 'choices' in chunk and chunk['choices']:
                        choice0 = chunk['choices'][0]
                        delta = choice0.get('delta', {})
                        piece = delta.get('content') or (choice0.get('message', {}) or {}).get('content')
                        if piece:
                            full_response += piece
                            try:
                                print(f"[ComflyNanoBananaMirror] +{len(piece)} chars (total {len(full_response)})")
                            except Exception:
                                pass
            else:
                # éæµå¼ï¼šç›´æ¥è§£æä¸€æ¬¡æ€§JSON
                try:
                    body = response.content.decode('utf-8', errors='ignore')
                except Exception:
                    body = response.text
                try:
                    obj = json.loads(body)
                    received_any = True
                    if isinstance(obj, dict):
                        choices = obj.get('choices') or []
                        if choices:
                            msg = choices[0].get('message', {})
                            content_text = isinstance(msg, dict) and msg.get('content')
                            if isinstance(content_text, str):
                                full_response += content_text
                except Exception:
                    pass

            if not received_any:
                print("[ComflyNanoBananaMirror] Warning: no bytes received from stream; fallback to non-streaming request")
                # Fallback once with non-streaming
                try:
                    resp2 = session.post(
                        url,
                        headers=headers,
                        json={**payload, "stream": False},
                        timeout=(20, 120),
                        allow_redirects=True,
                    )
                    if resp2.status_code != 200:
                        try:
                            print(f"[ComflyNanoBananaMirror] Fallback error body: {resp2.text[:500]}")
                        except Exception:
                            pass
                    resp2.raise_for_status()
                    obj = resp2.json()
                    choices = obj.get('choices') or []
                    if choices:
                        msg = choices[0].get('message', {})
                        content_text = isinstance(msg, dict) and msg.get('content')
                        if isinstance(content_text, str):
                            full_response += content_text
                except Exception as _:
                    pass

            # ä¼˜å…ˆæå–base64å›¾ç‰‡
            base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
            matches = re.findall(base64_pattern, full_response)
            if matches:
                return {
                    "data": [{
                        "b64_json": matches[0],
                        "url": "",
                        "revised_prompt": prompt
                    }],
                    "response_text": full_response
                }

            # å…¶æ¬¡æå–å›¾ç‰‡URL
            image_md_pattern = r'!\[.*?\]\((.*?)\)'
            url_matches = re.findall(image_md_pattern, full_response)
            if not url_matches:
                url_pattern = r'https?://\S+\.(?:jpg|jpeg|png|gif|webp)'
                url_matches = re.findall(url_pattern, full_response)
            if not url_matches:
                all_urls_pattern = r'https?://\S+'
                url_matches = re.findall(all_urls_pattern, full_response)

            if url_matches:
                image_url = url_matches[0]
                try:
                    print(f"[ComflyNanoBananaMirror] Found image URL: {image_url}")
                except Exception:
                    pass
                try:
                    img_response = requests.get(image_url, timeout=30)
                    img_response.raise_for_status()
                    # åå¤„ç†ï¼šæŒ‰ size ç»Ÿä¸€é€‚é…è¾“å‡ºå°ºå¯¸ï¼ˆä¿éšœå°ºå¯¸æ§åˆ¶å¿…ç”Ÿæ•ˆï¼‰
                    try:
                        from io import BytesIO
                        from PIL import Image as _PILImage
                        raw_img = _PILImage.open(BytesIO(img_response.content)).convert('RGB')
                        target_size = size if isinstance(size, str) else str(size)
                        if 'x' in target_size:
                            tw, th = map(int, target_size.split('x'))
                            # ä½¿ç”¨ç­‰æ¯”æ‰©å±•+ç•™ç™½ï¼Œé¿å…æ‹‰ä¼¸
                            processed = smart_resize_with_padding(raw_img, (tw, th))
                        else:
                            processed = raw_img
                        buf = BytesIO()
                        processed.save(buf, format='JPEG', quality=95)
                        image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
                        try:
                            print(f"ğŸ”§ Final output size: {processed.size[0]}x{processed.size[1]}")
                        except Exception:
                            pass
                    except Exception:
                        # å›é€€ï¼šç›´æ¥é€ä¼ 
                        image_base64 = base64.b64encode(img_response.content).decode('utf-8')

                    return {
                        "data": [{
                            "b64_json": image_base64,
                            "url": image_url,
                            "revised_prompt": prompt
                        }],
                        "response_text": full_response
                    }
                except Exception:
                    return {
                        "data": [{
                            "b64_json": "",
                            "url": image_url,
                            "revised_prompt": prompt
                        }],
                        "response_text": full_response
                    }

            # æ²¡æœ‰å›¾ç‰‡ï¼Œä»…è¿”å›å“åº”æ–‡æœ¬
            return {
                "data": [{
                    "b64_json": "",
                    "url": "",
                    "revised_prompt": prompt
                }],
                "response_text": full_response
            }

        except requests.exceptions.HTTPError as http_err:
            status = getattr(http_err.response, 'status_code', None)
            try:
                body = http_err.response.text if http_err.response is not None else ''
                if body:
                    print(f"[ComflyNanoBananaMirror] HTTP {status} body: {body[:1000]}")
            except Exception:
                pass
            last_error = http_err
            # å¯¹408/429/5xxè¿›è¡Œé‡è¯•
            if status in (408, 409, 429) or (status is not None and 500 <= status < 600):
                if attempt < max_retries:
                    time.sleep(backoff_seconds * attempt)
                    continue
            raise Exception(f"Error in nano-banana image editing: {str(http_err)}")
        except requests.exceptions.Timeout as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception("Error in nano-banana image editing: request timed out")
        except Exception as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception(f"Error in nano-banana image editing: {str(e)}")

    # å¦‚æœåˆ°è¿™é‡Œä»ç„¶å¤±è´¥
    raise Exception(f"Error in nano-banana image editing: {str(last_error)}")

def _comfly_falai_edit(api_url: str, api_key: str, model: str, prompt: str, pil_image: Image.Image, size: str) -> dict:
    """Call Comfly's OpenAI-compatible images edits endpoint for FAL AI models."""
    import requests, io
    url = api_url.rstrip('/') + "/v1/images/edits"
    headers = {"Authorization": f"Bearer {api_key}"}
    buf = io.BytesIO()
    pil_image.save(buf, format='PNG')
    buf.seek(0)
    data = {
        "model": _normalize_model_name(model),
        "prompt": prompt,
        "size": _parse_size_str(size)
    }
    files = {"image": ("image.png", buf, "image/png")}
    r = requests.post(url, headers=headers, data=data, files=files, timeout=180)
    r.raise_for_status()
    return r.json()

def _comfly_fal_ai_nano_banana(api_url: str, api_key: str, model: str, prompt: str, images: list = None, num_images: int = 1, seed: int = 0, image_way: str = "image_url") -> dict:
    """Call Comfly's fal-ai/nano-banana endpoint for image generation and editing."""
    import requests, io, base64, time
    from PIL import Image

    # ç¡®å®šæ˜¯ç¼–è¾‘æ¨¡å¼è¿˜æ˜¯ç”Ÿæˆæ¨¡å¼
    is_edit_mode = model.endswith("/edit") or (images and len(images) > 0)

    # æ„å»ºAPIç«¯ç‚¹ - å¤„ç†æ¨¡å‹åç§°ï¼Œé¿å…é‡å¤çš„fal-aiå‰ç¼€
    clean_model = model
    if model.startswith("fal-ai/"):
        clean_model = model[7:]  # ç§»é™¤ "fal-ai/" å‰ç¼€

    if is_edit_mode and not clean_model.endswith("/edit"):
        clean_model = f"{clean_model}/edit"

    # æ„å»ºæ­£ç¡®çš„APIç«¯ç‚¹ - åŒºåˆ†Comflyå’ŒT8é•œåƒç«™
    base_url = api_url.rstrip('/')

    # æ£€æŸ¥æ˜¯å¦æ˜¯T8é•œåƒç«™
    if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
        # T8é•œåƒç«™ä½¿ç”¨fal-aiç«¯ç‚¹ï¼Œæ ¼å¼ï¼šhttps://ai.t8star.cn/fal-ai/{model}
        if base_url.endswith('/v1/chat/completions'):
            base_url = base_url[:-20]  # ç§»é™¤/v1/chat/completionsåç¼€
        elif base_url.endswith('/v1'):
            base_url = base_url[:-3]  # ç§»é™¤/v1åç¼€
        api_endpoint = f"{base_url}/fal-ai/{clean_model}"
    else:
        # Comflyé•œåƒç«™ä½¿ç”¨ä¸“é—¨çš„fal-aiç«¯ç‚¹
        if base_url.endswith('/v1'):
            base_url = base_url[:-3]  # ç§»é™¤/v1åç¼€
        api_endpoint = f"{base_url}/fal-ai/{clean_model}"

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }

    # å‡†å¤‡payload - T8å’ŒComflyé•œåƒç«™éƒ½ä½¿ç”¨ç›¸åŒçš„fal-aiæ ¼å¼
    payload = {
        "prompt": prompt,
        "num_images": num_images
    }

    if seed > 0:
        payload["seed"] = seed

    # å¤„ç†è¾“å…¥å›¾åƒ - T8å’ŒComflyéƒ½ä½¿ç”¨ç›¸åŒçš„å¤„ç†æ–¹å¼
    if images and len(images) > 0:
        image_urls = []

        if image_way == "image":
            # ä½¿ç”¨base64ç¼–ç 
            for img in images:
                if isinstance(img, Image.Image):
                    buffered = io.BytesIO()
                    img.save(buffered, format="PNG")
                    base64_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
                    image_urls.append(f"data:image/png;base64,{base64_str}")
        else:
            # ä¸Šä¼ å›¾åƒè·å–URL
            for img in images:
                if isinstance(img, Image.Image):
                    try:
                        buffered = io.BytesIO()
                        img.save(buffered, format="PNG")
                        file_content = buffered.getvalue()

                        files = {'file': ('image.png', file_content, 'image/png')}
                        upload_headers = {"Authorization": f"Bearer {api_key}"}

                        # æ„å»ºæ­£ç¡®çš„ä¸Šä¼ URL
                        base_url = api_url.rstrip('/')
                        if base_url.endswith('/v1'):
                            upload_url = f"{base_url}/files"
                        else:
                            upload_url = f"{base_url}/v1/files"

                        print(f"ğŸ“¤ ä¸Šä¼ å›¾åƒåˆ°: {upload_url}")
                        upload_response = requests.post(
                            upload_url,
                            headers=upload_headers,
                            files=files,
                            timeout=60,
                            verify=False
                        )

                        print(f"ğŸ“¡ ä¸Šä¼ å“åº”çŠ¶æ€ç : {upload_response.status_code}")
                        if upload_response.status_code == 200:
                            upload_result = upload_response.json()
                            print(f"ğŸ“‹ ä¸Šä¼ å“åº”: {upload_result}")
                            if 'url' in upload_result:
                                image_urls.append(upload_result['url'])
                                print(f"âœ… å›¾åƒä¸Šä¼ æˆåŠŸ: {upload_result['url']}")
                            else:
                                print(f"âŒ ä¸Šä¼ å“åº”ä¸­æ²¡æœ‰URLå­—æ®µ: {upload_result}")
                        else:
                            print(f"âŒ å›¾åƒä¸Šä¼ å¤±è´¥: {upload_response.status_code} - {upload_response.text}")
                    except Exception as e:
                        print(f"âŒ å›¾åƒä¸Šä¼ å¼‚å¸¸: {e}")

        if image_urls:
            payload["image_urls"] = image_urls
            print(f"âœ… æˆåŠŸå‡†å¤‡äº†{len(image_urls)}ä¸ªå›¾åƒURL")
        elif images and len(images) > 0 and is_edit_mode:
            # å¦‚æœæ˜¯ç¼–è¾‘æ¨¡å¼ä½†æ²¡æœ‰æˆåŠŸä¸Šä¼ å›¾åƒï¼Œä½¿ç”¨base64ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            print("âš ï¸ å›¾åƒä¸Šä¼ å¤±è´¥ï¼Œä½¿ç”¨base64ç¼–ç ä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            image_urls = []
            for img in images:
                if isinstance(img, Image.Image):
                    buffered = io.BytesIO()
                    img.save(buffered, format="PNG")
                    base64_str = base64.b64encode(buffered.getvalue()).decode('utf-8')
                    image_urls.append(f"data:image/png;base64,{base64_str}")

            if image_urls:
                payload["image_urls"] = image_urls
                print(f"âœ… ä½¿ç”¨base64å‡†å¤‡äº†{len(image_urls)}ä¸ªå›¾åƒ")
            else:
                raise Exception("ç¼–è¾‘æ¨¡å¼éœ€è¦æä¾›å›¾åƒï¼Œä½†å›¾åƒå¤„ç†å¤±è´¥")

    # å‘é€è¯·æ±‚
    print(f"ğŸš€ å‘é€fal-aiè¯·æ±‚åˆ°: {api_endpoint}")
    # print(f"ğŸ“¦ è¯·æ±‚payload: {str(payload)[:300]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
    print(f"ğŸ“¦ è¯·æ±‚payloadç»“æ„: {list(payload.keys())}")  # åªæ˜¾ç¤ºpayloadçš„é”®å

    response = requests.post(api_endpoint, headers=headers, json=payload, timeout=300)

    print(f"ğŸ“¡ å“åº”çŠ¶æ€ç : {response.status_code}")
    if response.status_code != 200:
        print(f"âŒ è¯·æ±‚å¤±è´¥: {response.text}")
        response.raise_for_status()

    result = response.json()
    # print(f"ğŸ“‹ åˆå§‹å“åº”: {str(result)[:300]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„å†—é•¿è¾“å‡º
    print(f"ğŸ“‹ åˆå§‹å“åº”ç»“æ„: {list(result.keys())}")  # åªæ˜¾ç¤ºå“åº”çš„é”®å

    # T8å’ŒComflyé•œåƒç«™éƒ½ä½¿ç”¨ç›¸åŒçš„fal-aiå“åº”å¤„ç†é€»è¾‘
    # æ£€æŸ¥æ˜¯å¦æœ‰request_idï¼Œéœ€è¦è½®è¯¢ç»“æœ
    if "request_id" in result:
        request_id = result["request_id"]
        response_url = result.get("response_url", "")
        print(f"ğŸ†” è·å¾—request_id: {request_id}")
        print(f"ğŸ”— åŸå§‹response_url: {response_url}")

        # ä¿®æ­£response_url
        if "queue.fal.run" in response_url:
            response_url = response_url.replace("https://queue.fal.run", "https://ai.comfly.chat")

        if not response_url:
            # æ„å»ºæ­£ç¡®çš„è½®è¯¢URL
            base_url = api_url.rstrip('/')
            if base_url.endswith('/v1'):
                base_url = base_url[:-3]  # ç§»é™¤/v1åç¼€
            response_url = f"{base_url}/fal-ai/{clean_model}/requests/{request_id}"

        # è½®è¯¢ç»“æœ
        max_retries = 30  # 30æ¬¡è½®è¯¢ï¼Œçº¦1åˆ†é’Ÿ
        retry_count = 0
        result_data = None
        print(f"ğŸ”„ å¼€å§‹è½®è¯¢ç»“æœï¼ŒURL: {response_url}")

        while retry_count < max_retries:
            retry_count += 1

            try:
                result_response = requests.get(response_url, headers=headers, timeout=60)

                if result_response.status_code != 200:
                    time.sleep(1)
                    continue

                result_data = result_response.json()
                # print(f"ğŸ” è½®è¯¢å“åº” #{retry_count}: {str(result_data)[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                print(f"ğŸ” è½®è¯¢å“åº” #{retry_count}: {list(result_data.keys())}")  # åªæ˜¾ç¤ºå“åº”çš„é”®å

                # æ£€æŸ¥å¤šç§å¯èƒ½çš„å“åº”æ ¼å¼
                images_found = False
                images_data = []

                # æ ¼å¼1: æ ‡å‡†çš„imageså­—æ®µ
                if "images" in result_data and result_data["images"]:
                    print("ğŸ“¸ æ£€æµ‹åˆ°æ ‡å‡†imageså­—æ®µ")
                    for img_data in result_data["images"]:
                        if "url" in img_data:
                            img_url = img_data["url"]
                            if "queue.fal.run" in img_url:
                                img_url = img_url.replace("https://queue.fal.run", "https://ai.comfly.chat")

                            try:
                                img_response = requests.get(img_url, timeout=60)
                                if img_response.status_code == 200:
                                    img_base64 = base64.b64encode(img_response.content).decode('utf-8')
                                    images_data.append({
                                        "b64_json": img_base64,
                                        "url": img_url
                                    })
                                    images_found = True
                            except Exception as e:
                                print(f"Error downloading image: {e}")

                # æ ¼å¼2: æ£€æŸ¥dataå­—æ®µ
                elif "data" in result_data and result_data["data"]:
                    print("ğŸ“¸ æ£€æµ‹åˆ°dataå­—æ®µ")
                    data = result_data["data"]
                    if isinstance(data, list) and len(data) > 0:
                        for item in data:
                            if isinstance(item, dict) and "url" in item:
                                img_url = item["url"]
                                if "queue.fal.run" in img_url:
                                    img_url = img_url.replace("https://queue.fal.run", "https://ai.comfly.chat")

                                try:
                                    img_response = requests.get(img_url, timeout=60)
                                    if img_response.status_code == 200:
                                        img_base64 = base64.b64encode(img_response.content).decode('utf-8')
                                        images_data.append({
                                            "b64_json": img_base64,
                                            "url": img_url
                                        })
                                        images_found = True
                                except Exception as e:
                                    print(f"Error downloading image: {e}")

                # æ ¼å¼3: ç›´æ¥æ£€æŸ¥urlå­—æ®µ
                elif "url" in result_data:
                    print("ğŸ“¸ æ£€æµ‹åˆ°ç›´æ¥urlå­—æ®µ")
                    img_url = result_data["url"]
                    if "queue.fal.run" in img_url:
                        img_url = img_url.replace("https://queue.fal.run", "https://ai.comfly.chat")

                    try:
                        img_response = requests.get(img_url, timeout=60)
                        if img_response.status_code == 200:
                            img_base64 = base64.b64encode(img_response.content).decode('utf-8')
                            images_data.append({
                                "b64_json": img_base64,
                                "url": img_url
                            })
                            images_found = True
                    except Exception as e:
                        print(f"Error downloading image: {e}")

                if images_found and images_data:
                    print(f"âœ… æˆåŠŸè·å–{len(images_data)}å¼ å›¾åƒ")
                    return {
                        "data": images_data,
                        "response_text": f"Successfully generated {len(images_data)} images using fal-ai/{model}"
                    }

                time.sleep(1)

            except Exception as e:
                print(f"Error fetching result: {e}")
                time.sleep(1)

        # å¦‚æœè½®è¯¢ç»“æŸä»æœªè·å¾—ç»“æœ
        if result_data is None:
            raise Exception("Failed to retrieve results after multiple attempts")
        else:
            raise Exception("No images in response: " + str(result_data))

    # ç›´æ¥è¿”å›ç»“æœçš„æƒ…å†µ
    return result


def get_mirror_site_config(mirror_site_name: str) -> Dict[str, str]:
    """æ ¹æ®é•œåƒç«™åç§°è·å–å¯¹åº”çš„é…ç½®"""
    try:
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        mirror_sites = config.get('mirror_sites', {})
        
        if mirror_site_name in mirror_sites:
            site_config = mirror_sites[mirror_site_name]
            return {
                "url": site_config.get("url", ""),
                "api_key": site_config.get("api_key", ""),
                "api_format": site_config.get("api_format", ""),
                "models": site_config.get("models", []),
                "description": site_config.get("description", "")
            }
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "url": "https://generativelanguage.googleapis.com",
                "api_key": "",
                "api_format": "gemini",
                "models": [],
                "description": ""
            }
    except Exception as e:
        _log_warning(f"Failed to get mirror site config: {e}")
        return {
            "url": "https://generativelanguage.googleapis.com",
            "api_key": "",
            "api_format": "gemini",
            "models": [],
            "description": ""
        }

def validate_openrouter_config(api_url: str, api_key: str, model: str) -> Dict[str, Any]:
    """éªŒè¯OpenRouteré…ç½®å¹¶è¿”å›ä¼˜åŒ–å»ºè®®"""
    validation_result = {
        "is_valid": True,
        "warnings": [],
        "suggestions": [],
        "optimized_params": {}
    }
    
    # éªŒè¯API URL
    if not api_url or "openrouter.ai" not in api_url:
        validation_result["is_valid"] = False
        validation_result["warnings"].append("OpenRouter API URL æ— æ•ˆ")
        return validation_result
    
    # éªŒè¯API Key
    if not api_key or not api_key.startswith("sk-or-v1-"):
        validation_result["warnings"].append("OpenRouter API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
    
    # éªŒè¯æ¨¡å‹åç§°
    if "dall-e" in model.lower():
        validation_result["optimized_params"]["size"] = ["1024x1024", "1792x1024", "1024x1792"]
        validation_result["suggestions"].append("DALL-E æ¨¡å‹æ¨èä½¿ç”¨æ ‡å‡†å°ºå¯¸ä»¥è·å¾—æœ€ä½³æ•ˆæœ")
    elif "stable-diffusion" in model.lower():
        validation_result["suggestions"].append("Stable Diffusion æ¨¡å‹å°ºå¯¸ä¼šè‡ªåŠ¨è°ƒæ•´ä¸º8çš„å€æ•°")
    elif "gemini" in model.lower():
        validation_result["suggestions"].append("Gemini æ¨¡å‹æ”¯æŒå¤šç§å°ºå¯¸å’Œè´¨é‡è®¾ç½®")
    
    return validation_result

def process_openrouter_stream(response) -> str:
    """å¤„ç†OpenRouterçš„æµå¼å“åº”"""
    accumulated_content = ""
    chunk_count = 0
    empty_chunks = 0
    content_chunks = 0
    last_content_chunk = 0
    
    print(f"ğŸ”„ å¼€å§‹å¤„ç†OpenRouteræµå¼å“åº”...")
    
    try:
        for line in response.iter_lines(decode_unicode=True, chunk_size=None):
            if line and line.startswith('data: '):
                chunk_count += 1
                data_content = line[6:]  # Remove 'data: ' prefix
                
                print(f"ğŸ“¦ å¤„ç†ç¬¬{chunk_count}ä¸ªæ•°æ®å—...")
                
                if data_content.strip() == '[DONE]':
                    print(f"âœ… æ”¶åˆ°ç»“æŸä¿¡å·[DONE]")
                    break
                
                try:
                    # å°è¯•è§£æJSON
                    chunk_data = json.loads(data_content)
                    print(f"ğŸ” æ•°æ®å—ç»“æ„: {list(chunk_data.keys())}")
                    
                    # æå–å†…å®¹
                    if 'choices' in chunk_data and chunk_data['choices']:
                        choice = chunk_data['choices'][0]
                        print(f"ğŸ” é€‰æ‹©é¡¹ç»“æ„: {list(choice.keys())}")
                        
                        if 'delta' in choice:
                            delta = choice['delta']
                            print(f"ğŸ” Deltaç»“æ„: {list(delta.keys())}")
                            
                            # æ£€æŸ¥imageså­—æ®µï¼ˆOpenRouterå¯èƒ½åœ¨è¿™é‡Œè¿”å›å›¾åƒæ•°æ®ï¼‰
                            if 'images' in delta and delta['images']:
                                print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouter imageså­—æ®µï¼")
                                images_data = delta['images']
                                print(f"ğŸ” Imageså­—æ®µç±»å‹: {type(images_data)}")
                                # print(f"ğŸ” Imageså­—æ®µå†…å®¹: {str(images_data)[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ” Imageså­—æ®µé•¿åº¦: {len(str(images_data))} å­—ç¬¦")
                                
                                # ä½¿ç”¨å‚è€ƒé¡¹ç›®çš„æ–¹æ³•ï¼šç›´æ¥æœç´¢data:image/å­—ç¬¦ä¸²
                                import re
                                chunk_str = str(images_data)
                                if 'data:image/' in chunk_str:
                                    print(f"ğŸ¯ OpenRouteråœ¨imageså­—æ®µä¸­å‘ç°å›¾ç‰‡æ•°æ®!")
                                    # ä½¿ç”¨å‚è€ƒé¡¹ç›®çš„æ­£ç¡®æ­£åˆ™è¡¨è¾¾å¼
                                    base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                    matches = re.findall(base64_pattern, chunk_str)
                                    if matches:
                                        for url in matches:
                                            print(f"ğŸ¯ OpenRouteræå–base64å›¾ç‰‡ï¼Œé•¿åº¦: {len(url)}å­—ç¬¦")
                                            accumulated_content += " " + url
                                    else:
                                        print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡æ•°æ®")
                                        # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æå–data:image/å¼€å§‹åˆ°å­—ç¬¦ä¸²ç»“æŸ
                                        start_pos = chunk_str.find('data:image/')
                                        if start_pos != -1:
                                            extracted_data = chunk_str[start_pos:]
                                            print(f"ğŸ¯ å¤‡ç”¨æ–¹æ³•æå–å›¾ç‰‡æ•°æ®ï¼Œé•¿åº¦: {len(extracted_data)}å­—ç¬¦")
                                            accumulated_content += " " + extracted_data
                                else:
                                    print(f"âš ï¸ æœªæ‰¾åˆ°data:image/æ ‡è®°")
                                
                                content_chunks += 1
                                last_content_chunk = chunk_count
                            
                            # æ£€æŸ¥contentå­—æ®µ
                            if 'content' in delta and delta['content']:
                                content = delta['content']
                                accumulated_content += content
                                content_chunks += 1
                                last_content_chunk = chunk_count
                                print(f"ğŸ“ æ·»åŠ å†…å®¹: {len(content)}å­—ç¬¦ (ç´¯è®¡: {len(accumulated_content)}å­—ç¬¦)")
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                                if '![image]' in content:
                                    print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒæ•°æ®æ ‡è®°ï¼")
                            
                            # å¦‚æœæ—¢æ²¡æœ‰imagesä¹Ÿæ²¡æœ‰contentï¼Œæ ‡è®°ä¸ºç©ºå—
                            if not ('images' in delta and delta['images']) and not ('content' in delta and delta['content']):
                                empty_chunks += 1
                                print(f"âš ï¸ ç©ºçš„deltaå— (æ— imageså’Œcontentå­—æ®µ) - è¿™æ˜¯æ­£å¸¸çš„ï¼ŒOpenRouterç”¨ç©ºå—ä¿æŒè¿æ¥")
                        
                        elif 'message' in choice:
                            message = choice['message']
                            print(f"ğŸ” Messageç»“æ„: {list(message.keys())}")
                            
                            if 'content' in message and message['content']:
                                content = message['content']
                                accumulated_content += content
                                content_chunks += 1
                                last_content_chunk = chunk_count
                                print(f"ğŸ“ æ·»åŠ æ¶ˆæ¯å†…å®¹: {len(content)}å­—ç¬¦ (ç´¯è®¡: {len(accumulated_content)}å­—ç¬¦)")
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                                if '![image]' in content:
                                    print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒæ•°æ®æ ‡è®°ï¼")
                            else:
                                empty_chunks += 1
                                print(f"âš ï¸ ç©ºçš„æ¶ˆæ¯å— (æ— contentå­—æ®µ)")
                        else:
                            empty_chunks += 1
                            print(f"âš ï¸ æœªçŸ¥çš„é€‰æ‹©é¡¹ç»“æ„")
                    else:
                        empty_chunks += 1
                        print(f"âš ï¸ æ•°æ®å—ä¸­æ²¡æœ‰choiceså­—æ®µ")
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}, è·³è¿‡æ­¤å—")
                    continue
        
        print(f"âœ… æµå¼å“åº”å¤„ç†å®Œæˆ:")
        print(f"   ğŸ“Š æ€»å—æ•°: {chunk_count}")
        print(f"   ğŸ“ å†…å®¹å—æ•°: {content_chunks}")
        print(f"   âš ï¸ ç©ºå—æ•°: {empty_chunks}")
        print(f"   ğŸ“ æ€»å†…å®¹é•¿åº¦: {len(accumulated_content)}")
        
        if accumulated_content:
            # print(f"   ğŸ” å†…å®¹é¢„è§ˆ: {accumulated_content[:200]}{'...' if len(accumulated_content) > 200 else ''}")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
            print(f"   ğŸ” å†…å®¹ç±»å‹: {'åŒ…å«å›¾åƒæ•°æ®' if 'data:image/' in accumulated_content else 'çº¯æ–‡æœ¬å†…å®¹'}")
            if last_content_chunk > 0:
                print(f"   ğŸ“ æœ€åä¸€ä¸ªå†…å®¹å—ä½ç½®: ç¬¬{last_content_chunk}å—")
        else:
            print(f"   âš ï¸ è­¦å‘Š: æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹ï¼")
            print(f"   ğŸ’¡ å»ºè®®: æ£€æŸ¥OpenRouter APIå“åº”æ ¼å¼æˆ–æ¨¡å‹é…ç½®")
        
        return accumulated_content
        
    except Exception as e:
        print(f"âŒ æµå¼å“åº”å¤„ç†å¤±è´¥: {e}")
        return accumulated_content

def validate_api_url(url):
    """éªŒè¯API URLæ ¼å¼å¹¶è‡ªåŠ¨è¡¥å…¨"""
    if not url or not url.strip():
        return False
    
    url = url.strip()
    
    # å¦‚æœå·²ç»æ˜¯å®Œæ•´URLæ ¼å¼ï¼Œç›´æ¥è¿”å›True
    if url.startswith(('http://', 'https://')):
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        return url_pattern.match(url) is not None
    
    # å¦‚æœä¸æ˜¯å®Œæ•´URLï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå/IP
    domain_pattern = re.compile(
        r'^(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)?$', re.IGNORECASE)
    
    return domain_pattern.match(url) is not None

def build_api_url(base_url, model, api_format="gemini"):
    """æ„å»ºå®Œæ•´çš„API URL"""
    base_url = base_url.strip().rstrip('/')

    # è‡ªåŠ¨è¡¥å…¨åè®®å‰ç¼€
    if not base_url.startswith(('http://', 'https://')):
        base_url = f"https://{base_url}"

    # OpenRouteré•œåƒç«™ç‰¹æ®Šå¤„ç†
    if "openrouter.ai" in base_url:
        # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹ï¼ŒURLæ„å»ºåœ¨OpenRouterå¤„ç†é€»è¾‘ä¸­
        return base_url

    # è§„èŒƒåŒ–æ¨¡å‹åç§°
    normalized_model = _normalize_model_name(model)

    # T8é•œåƒç«™ç‰¹æ®Šå¤„ç† - æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ç«¯ç‚¹ï¼ˆä¸Comflyå¯¹é½ï¼‰
    if "t8star.cn" in base_url or "ai.t8star.cn" in base_url:
        # ç§»é™¤base_urlæœ«å°¾çš„/v1ï¼ˆå¦‚æœæœ‰ï¼‰
        base_url_clean = base_url.rstrip('/v1').rstrip('/')

        if normalized_model in ["nano-banana", "nano-banana-hd"]:
            # nano-bananaæ¨¡å‹ä½¿ç”¨chat/completionsç«¯ç‚¹
            return f"{base_url_clean}/v1/chat/completions"
        else:
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼ç«¯ç‚¹ï¼ˆä¸Comflyç›¸åŒçš„æ ¼å¼ï¼‰
            return f"{base_url_clean}/v1/models/{normalized_model}:generateContent"

    # Comflyé•œåƒç«™ç‰¹æ®Šå¤„ç† - å¯¹nano-bananaä½¿ç”¨chat/completionsï¼Œå…¶ä»–æœåŠ¡ä½¿ç”¨æ ‡å‡†Gemini API
    if _is_comfly_base(base_url):
        # ç§»é™¤base_urlæœ«å°¾çš„/v1ï¼ˆå¦‚æœæœ‰ï¼‰
        base_url_clean = base_url.rstrip('/v1').rstrip('/')

        if normalized_model in ["nano-banana", "nano-banana-hd"]:
            return f"{base_url_clean}/v1/chat/completions"
        else:
            # å…¶ä»–ComflyæœåŠ¡ä½¿ç”¨æ ‡å‡†Gemini APIæ ¼å¼
            return f"{base_url_clean}/v1/models/{normalized_model}:generateContent"
    
    # API4GPTé•œåƒç«™ç‰¹æ®Šå¤„ç†
    if "www.api4gpt.com" in base_url:
        # API4GPTçš„URLæ„å»ºåœ¨call_api4gpt_apiå‡½æ•°ä¸­å¤„ç†
        return base_url
    
    # å¦‚æœç”¨æˆ·æä¾›çš„æ˜¯å®Œæ•´URLï¼Œç›´æ¥ä½¿ç”¨
    if '/models/' in base_url and ':generateContent' in base_url:
        return base_url
    
    # å¦‚æœæ˜¯åŸºç¡€URLï¼Œæ„å»ºå®Œæ•´è·¯å¾„
    if base_url.endswith('/v1beta') or base_url.endswith('/v1'):
        return f"{base_url}/models/{model}:generateContent"
    
    # é»˜è®¤æ·»åŠ v1betaè·¯å¾„
    return f"{base_url}/v1beta/models/{model}:generateContent"

def build_t8_api_request(model, prompt, image_base64=None, temperature=0.9, max_tokens=2048):
    """æ„å»ºT8é•œåƒç«™çš„APIè¯·æ±‚æ ¼å¼
    
    T8é•œåƒç«™ä½¿ç”¨OpenAIå…¼å®¹çš„APIæ ¼å¼ï¼Œä½†éœ€è¦ç‰¹æ®Šå¤„ç†
    """
    # æ„å»ºæ¶ˆæ¯å†…å®¹
    content = []
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    content.append({
        "type": "text",
        "text": prompt
    })
    
    # å¦‚æœæœ‰å›¾åƒï¼Œæ·»åŠ å›¾åƒå†…å®¹
    if image_base64:
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_base64}"
            }
        })
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "model": _normalize_model_name(model),
        "messages": [
            {
                "role": "user",
                "content": content
            }
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }
    
    return request_data

def call_t8_api(url, api_key, request_data, timeout=300):
    """è°ƒç”¨T8é•œåƒç«™API"""
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key.strip()}"
    }
    
    try:
        response = requests.post(url, headers=headers, json=request_data, timeout=timeout, verify=False)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        raise ValueError(f"T8é•œåƒç«™APIè°ƒç”¨å¤±è´¥: {str(e)}")
    except json.JSONDecodeError as e:
        raise ValueError(f"T8é•œåƒç«™APIå“åº”è§£æå¤±è´¥: {str(e)}")

def build_api4gpt_request(service_type, model, prompt, image_base64=None, size="1024x1024", quality="hd", style="natural", temperature=0.9, max_tokens=2048):
    """æ„å»ºAPI4GPTçš„APIè¯·æ±‚æ ¼å¼
    
    API4GPTæ”¯æŒå¤šç§å›¾åƒæœåŠ¡ï¼ŒåŒ…æ‹¬nano-bananaã€DALL-E 3ã€Stable-Diffusionã€Fluxç­‰
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
    """
    if service_type == "nano-banana":
        # nano-bananaæœåŠ¡ä½¿ç”¨å®˜æ–¹æ–‡æ¡£æ ¼å¼
        request_data = {
            "prompt": prompt,
            "n": 1,
            "model": "gemini-2.5-flash-image"
        }
        
        # å¦‚æœæœ‰å›¾åƒï¼Œæ·»åŠ å›¾åƒå†…å®¹ï¼ˆç”¨äºå›¾åƒç¼–è¾‘ï¼‰
        if image_base64:
            # å¯¹äºå›¾åƒç¼–è¾‘ï¼Œä½¿ç”¨multipart/form-dataæ ¼å¼
            # è¿™é‡Œè¿”å›ä¸€ä¸ªæ ‡è®°ï¼Œè¡¨ç¤ºéœ€è¦ä½¿ç”¨multipartæ ¼å¼
            request_data["_multipart"] = True
            request_data["image"] = image_base64
            
    elif service_type == "dall-e-3":
        # DALL-E 3æœåŠ¡ä½¿ç”¨OpenAIæ ¼å¼
        request_data = {
            "model": _normalize_model_name(model),
            "prompt": prompt,
            "n": 1,
            "size": size,
            "quality": quality,
            "style": style,
            "response_format": "b64_json"
        }
        
    elif service_type == "stable-diffusion":
        # Stable-DiffusionæœåŠ¡
        request_data = {
            "prompt": prompt,
            "negative_prompt": "",
            "width": int(size.split('x')[0]),
            "height": int(size.split('x')[1]),
            "num_inference_steps": 20,
            "guidance_scale": 7.5,
            "num_images_per_prompt": 1
        }
        
    elif service_type == "flux":
        # FluxæœåŠ¡
        request_data = {
            "prompt": prompt,
            "width": int(size.split('x')[0]),
            "height": int(size.split('x')[1]),
            "num_images": 1,
            "scheduler": "euler",
            "num_inference_steps": 20,
            "guidance_scale": 7.5
        }
        
    else:
        # é»˜è®¤ä½¿ç”¨nano-bananaæ ¼å¼
        request_data = {
            "prompt": prompt,
            "n": 1,
            "model": "gemini-2.5-flash-image"
        }
    
    return request_data

def call_api4gpt_api(url, api_key, service_type, request_data, timeout=300):
    """è°ƒç”¨API4GPT API
    
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
    """
    # æ ¹æ®æœåŠ¡ç±»å‹æ„å»ºä¸åŒçš„APIç«¯ç‚¹
    if service_type == "nano-banana":
        # nano-bananaä½¿ç”¨ /v1/images/generations ç«¯ç‚¹
        api_endpoint = f"{url}/v1/images/generations"
    elif service_type == "dall-e-3":
        api_endpoint = f"{url}/v1/images/generations"
    elif service_type == "stable-diffusion":
        api_endpoint = f"{url}/v1/images/generations"
    elif service_type == "flux":
        api_endpoint = f"{url}/v1/images/generations"
    else:
        api_endpoint = f"{url}/v1/images/generations"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨multipartæ ¼å¼
    if request_data.get("_multipart") and service_type == "nano-banana":
        # å¯¹äºå›¾åƒç¼–è¾‘ï¼Œä½¿ç”¨multipart/form-dataæ ¼å¼
        print("ğŸ”— ä½¿ç”¨API4GPT multipartæ ¼å¼è¿›è¡Œå›¾åƒç¼–è¾‘")
        
        # å‡†å¤‡multipartæ•°æ®
        files = {}
        data = {}
        
        # æ·»åŠ å›¾åƒæ–‡ä»¶
        if "image" in request_data:
            # å°†base64è½¬æ¢ä¸ºæ–‡ä»¶å¯¹è±¡
            image_data = base64.b64decode(request_data["image"])
            files["image"] = ("image.jpg", image_data, "image/jpeg")
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        data["prompt"] = request_data["prompt"]
        data["n"] = request_data["n"]
        data["model"] = request_data["model"]
        
        # ä½¿ç”¨multipartç«¯ç‚¹
        edit_endpoint = f"{url}/v1/images/edits"
        
        headers = {
            "Authorization": f"Bearer {api_key.strip()}"
        }
        
        try:
            response = requests.post(
                edit_endpoint, 
                headers=headers, 
                files=files, 
                data=data, 
                timeout=timeout
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise ValueError(f"API4GPT multipart APIè°ƒç”¨å¤±è´¥: {str(e)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"API4GPT multipart APIå“åº”è§£æå¤±è´¥: {str(e)}")
    else:
        # æ ‡å‡†JSONæ ¼å¼
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key.strip()}"
        }
        
        try:
            response = requests.post(api_endpoint, headers=headers, json=request_data, timeout=timeout)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {str(e)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"API4GPT APIå“åº”è§£æå¤±è´¥: {str(e)}")

def parse_api4gpt_response(response_data, service_type):
    """è§£æAPI4GPTçš„å“åº”æ•°æ®
    
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
    """
    if service_type == "nano-banana":
        # è§£ænano-bananaå“åº”ï¼ˆå®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼‰
        response_text = "nano-bananaå›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå“åº”æ ¼å¼ä¸ºï¼š
        # {
        #   "created": 1745711868,
        #   "data": [
        #     {
        #       "revised_prompt": "ä¸€åªç†ŠçŒ«åœ¨éª‘è‡ªè¡Œè½¦...",
        #       "url": "https://filesystem.site/cdn/..."
        #     }
        #   ]
        # }
        
        if "data" in response_data and response_data["data"]:
            image_data = response_data["data"][0]
            
            # æå–ä¿®è®¢åçš„æç¤ºè¯
            if "revised_prompt" in image_data:
                response_text = f"å›¾åƒç”Ÿæˆå®Œæˆã€‚ä¿®è®¢åçš„æç¤ºè¯: {image_data['revised_prompt']}"
            
            # æå–å›¾åƒURL
            if "url" in image_data:
                image_url = image_data["url"]
                try:
                    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½API4GPTç”Ÿæˆçš„å›¾åƒ: {image_url}")
                    response = requests.get(image_url, timeout=30)
                    if response.status_code == 200:
                        image_bytes = response.content
                        generated_image = Image.open(io.BytesIO(image_bytes))
                        print("âœ… æˆåŠŸä¸‹è½½API4GPTç”Ÿæˆçš„å›¾åƒ")
                    else:
                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
        
        return response_text, generated_image
        
    elif service_type == "dall-e-3":
        # è§£æDALL-E 3å“åº”
        response_text = "DALL-E 3å›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        if "data" in response_data and response_data["data"]:
            image_data = response_data["data"][0]
            if "url" in image_data:
                image_url = image_data["url"]
                try:
                    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½DALL-E 3ç”Ÿæˆçš„å›¾åƒ: {image_url}")
                    response = requests.get(image_url, timeout=30)
                    if response.status_code == 200:
                        image_bytes = response.content
                        generated_image = Image.open(io.BytesIO(image_bytes))
                        print("âœ… æˆåŠŸä¸‹è½½DALL-E 3ç”Ÿæˆçš„å›¾åƒ")
                    else:
                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
        
        return response_text, generated_image
        
    elif service_type == "stable-diffusion":
        # è§£æStable-Diffusionå“åº”
        response_text = "Stable-Diffusionå›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        if "images" in response_data and response_data["images"]:
            image_data = response_data["images"][0]
            if image_data:
                try:
                    image_bytes = base64.b64decode(image_data)
                    generated_image = Image.open(io.BytesIO(image_bytes))
                    print("âœ… æˆåŠŸè§£æStable-Diffusionå›¾åƒæ•°æ®")
                except Exception as e:
                    print(f"âš ï¸ Stable-Diffusionå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
        
        return response_text, generated_image
        
    elif service_type == "flux":
        # è§£æFluxå“åº”
        response_text = "Fluxå›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        if "images" in response_data and response_data["images"]:
            image_data = response_data["images"][0]
            if image_data:
                try:
                    image_bytes = base64.b64decode(image_data)
                    generated_image = Image.open(io.BytesIO(image_bytes))
                    print("âœ… æˆåŠŸè§£æFluxå›¾åƒæ•°æ®")
                except Exception as e:
                    print(f"âš ï¸ Fluxå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
        
        return response_text, generated_image
        
    else:
        # é»˜è®¤è¿”å›ç©ºç»“æœ
        return "", None

def smart_retry_delay(attempt, error_code=None):
    """æ™ºèƒ½é‡è¯•å»¶è¿Ÿ - æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­‰å¾…æ—¶é—´"""
    base_delay = 2 ** attempt  # æŒ‡æ•°é€€é¿
    
    if error_code == 429:  # é™æµé”™è¯¯
        rate_limit_delay = 60 + random.uniform(10, 30)  # 60-90ç§’éšæœºç­‰å¾…
        return max(base_delay, rate_limit_delay)
    elif error_code in [500, 502, 503, 504]:  # æœåŠ¡å™¨é”™è¯¯
        return base_delay + random.uniform(1, 5)  # æ·»åŠ éšæœºæŠ–åŠ¨
    else:
        return base_delay

def create_dummy_image(width=512, height=512):
    """Create a placeholder image"""
    dummy_array = np.zeros((height, width, 3), dtype=np.uint8)
    dummy_tensor = torch.from_numpy(dummy_array).float() / 255.0
    return dummy_tensor.unsqueeze(0)

def resize_image_for_api(image, max_size=2048):
    """è°ƒæ•´å›¾åƒå¤§å°ä»¥æ»¡è¶³APIé™åˆ¶"""
    if max(image.size) > max_size:
        ratio = max_size / max(image.size)
        new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        _log_info(f"Image resized to {new_size} for API compatibility")
    return image

def remove_white_areas(image: Image.Image, white_threshold: int = 250) -> Image.Image:
    """
    æ£€æµ‹å¹¶å»é™¤å›¾åƒä¸­çš„ç™½è‰²åŒºåŸŸ

    Args:
        image: è¾“å…¥å›¾åƒ
        white_threshold: ç™½è‰²é˜ˆå€¼ï¼Œåƒç´ å€¼å¤§äºæ­¤å€¼è¢«è®¤ä¸ºæ˜¯ç™½è‰² (0-255)

    Returns:
        å»é™¤ç™½è‰²åŒºåŸŸåçš„å›¾åƒ
    """
    try:
        import numpy as np

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(image)
        height, width = img_array.shape[:2]

        print(f"ğŸ” å¼€å§‹æ£€æµ‹ç™½è‰²åŒºåŸŸï¼Œé˜ˆå€¼: {white_threshold}")

        # å¤šç§ç™½è‰²æ£€æµ‹ç­–ç•¥
        white_masks = []

        # ç­–ç•¥1: ä¸¥æ ¼ç™½è‰²æ£€æµ‹ (RGBä¸‰ä¸ªé€šé“éƒ½å¤§äºé˜ˆå€¼)
        if len(img_array.shape) == 3:  # RGBå›¾åƒ
            strict_white_mask = np.all(img_array >= white_threshold, axis=2)
            white_masks.append(strict_white_mask)

            # ç­–ç•¥2: è¿‘ä¼¼ç™½è‰²æ£€æµ‹ (RGBå·®å¼‚å°ä¸”å¹³å‡å€¼é«˜)
            rgb_mean = np.mean(img_array, axis=2)
            rgb_std = np.std(img_array, axis=2)
            approx_white_mask = (rgb_mean >= white_threshold - 10) & (rgb_std <= 15)
            white_masks.append(approx_white_mask)

            # ç­–ç•¥3: çº¯ç™½è‰²æ£€æµ‹ (RGB = 255)
            pure_white_mask = np.all(img_array == 255, axis=2)
            white_masks.append(pure_white_mask)

        else:  # ç°åº¦å›¾åƒ
            white_mask = img_array >= white_threshold
            white_masks.append(white_mask)

        # åˆå¹¶æ‰€æœ‰ç™½è‰²æ£€æµ‹ç»“æœ
        combined_white_mask = np.logical_or.reduce(white_masks)

        # è®¡ç®—ç™½è‰²åƒç´ æ¯”ä¾‹
        white_ratio = np.sum(combined_white_mask) / (height * width)
        print(f"ğŸ” ç™½è‰²åƒç´ æ¯”ä¾‹: {white_ratio:.2%}")

        # å¦‚æœç™½è‰²åƒç´ æ¯”ä¾‹å¤ªä½ï¼Œä¸éœ€è¦å¤„ç†
        if white_ratio < 0.02:  # å°äº2%
            print(f"â„¹ï¸ ç™½è‰²åƒç´ æ¯”ä¾‹è¾ƒä½({white_ratio:.2%})ï¼Œè·³è¿‡å¤„ç†")
            return image

        # æ‰¾åˆ°éç™½è‰²åŒºåŸŸçš„è¾¹ç•Œæ¡†
        non_white_mask = ~combined_white_mask

        # æ‰¾åˆ°éç™½è‰²åƒç´ çš„è¡Œå’Œåˆ—
        non_white_rows = np.any(non_white_mask, axis=1)
        non_white_cols = np.any(non_white_mask, axis=0)

        # å¦‚æœæ²¡æœ‰éç™½è‰²åƒç´ ï¼Œè¿”å›åŸå›¾
        if not np.any(non_white_rows) or not np.any(non_white_cols):
            print(f"âš ï¸ å›¾åƒå‡ ä¹å…¨æ˜¯ç™½è‰²ï¼Œä¿æŒåŸå›¾")
            return image

        # æ‰¾åˆ°è¾¹ç•Œ
        top = np.argmax(non_white_rows)
        bottom = len(non_white_rows) - 1 - np.argmax(non_white_rows[::-1])
        left = np.argmax(non_white_cols)
        right = len(non_white_cols) - 1 - np.argmax(non_white_cols[::-1])

        # æ£€æµ‹è¾¹ç¼˜ç™½è‰²åŒºåŸŸçš„åšåº¦
        edge_thickness = {
            'top': top,
            'bottom': height - 1 - bottom,
            'left': left,
            'right': width - 1 - right
        }

        print(f"ğŸ” è¾¹ç¼˜ç™½è‰²åšåº¦: ä¸Š{edge_thickness['top']}, ä¸‹{edge_thickness['bottom']}, å·¦{edge_thickness['left']}, å³{edge_thickness['right']}")

        # åªæœ‰å½“è¾¹ç¼˜ç™½è‰²åŒºåŸŸè¶³å¤Ÿåšä¸”æ˜¯çœŸæ­£çš„è¾¹æ¡†æ—¶æ‰è¿›è¡Œè£å‰ª
        min_edge_thickness = max(30, width // 8, height // 8)  # è‡³å°‘30åƒç´ æˆ–å›¾åƒå°ºå¯¸çš„12.5%

        # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„è¾¹æ¡†ï¼ˆå››è¾¹éƒ½æœ‰ç™½è‰²æˆ–è€…å¯¹è¾¹æœ‰ç™½è‰²ï¼‰
        thick_edges = [k for k, v in edge_thickness.items() if v >= min_edge_thickness]

        # åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µæ‰è£å‰ªï¼š
        # 1. å››è¾¹éƒ½æœ‰åšç™½è¾¹
        # 2. å¯¹è¾¹éƒ½æœ‰åšç™½è¾¹ï¼ˆä¸Šä¸‹æˆ–å·¦å³ï¼‰
        # 3. ä¸‰è¾¹æœ‰åšç™½è¾¹
        is_border = (
            len(thick_edges) >= 3 or  # ä¸‰è¾¹æˆ–å››è¾¹æœ‰åšç™½è¾¹
            ('top' in thick_edges and 'bottom' in thick_edges) or  # ä¸Šä¸‹éƒ½æœ‰
            ('left' in thick_edges and 'right' in thick_edges)     # å·¦å³éƒ½æœ‰
        )

        if not is_border:
            print(f"â„¹ï¸ ä¸æ˜¯çœŸæ­£çš„ç™½è‰²è¾¹æ¡†ï¼Œè·³è¿‡è£å‰ªã€‚åšè¾¹: {thick_edges}")
            return image

        print(f"âœ… æ£€æµ‹åˆ°ç™½è‰²è¾¹æ¡†ï¼Œåšè¾¹: {thick_edges}")

        # æ·»åŠ ä¸€äº›è¾¹è·ï¼Œé¿å…è£å‰ªè¿‡ç´§
        margin = min(5, width // 50, height // 50)  # æœ€å¤š5åƒç´ æˆ–å›¾åƒå°ºå¯¸çš„2%
        top = max(0, top - margin)
        bottom = min(height - 1, bottom + margin)
        left = max(0, left - margin)
        right = min(width - 1, right + margin)

        # æ£€æŸ¥è£å‰ªåŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
        crop_width = right - left + 1
        crop_height = bottom - top + 1

        if crop_width <= 0 or crop_height <= 0:
            print(f"âš ï¸ è£å‰ªåŒºåŸŸæ— æ•ˆï¼Œä¿æŒåŸå›¾")
            return image

        # è®¡ç®—è£å‰ªæ¯”ä¾‹
        crop_ratio = (crop_width * crop_height) / (width * height)

        # å¦‚æœè£å‰ªåçš„åŒºåŸŸå¤ªå°ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤
        if crop_ratio < 0.3:  # å°äº30%
            print(f"âš ï¸ è£å‰ªååŒºåŸŸè¿‡å°({crop_ratio:.2%})ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤ï¼Œä¿æŒåŸå›¾")
            return image

        print(f"âœ… æ£€æµ‹åˆ°ç™½è‰²è¾¹æ¡†ï¼Œè£å‰ªåŒºåŸŸ: ({left}, {top}) -> ({right}, {bottom})")
        print(f"âœ… è£å‰ªå°ºå¯¸: {crop_width}x{crop_height} (ä¿ç•™{crop_ratio:.1%})")

        # è£å‰ªå›¾åƒ
        cropped_image = image.crop((left, top, right + 1, bottom + 1))

        return cropped_image

    except Exception as e:
        print(f"âŒ ç™½è‰²åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return image

def smart_resize_with_padding(image: Image.Image, target_size: Tuple[int, int],
                             fill_color: Tuple[int, int, int] = (255, 255, 255)) -> Image.Image:
    """
    ç»Ÿä¸€è°ƒç”¨ä¸»å®ç°ï¼šæŒ‰ç›®æ ‡å°ºå¯¸ç›´æ¥æ‰©å›¾ï¼Œé¿å…å…ˆæ”¾å¤§å†è£å‰ªã€‚
    å¢å¼ºåŠŸèƒ½ï¼šé¦–å…ˆæ£€æµ‹å¹¶å»é™¤ç™½è‰²åŒºåŸŸï¼Œç„¶åè¿›è¡Œæ™ºèƒ½å¤„ç†ã€‚
    """
    try:
        img_width, img_height = image.size
        target_width, target_height = target_size

        print(f"ğŸ¯ å¼€å§‹å›¾åƒå¤„ç†: {img_width}x{img_height} -> {target_width}x{target_height}")

        # ğŸš€ ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å¹¶å»é™¤ç™½è‰²åŒºåŸŸ
        processed_image = remove_white_areas(image)
        if processed_image.size != image.size:
            print(f"âœ… ç™½è‰²åŒºåŸŸå·²å»é™¤: {image.size} -> {processed_image.size}")
            image = processed_image
            img_width, img_height = image.size

            # å¦‚æœè¿˜æœ‰ç™½è‰²åŒºåŸŸï¼Œå°è¯•æ›´æ¿€è¿›çš„æ£€æµ‹
            processed_image2 = remove_white_areas(image, white_threshold=230)
            if processed_image2.size != image.size:
                print(f"âœ… æ¿€è¿›æ¨¡å¼å†æ¬¡å»é™¤ç™½è‰²åŒºåŸŸ: {image.size} -> {processed_image2.size}")
                image = processed_image2
                img_width, img_height = image.size
        else:
            print(f"â„¹ï¸ æœªæ£€æµ‹åˆ°éœ€è¦å»é™¤çš„ç™½è‰²åŒºåŸŸ")

        # æ¯”ä¾‹ç›¸åŒæ—¶ï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸
        if abs(img_width/img_height - target_width/target_height) < 0.01:
            print(f"ğŸ¯ æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸")
            resized_img = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
            return resized_img

        # ä½¿ç”¨è£å‰ªæ¨¡å¼ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œç„¶åæ™ºèƒ½è£å‰ª
        print(f"ğŸ¯ è£å‰ªæ¨¡å¼ï¼šæ™ºèƒ½é«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œç„¶åæ™ºèƒ½è£å‰ª")

        # ğŸš€ æ™ºèƒ½åˆ†ææ”¾å¤§éœ€æ±‚
        width_ratio = target_width / img_width   # å®½åº¦éœ€è¦çš„æ”¾å¤§æ¯”ä¾‹
        height_ratio = target_height / img_height # é«˜åº¦éœ€è¦çš„æ”¾å¤§æ¯”ä¾‹

        print(f"ğŸ“Š åŸå§‹å°ºå¯¸: {img_width}x{img_height}")
        print(f"ğŸ“Š ç›®æ ‡å°ºå¯¸: {target_width}x{target_height}")
        print(f"ğŸ“Š å®½åº¦æ¯”ä¾‹: {width_ratio:.3f} ({'éœ€è¦æ”¾å¤§' if width_ratio > 1 else 'éœ€è¦ç¼©å°' if width_ratio < 1 else 'æ— éœ€è°ƒæ•´'})")
        print(f"ğŸ“Š é«˜åº¦æ¯”ä¾‹: {height_ratio:.3f} ({'éœ€è¦æ”¾å¤§' if height_ratio > 1 else 'éœ€è¦ç¼©å°' if height_ratio < 1 else 'æ— éœ€è°ƒæ•´'})")

        # ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
        scale = max(width_ratio, height_ratio)

        # è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸
        enlarged_width = int(img_width * scale)
        enlarged_height = int(img_height * scale)

        print(f"ğŸ”§ æ™ºèƒ½æ”¾å¤§: {img_width}x{img_height} -> {enlarged_width}x{enlarged_height}")
        print(f"ğŸ”§ æœ€ç»ˆç¼©æ”¾æ¯”ä¾‹: {scale:.3f} (æŒ‰{'é«˜åº¦' if height_ratio > width_ratio else 'å®½åº¦'}éœ€æ±‚æ”¾å¤§)")

        print(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§: {img_width}x{img_height} -> {enlarged_width}x{enlarged_height}")
        print(f"ğŸ”§ ç¼©æ”¾æ¯”ä¾‹: {scale:.3f}")

        # ä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·è¿›è¡Œæ”¾å¤§ï¼ˆç¦ç”¨AIæ”¾å¤§é¿å…é—®é¢˜ï¼‰
        enlarged_image = image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)

        # æ™ºèƒ½è£å‰ª - ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
        if enlarged_width >= target_width and enlarged_height >= target_height:
            print(f"ğŸ”§ æ™ºèƒ½è£å‰ªï¼šä»é«˜æ¸…æ”¾å¤§å›¾åƒä¸­è£å‰ªç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å±…ä¸­")

            # ç²¾ç¡®è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“å®Œå…¨å±…ä¸­
            crop_x = (enlarged_width - target_width) // 2
            crop_y = (enlarged_height - target_height) // 2

            print(f"ğŸ”§ ç²¾ç¡®å±…ä¸­è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")

            # ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
            final_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))

            print(f"âœ… é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªå®Œæˆ")
            return final_image
        else:
            print(f"âš ï¸ é«˜æ¸…æ”¾å¤§åå°ºå¯¸ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……")
            # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒ
            final_image = Image.new('RGB', (target_width, target_height), fill_color)

            # å°†é«˜æ¸…æ”¾å¤§çš„å›¾åƒå±…ä¸­æ”¾ç½®
            paste_x = (target_width - enlarged_width) // 2
            paste_y = (target_height - enlarged_height) // 2
            final_image.paste(enlarged_image, (paste_x, paste_y))

            print(f"âœ… æ™ºèƒ½å¡«å……å®Œæˆ")
            return final_image

    except Exception as e:
        print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ: {e}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

        # å›é€€åˆ°æœ€ç®€å•çš„å¤„ç†æ–¹å¼
        try:
            from .gemini_banana import smart_resize_with_padding as core_resize
        except ImportError:
            from gemini_banana import smart_resize_with_padding as core_resize
        return core_resize(image, target_size, fill_color=fill_color, fill_strategy="paste")

class KenChenLLMGeminiBananaMirrorImageGenNode:
    """Gemini Banana é•œåƒç«™å›¾ç‰‡ç”ŸæˆèŠ‚ç‚¹

    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒé€‰æ‹©é¢„é…ç½®çš„é•œåƒç«™ï¼ˆofficial, comfly, customï¼‰
    - è‡ªåŠ¨å¡«å……å¯¹åº”é•œåƒç«™çš„ API URL å’Œ API Key
    - é€‰æ‹© custom æ—¶å¯æ‰‹åŠ¨è¾“å…¥è‡ªå®šä¹‰é•œåƒç«™ä¿¡æ¯
    - æ™ºèƒ½URLæ ¼å¼éªŒè¯å’Œè‡ªåŠ¨è¡¥å…¨
    """

    # è®¾ç½®èŠ‚ç‚¹é¢œè‰²ä¸ºæ©™è‰²/æ£•è‰²ç³»
    @classmethod
    def get_node_color(cls):
        return "#D2691E"  # å·§å…‹åŠ›æ©™è‰²

    @classmethod
    def get_node_bgcolor(cls):
        return "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯

    @classmethod
    def INPUT_TYPES(cls):
        # å¯¹é½ Gemini Banana Text to Image Banana çš„æ§ä»¶
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        image_settings = config.get('image_settings', {})
        
        # è·å–é•œåƒç«™é…ç½®
        mirror_sites = config.get('mirror_sites', {})
        
        # ä¸å†é‡å¤æ·»åŠ T8é•œåƒç«™é…ç½®ï¼Œå› ä¸ºé…ç½®æ–‡ä»¶ä¸­å·²ç»æœ‰äº†
        
        mirror_options = list(mirror_sites.keys())
        if not mirror_options:
            mirror_options = ["official", "comfly", "custom"]
        
        # è·å–é»˜è®¤é•œåƒç«™é…ç½®
        default_site = "comfly" if "comfly" in mirror_options else mirror_options[0] if mirror_options else "official"
        default_config = get_mirror_site_config(default_site)
        
        # ğŸš€ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶é¢„è®¾
        aspect_ratios = image_settings.get('aspect_ratios', [
            "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
        ])
        response_modalities = image_settings.get('response_modalities', [
            "TEXT_AND_IMAGE", "IMAGE_ONLY"
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„è¶…é«˜æ¸…é€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "mirror_site": (mirror_options, {"default": default_site}),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "é•œåƒç«™APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç•™ç©ºæ—¶è‡ªåŠ¨è·å–ï¼‰"
                }),
                "prompt": ("STRING", {"default": "A beautiful mountain landscape at sunset", "multiline": True}),
                # æ”¯æŒå¤šç§AIæ¨¡å‹å’Œå›¾åƒç”ŸæˆæœåŠ¡: nano-bananaæ”¯æŒComflyå’ŒT8é•œåƒç«™, [All]æ”¯æŒæ‰€æœ‰é•œåƒç«™, API4GPTæ¨¡å‹, OpenRouteræ¨¡å‹
                "model": (["nano-banana [Comfly-T8]", "nano-banana-hd [Comfly-T8]", "gemini-2.5-flash-image [All]", "gemini-2.5-flash-image-preview [All]", "gemini-2.0-flash-preview-image-generation", "gemini-2.5-flash-image-hd [API4GPT]", "gemini-2.5-flash-image-vip [API4GPT]", "google/gemini-2.5-flash-image [OpenRouter]", "google/gemini-2.5-flash-image-preview [OpenRouter]"], {"default": "nano-banana [Comfly-T8]"}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),

                # ğŸ“ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶å‚æ•°
                "aspect_ratio": (aspect_ratios, {
                    "default": image_settings.get('default_aspect_ratio', "1:1"),
                    "tooltip": "å›¾åƒå®½é«˜æ¯” (Geminiå®˜æ–¹APIæ”¯æŒ)"
                }),
                "response_modality": (response_modalities, {
                    "default": image_settings.get('default_response_modality', "TEXT_AND_IMAGE"),
                    "tooltip": "å“åº”æ¨¡å¼ï¼šTEXT_AND_IMAGE=æ–‡å­—+å›¾åƒï¼ŒIMAGE_ONLY=ä»…å›¾åƒ"
                }),

                # ğŸ” Topaz Gigapixel AIæ”¾å¤§æ§åˆ¶
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),

                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),

                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {"unique_id": "UNIQUE_ID"}
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "response_text")
    FUNCTION = "generate_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    # è®¾ç½®èŠ‚ç‚¹é¢œè‰² - ä½¿ç”¨ComfyUIæ ‡å‡†å±æ€§
    color = "#D2691E"  # å·§å…‹åŠ›æ©™è‰²
    bgcolor = "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯
    groupcolor = "#CD853F"  # æ²™æ£•è‰²

    def __init__(self):
        # å¼ºåˆ¶è®¾ç½®é¢œè‰²å±æ€§
        self.color = "#D2691E"
        self.bgcolor = "#8B4513"
        self.groupcolor = "#CD853F"
    
    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            print(f"âš ï¸ æ— æ³•æ¨é€å¯¹è¯: PromptServer={PromptServer is not None}, unique_id={unique_id}")
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000

            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")

            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt,
                            "response": display_response,
                            "response_id": str(random.randint(100000, 999999)),
                            "timestamp": time.time()
                        }
                    ], ensure_ascii=False)
                },
            }
            print(f"ğŸ’¬ æ¨é€å¯¹è¯æ°”æ³¡åˆ°èŠ‚ç‚¹ {unique_id}")
            PromptServer.instance.send_sync("display_component", render_spec)
            print(f"âœ… å¯¹è¯æ°”æ³¡æ¨é€æˆåŠŸ")
        except Exception as e:
            print(f"âŒ [LLM Agent Assistant] Chat push failed: {e}")
            import traceback
            traceback.print_exc()
            pass
    
    def generate_image(self, mirror_site: str, api_key: str, prompt: str, model: str,
                      proxy: str, aspect_ratio: str, response_modality: str, upscale_factor: str, gigapixel_model: str,
                      quality: str, style: str, detail_level: str, camera_control: str, lighting_control: str,
                      template_selection: str, temperature: float, top_p: float, top_k: int,
                      max_output_tokens: int, seed: int,
                      custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨é•œåƒç«™APIç”Ÿæˆå›¾ç‰‡"""

        # ğŸ”§ ç¡®ä¿requestsæ¨¡å—å¯ç”¨
        import requests

        # ğŸš€ ç«‹å³è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå»é™¤UIæ ‡è¯†
        model = _normalize_model_name(model)
        
        # æ ¹æ®é•œåƒç«™ä»é…ç½®è·å–URLå’ŒAPI Key
        site_config = get_mirror_site_config(mirror_site) if mirror_site else {"url": "", "api_key": ""}
        api_url = site_config.get("url", "").strip()
        if site_config.get("api_key") and not api_key.strip():
            api_key = site_config["api_key"]
            print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™API Key: {api_key[:8]}...")
        if not api_url:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘è¯¥é•œåƒç«™çš„API URL")
        print(f"ğŸ”— è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™URL: {api_url}")
        
        # éªŒè¯API URL
        if not validate_api_url(api_url):
            raise ValueError("API URLæ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")

        # ğŸ¨ æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆä½¿ç”¨enhance_prompt_with_controlså‡½æ•°ï¼‰
        try:
            from .gemini_banana import process_image_controls, enhance_prompt_with_controls
        except ImportError:
            from gemini_banana import process_image_controls, enhance_prompt_with_controls

        controls = process_image_controls(quality, style)

        # ä½¿ç”¨enhance_prompt_with_controlså‡½æ•°è¿›è¡Œå®Œæ•´çš„æç¤ºè¯å¢å¼º
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(),
            controls,
            detail_level,
            camera_control,
            lighting_control,
            template_selection,
            quality_enhancement="Auto",  # é»˜è®¤å€¼
            enhance_quality=True,  # é»˜è®¤å€¼
            smart_resize=True,  # é»˜è®¤å€¼
            fill_color="white"  # é»˜è®¤å€¼
        )

        # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
        if custom_additions and custom_additions.strip():
            enhanced_prompt += f"\n\n{custom_additions.strip()}"
            print(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_additions[:100]}...")

        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: aspect_ratio={aspect_ratio}, quality={quality}, style={style}")

        # è®¾ç½®ä»£ç†ï¼ˆç”¨äº requests åº“ï¼‰
        proxies = None
        if proxy and proxy.strip() and "None" not in proxy:
            proxies = {
                'http': proxy.strip(),
                'https': proxy.strip()
            }
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
            print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
        else:
            existing = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
            if existing:
                print(f"ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼Œæ²¿ç”¨ç³»ç»Ÿä»£ç†: {existing}")
            else:
                print("ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼ˆç³»ç»Ÿæ— ä»£ç†ï¼‰")
        
        # æ„å»ºå®Œæ•´çš„API URL
        full_url = build_api_url(api_url, model)
        print(f"ğŸŒ ä½¿ç”¨APIåœ°å€: {full_url}")
        
        # æ£€æŸ¥é•œåƒç«™ç±»å‹ - æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        is_nano_banana_official = mirror_site == "nano-bananaå®˜æ–¹"
        is_t8_mirror = "t8star.cn" in api_url or "ai.t8star.cn" in api_url
        is_api4gpt_mirror = "api4gpt.com" in api_url or "[API4GPT]" in model
        is_comfly_mirror = _is_comfly_base(api_url)
        is_openrouter_mirror = "openrouter.ai" in api_url or "[OpenRouter]" in model
        is_openai_mirror = "api.openai.com" in api_url or site_config.get("api_format") == "openai"

        # å¦‚æœæ£€æµ‹åˆ° OpenRouter æ¨¡å‹ä½† URL ä¸æ˜¯ OpenRouterï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter
        if "[OpenRouter]" in model and "openrouter.ai" not in api_url:
            api_url = "https://openrouter.ai/api/v1"
            is_openrouter_mirror = True
            print(f"ğŸ”„ æ£€æµ‹åˆ° OpenRouter æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter API: {api_url}")

        # å¦‚æœæ£€æµ‹åˆ° API4GPT æ¨¡å‹ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ URL
        if "[API4GPT]" in model:
            if "one.api4gpt.com" not in api_url:
                api_url = "https://one.api4gpt.com"
                is_api4gpt_mirror = True
                print(f"ğŸ”„ æ£€æµ‹åˆ° API4GPT æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° API4GPT API: {api_url}")
        # å¦‚æœ URL åŒ…å« api4gpt.com ä½†ä¸æ˜¯ one.api4gpt.comï¼Œä¹Ÿè¦åˆ‡æ¢
        elif "api4gpt.com" in api_url and "one.api4gpt.com" not in api_url:
            api_url = "https://one.api4gpt.com"
            is_api4gpt_mirror = True
            print(f"ğŸ”„ æ£€æµ‹åˆ°é”™è¯¯çš„ API4GPT URLï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æ­£ç¡®çš„ URL: {api_url}")

        # æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºå¤„ç†é•œåƒç«™ï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        
        # 1. nano-bananaå®˜æ–¹é•œåƒç«™å¤„ç†
        if is_nano_banana_official:
            print("ğŸ”— æ£€æµ‹åˆ°nano-bananaå®˜æ–¹é•œåƒç«™ï¼Œä½¿ç”¨Googleå®˜æ–¹API")

            # æ„å»ºå†…å®¹éƒ¨åˆ†
            content_parts = [{"text": enhanced_prompt}]

            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
            }

            # ğŸ¯ Geminiå®˜æ–¹APIï¼šResponse Modalitiesæ§åˆ¶
            if response_modality == "IMAGE_ONLY":
                generation_config["responseModalities"] = ["Image"]
                print("ğŸ“Š å“åº”æ¨¡å¼ï¼šä»…å›¾åƒï¼ˆIMAGE_ONLYï¼‰")
            else:
                generation_config["responseModalities"] = ["Text", "Image"]
                print("ğŸ“Š å“åº”æ¨¡å¼ï¼šæ–‡å­—+å›¾åƒï¼ˆTEXT_AND_IMAGEï¼‰")

            # ğŸ“ Geminiå®˜æ–¹APIï¼šAspect Ratioæ§åˆ¶
            if aspect_ratio and aspect_ratio != "1:1":
                generation_config["imageConfig"] = {
                    "aspectRatio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            try:
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨ï¼ˆå®˜æ–¹APIä¼˜å…ˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST APIï¼‰
                response_json = generate_with_priority_api(
                    api_key=api_key,
                    model=_normalize_model_name(model),
                    content_parts=content_parts,
                    generation_config=generation_config,
                    max_retries=5,
                    proxy=proxy
                )

                if response_json:
                    # æå–ç”Ÿæˆçš„å›¾åƒ
                    generated_image = process_generated_image_from_response(response_json)
                    print(f"ğŸ” å›¾åƒæå–ç»“æœ: {generated_image is not None}")
                    if generated_image:
                        print(f"ğŸ” åŸå§‹å›¾åƒå°ºå¯¸: {generated_image.size}")

                    # æå–å“åº”æ–‡æœ¬
                    response_text = extract_text_from_response(response_json)

                    if generated_image:
                        # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                # æå–æ”¾å¤§å€æ•°
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                    # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale

                                    # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale

                                    # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                    upscaled_image = smart_upscale(
                                        generated_image,
                                        target_w,
                                        target_h,
                                        gigapixel_model
                                    )

                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                                    else:
                                        print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆnano-bananaå®˜æ–¹ï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(generated_image), response_text)
                    else:
                        print("âš ï¸ nano-bananaå®˜æ–¹APIå“åº”ä¸­æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
                        # è¿”å›é»˜è®¤å›¾åƒ
                        default_image = Image.new('RGB', (1024, 1024), color='black')
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(default_image), response_text)
                else:
                    raise Exception("nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥")

            except Exception as e:
                print(f"âŒ nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
                raise e
            
        # 2. Comflyé•œåƒç«™å¤„ç†
        elif is_comfly_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°Comflyé•œåƒç«™ï¼Œä½¿ç”¨Comfly APIæ ¼å¼")

            # è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼ˆå»é™¤æ ‡è®°ï¼‰
            normalized_model = _normalize_model_name(model)

            if normalized_model in ["nano-banana", "nano-banana-hd", "fal-ai/nano-banana", "nano-banana/edit", "fal-ai/nano-banana/edit"]:
                # æ£€æŸ¥æ˜¯å¦æ˜¯fal-aiæ¨¡å‹
                if normalized_model.startswith("fal-ai/") or normalized_model.endswith("/edit"):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç¼–è¾‘æ¨¡å‹ä½†åœ¨ç”ŸæˆèŠ‚ç‚¹ä¸­ä½¿ç”¨
                    if normalized_model.endswith("/edit"):
                        print("âš ï¸ æ£€æµ‹åˆ°ç¼–è¾‘æ¨¡å‹åœ¨å›¾åƒç”ŸæˆèŠ‚ç‚¹ä¸­ä½¿ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç”Ÿæˆæ¨¡å‹")
                        # å°†ç¼–è¾‘æ¨¡å‹è½¬æ¢ä¸ºç”Ÿæˆæ¨¡å‹
                        generation_model = normalized_model.replace("/edit", "")
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, generation_model, enhanced_prompt, None, 1, seed, "image_url")
                    else:
                        # ä½¿ç”¨fal-aiç«¯ç‚¹
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, normalized_model, enhanced_prompt, None, 1, seed, "image_url")

                    # print(f"ğŸ” Comfly fal-aiå‡½æ•°è¿”å›ç»“æœ: {str(result)[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                    print(f"ğŸ” Comfly fal-aiå‡½æ•°è¿”å›ç»“æœç±»å‹: {type(result)}")
                    generated_image = None
                    response_text = ""

                    if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ Comfly fal-ai/nano-banana æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"Comfly fal-ai/nano-banana æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            if result['data']:
                                b64 = result['data'][0].get('b64_json')
                                image_url = result['data'][0].get('url', '')
                                response_text = result.get('response_text', "")

                                if image_url and image_url not in response_text:
                                    response_text += f"\nå›¾åƒURL: {image_url}"

                                if b64:
                                    from base64 import b64decode
                                    import io
                                    try:
                                        b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                        img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                    except Exception as decode_error:
                                        print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                        try:
                                            img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                        except Exception as e2:
                                            print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                            img = None
                                    generated_image = img
                                else:
                                    import re
                                    base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                    matches = re.findall(base64_pattern, response_text)
                                    if matches:
                                        from base64 import b64decode
                                        import io
                                        img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                        generated_image = img

                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨Gigapixel AIæ”¾å¤§åå†è¿”å›
                    if generated_image:
                        # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale
                                    upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                        image_tensor = pil_to_tensor(generated_image)
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆComfly fal-ai/nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)
                    else:
                        print(f"âš ï¸ APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                        raise Exception(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                else:
                    # ä½¿ç”¨åŸæœ‰çš„nano-bananaç«¯ç‚¹
                    try:
                        # ç”±äºç§»é™¤äº†sizeå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å°ºå¯¸"1024x1024"
                        result = _comfly_nano_banana_generate(api_url, api_key, normalized_model, enhanced_prompt, "1024x1024", temperature, top_p, max_output_tokens, seed)
                        # print(f"[DEBUG] result type: {type(result)}")
                        # print(f"[DEBUG] result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
                        # print(f"[DEBUG] result content: {str(result)[:500]}...")
                        generated_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                # å¦‚æœdataä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰ç”Ÿæˆå›¾åƒ
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ Comfly nano-banana æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"Comfly nano-banana æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            # å¦‚æœdataä¸ä¸ºç©ºï¼Œç»§ç»­å¤„ç†
                            if result['data']:
                                b64 = result['data'][0].get('b64_json')
                                image_url = result['data'][0].get('url', '')
                                response_text = result.get('response_text', "")

                                # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                                if image_url and image_url not in response_text:
                                    response_text += f"\nå›¾åƒURL: {image_url}"

                                # print(f"[DEBUG] å›¾åƒURL: {image_url}")
                                # print(f"[DEBUG] å“åº”æ–‡æœ¬: {response_text}")

                                if b64:
                                    from base64 import b64decode
                                    import io
                                    try:
                                        # ä¿®å¤base64å¡«å……é—®é¢˜
                                        b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                        img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                    except Exception as decode_error:
                                        print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                        # å°è¯•ç›´æ¥è§£ç 
                                        try:
                                            img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                        except Exception as e2:
                                            print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                            img = None
                                    generated_image = img
                                else:
                                    # å¦‚æœæ²¡æœ‰base64æ•°æ®ï¼Œå°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å›¾åƒ
                                    import re
                                    base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                    matches = re.findall(base64_pattern, response_text)
                                    if matches:
                                        from base64 import b64decode
                                        import io
                                        img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                        generated_image = img

                        # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨Gigapixel AIæ”¾å¤§åå†è¿”å›
                        if generated_image:
                            # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                            if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                                try:
                                    scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                    if scale > 1:
                                        print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                        try:
                                            from .banana_upscale import smart_upscale
                                        except ImportError:
                                            from banana_upscale import smart_upscale
                                        target_w = generated_image.width * scale
                                        target_h = generated_image.height * scale
                                        upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                        if upscaled_image:
                                            generated_image = upscaled_image
                                            print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                                except Exception as e:
                                    print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                            image_tensor = pil_to_tensor(generated_image)
                            print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆComfly nano-bananaï¼‰")
                            self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                            return (image_tensor, response_text)

                    except Exception as e:
                        print(f"âŒ Comfly(nano-banana) ç”Ÿæˆå¤±è´¥: {e}")
                        raise e
            else:
                # énano-bananaæ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼
                generation_config = {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"]
                }

                # ğŸ“ Aspect Ratioæ§åˆ¶
                if aspect_ratio and aspect_ratio != "1:1":
                    generation_config["imageConfig"] = {
                        "aspectRatio": aspect_ratio
                    }
                    print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

                # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
                if seed and seed > 0:
                    generation_config["seed"] = seed

                request_data = {
                    "model": normalized_model,  # ä½¿ç”¨è§„èŒƒåŒ–çš„æ¨¡å‹åç§°
                    "contents": [{
                        "parts": [{"text": enhanced_prompt}]
                    }],
                    "generationConfig": generation_config
                }

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key.strip()}"
                }

        # 3. T8é•œåƒç«™å¤„ç†
        elif is_t8_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨T8 APIæ ¼å¼")

            # è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼ˆå»é™¤æ ‡è®°ï¼‰
            normalized_model = _normalize_model_name(model)

            if normalized_model in ["nano-banana", "nano-banana-hd", "fal-ai/nano-banana", "nano-banana/edit", "fal-ai/nano-banana/edit"]:
                # æ£€æŸ¥æ˜¯å¦æ˜¯fal-aiæ¨¡å‹
                if normalized_model.startswith("fal-ai/") or normalized_model.endswith("/edit"):
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç¼–è¾‘æ¨¡å‹ä½†åœ¨ç”ŸæˆèŠ‚ç‚¹ä¸­ä½¿ç”¨
                    if normalized_model.endswith("/edit"):
                        print("âš ï¸ æ£€æµ‹åˆ°ç¼–è¾‘æ¨¡å‹åœ¨å›¾åƒç”ŸæˆèŠ‚ç‚¹ä¸­ä½¿ç”¨ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°ç”Ÿæˆæ¨¡å‹")
                        # å°†ç¼–è¾‘æ¨¡å‹è½¬æ¢ä¸ºç”Ÿæˆæ¨¡å‹
                        generation_model = normalized_model.replace("/edit", "")
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, generation_model, enhanced_prompt, None, 1, seed, "image_url")
                    else:
                        # T8é•œåƒç«™ä½¿ç”¨ä¸Comflyç›¸åŒçš„fal-aiç«¯ç‚¹è°ƒç”¨æ–¹å¼
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, normalized_model, enhanced_prompt, None, 1, seed, "image_url")

                    generated_image = None
                    response_text = ""

                    if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ T8 fal-ai/nano-banana æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"T8 fal-ai/nano-banana æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            # å¤„ç†è¿”å›çš„å›¾åƒæ•°æ®
                            b64 = result['data'][0].get('b64_json')
                            image_url = result['data'][0].get('url', '')
                            response_text = result.get('response_text', "")

                            # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                            if image_url and image_url not in response_text:
                                response_text += f"\nğŸ”— å›¾åƒURL: {image_url}"

                            if b64:
                                from base64 import b64decode
                                import io
                                try:
                                    b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                    generated_image = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                except Exception as e:
                                    print(f"âš ï¸ base64è§£ç å¤±è´¥: {e}")
                                    # å°è¯•ä»URLä¸‹è½½
                                    if image_url:
                                        try:
                                            import requests
                                            response = requests.get(image_url, timeout=30)
                                            if response.status_code == 200:
                                                generated_image = Image.open(io.BytesIO(response.content)).convert('RGB')
                                                print("âœ… æˆåŠŸä»URLä¸‹è½½å›¾åƒ")
                                            else:
                                                print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                        except Exception as e:
                                            print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")

                    if generated_image:
                        # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale
                                    upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                        print("âœ… T8 fal-aiå›¾ç‰‡ç”Ÿæˆå®Œæˆ")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(generated_image), response_text)
                    else:
                        raise Exception("T8 fal-ai/nano-banana æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›¾åƒ")

                # åŸç”Ÿnano-bananaæ¨¡å‹å¤„ç†
                elif _normalize_model_name(model) in ["nano-banana", "nano-banana-hd"]:
                    print("ğŸ”— T8é•œåƒç«™ä½¿ç”¨chat/completionsç«¯ç‚¹ (nano-banana ç›´è¿)")
                try:
                    result = _comfly_nano_banana_generate(api_url, api_key, _normalize_model_name(model), enhanced_prompt, "1024x1024", temperature, top_p, max_output_tokens, seed)
                    # ğŸ” è°ƒè¯•ï¼šæ‰“å°T8è¿”å›çš„ç»“æœæ ¼å¼ - å·²å…³é—­
                    # print(f"[DEBUG] T8 result type: {type(result)}")
                    # print(f"[DEBUG] T8 result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
                    # print(f"[DEBUG] T8 result content: {str(result)[:500]}...")

                    generated_image = None
                    response_text = ""
                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        image_url = result['data'][0].get('url', '')
                        response_text = result.get('response_text', "")

                        # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                        if image_url and image_url not in response_text:
                            response_text += f"\nå›¾åƒURL: {image_url}"

                        # print(f"[DEBUG] T8 å›¾åƒURL: {image_url}")
                        # print(f"[DEBUG] T8 å“åº”æ–‡æœ¬: {response_text}")

                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception:
                                img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                            generated_image = img
                    if generated_image:
                        # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale
                                    upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆT8 nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(generated_image), response_text)
                except Exception as e:
                    print(f"âŒ T8(nano-banana) ç”Ÿæˆå¤±è´¥: {e}")
                    raise e
            # å…¶ä»–æ¨¡å‹ä½¿ç”¨Geminiå®˜æ–¹æ ¼å¼ï¼ˆT8é•œåƒç«™æ”¯æŒGeminiæ ¼å¼ï¼‰
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨Gemini APIæ ¼å¼")

            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
                "responseModalities": ["TEXT", "IMAGE"]
            }

            # ğŸ“ Aspect Ratioæ§åˆ¶
            if aspect_ratio and aspect_ratio != "1:1":
                generation_config["imageConfig"] = {
                    "aspectRatio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            request_data = {
                "model": normalized_model,  # ä½¿ç”¨è§„èŒƒåŒ–çš„æ¨¡å‹åç§°
                "contents": [{
                    "parts": [{"text": enhanced_prompt}]
                }],
                "generationConfig": generation_config
            }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
            
        # 4. API4GPTé•œåƒç«™å¤„ç†ï¼ˆä½¿ç”¨æ ‡å‡† OpenAI å…¼å®¹æ ¼å¼ï¼‰
        elif is_api4gpt_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°API4GPTé•œåƒç«™ï¼Œä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼")

            # API4GPT çš„ nano-banana æœåŠ¡ä½¿ç”¨æ ‡å‡† OpenAI /v1/images/generations ç«¯ç‚¹
            # å‚è€ƒæ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
            request_data = {
                "model": _normalize_model_name(model),
                "prompt": enhanced_prompt,
                "n": 1
            }

            # æ„å»ºå®Œæ•´çš„ API URL
            full_url = f"{api_url}/v1/images/generations"
            print(f"ğŸ”— ä½¿ç”¨ API4GPT ç«¯ç‚¹: {full_url}")

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }

            try:
                # è°ƒç”¨ API4GPT API
                response = requests.post(
                    full_url,
                    headers=headers,
                    json=request_data,
                    proxies=proxies,
                    timeout=120
                )
                response.raise_for_status()

                # è§£æå“åº”ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
                result = response.json()
                print(f"âœ… API4GPT APIè°ƒç”¨æˆåŠŸ")
                print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")

                # ä» OpenAI å…¼å®¹æ ¼å¼ä¸­æå–å›¾åƒ URL
                if "data" in result and len(result["data"]) > 0:
                    image_url = result["data"][0].get("url")
                    if image_url:
                        print(f"ğŸ”— ä¸‹è½½å›¾åƒ: {image_url}")
                        image_response = requests.get(image_url, proxies=proxies, timeout=60)
                        image_response.raise_for_status()
                        import io
                        generated_image = Image.open(io.BytesIO(image_response.content))
                        print(f"âœ… æˆåŠŸä¸‹è½½API4GPTç”Ÿæˆçš„å›¾åƒ: {generated_image.size}")

                        # æå–å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
                        response_text = result["data"][0].get("revised_prompt", enhanced_prompt)

                        # âš ï¸ API4GPT ä¸æ”¯æŒ aspect_ratio å‚æ•°ï¼Œå§‹ç»ˆè¿”å› 1024x1024
                        if aspect_ratio and aspect_ratio != "1:1":
                            print(f"âš ï¸ æ³¨æ„ï¼šAPI4GPT ä¸æ”¯æŒ aspect_ratio å‚æ•°ï¼Œç”Ÿæˆçš„å›¾åƒä¸º 1024x1024 (1:1)")
                            print(f"ğŸ’¡ å»ºè®®ï¼šä½¿ç”¨å…¶ä»–é•œåƒç«™ï¼ˆå¦‚ OpenRouterã€å®˜æ–¹ APIï¼‰ä»¥æ”¯æŒè‡ªå®šä¹‰å®½é«˜æ¯”")
                    else:
                        raise ValueError("API4GPTå“åº”ä¸­æ²¡æœ‰å›¾åƒURL")
                else:
                    raise ValueError("API4GPTå“åº”æ ¼å¼é”™è¯¯")

                # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                if generated_image:
                    if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                        try:
                            scale = int(upscale_factor.replace("x", "").strip().split()[0])
                            if scale > 1:
                                print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                try:
                                    from .banana_upscale import smart_upscale
                                except ImportError:
                                    from banana_upscale import smart_upscale
                                target_w = generated_image.width * scale
                                target_h = generated_image.height * scale
                                upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                if upscaled_image:
                                    generated_image = upscaled_image
                                    print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                        except Exception as e:
                            print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                # è½¬æ¢ä¸ºtensor
                image_tensor = pil_to_tensor(generated_image)
                print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆAPI4GPTï¼‰")
                self._push_chat(enhanced_prompt, response_text or "", unique_id)
                return (image_tensor, response_text)

            except Exception as e:
                print(f"âŒ API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
                raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {e}")

        # 5. OpenRouteré•œåƒç«™å¤„ç†
        elif is_openrouter_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°OpenRouteré•œåƒç«™ï¼Œä½¿ç”¨OpenRouter APIæ ¼å¼")

            # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç”Ÿæˆ
            # æ„å»ºOpenRouterè¯·æ±‚æ ¼å¼
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": enhanced_prompt
                }],
                "modalities": ["image", "text"],  # OpenRouter å›¾åƒç”Ÿæˆå¿…éœ€å‚æ•°
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_output_tokens,
                "stream": True  # Required for gemini image models
            }

            # æ·»åŠ  image_configï¼ˆåŒ…å« aspect_ratioï¼‰
            if aspect_ratio and aspect_ratio != "1:1":
                request_data["image_config"] = {
                    "aspect_ratio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½® OpenRouter å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ  seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                if "image_config" not in request_data:
                    request_data["image_config"] = {}
                request_data["image_config"]["seed"] = seed
                print(f"ğŸ² è®¾ç½® OpenRouter seed: {seed}")

            # ä½¿ç”¨OpenRouterçš„chat/completionsç«¯ç‚¹
            if api_url.endswith('/v1'):
                full_url = f"{api_url}/chat/completions"
            else:
                full_url = f"{api_url}/v1/chat/completions"
            print(f"ğŸ”— ä½¿ç”¨OpenRouter chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç”Ÿæˆ: {full_url}")

            # è®¾ç½®OpenRouterè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}",
                "HTTP-Referer": "https://github.com/ComfyUI-LLM-Prompt",
                "X-Title": "ComfyUI LLM Prompt Plugin"
            }

            try:
                # å‘é€è¯·æ±‚
                response = requests.post(full_url, headers=headers, json=request_data, timeout=120, stream=True, verify=False)

                if response.status_code == 200:
                    # å¤„ç†æµå¼å“åº”
                    response_text = process_openrouter_stream(response)

                    # æ£€æŸ¥å“åº”æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                    generated_image = None
                    if "data:image/" in response_text:
                        print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouterè¿”å›çš„å›¾åƒæ•°æ®")
                        try:
                            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–base64å›¾åƒæ•°æ®
                            import re
                            base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                            image_matches = re.findall(base64_pattern, response_text)
                            if image_matches:
                                # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾åƒæ•°æ®
                                image_url = image_matches[0]
                                print(f"ğŸ¯ æˆåŠŸåŒ¹é…OpenRouterå›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_url)}å­—ç¬¦")

                                # æå–base64éƒ¨åˆ†
                                if ';base64,' in image_url:
                                    import io
                                    base64_data = image_url.split(';base64,', 1)[1]
                                    image_bytes = base64.b64decode(base64_data)
                                    generated_image = Image.open(io.BytesIO(image_bytes))
                                    print(f"âœ… æˆåŠŸæå–OpenRouterç”Ÿæˆçš„å›¾åƒ: {generated_image.size}")

                                    # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                    response_text = re.sub(base64_pattern, '[å›¾åƒå·²ç”Ÿæˆ]', response_text)
                        except Exception as e:
                            print(f"âš ï¸ OpenRouterå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")

                    if generated_image:
                        # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale
                                    upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆOpenRouterï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(generated_image), response_text)
                    else:
                        raise Exception("OpenRouteræœªè¿”å›å›¾åƒæ•°æ®")
                else:
                    raise Exception(f"OpenRouter APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")
            except Exception as e:
                print(f"âŒ OpenRouter APIè°ƒç”¨å¤±è´¥: {e}")
                raise ValueError(f"OpenRouter APIè°ƒç”¨å¤±è´¥: {e}")

        # ç»Ÿä¸€çš„è¯·æ±‚å‘é€é€»è¾‘ï¼ˆç”¨äºComfly/T8énano-bananaæ¨¡å‹ç­‰ï¼‰
        if 'request_data' in locals() and 'headers' in locals():
            print(f"ğŸ”— ä½¿ç”¨ç»Ÿä¸€è¯·æ±‚å‘é€é€»è¾‘")
            try:
                response = requests.post(
                    full_url,
                    headers=headers,
                    json=request_data,
                    proxies=proxies,
                    timeout=120,
                    verify=False
                )

                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
                    print(f"ğŸ“¡ å“åº”å¤´: {dict(response.headers)}")
                    print(f"ğŸ“¡ å“åº”å†…å®¹: {response.text[:500]}")

                if response.status_code == 200:
                    # è§£æJSONå“åº”
                    try:
                        result = response.json()
                        print("âœ… æˆåŠŸè§£æå“åº”")
                    except Exception as e:
                        print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
                        print(f"ğŸ“¡ åŸå§‹å“åº”å†…å®¹: {response.text[:1000]}")
                        raise

                    # æå–ç”Ÿæˆçš„å›¾åƒå’Œå“åº”æ–‡æœ¬
                    generated_image = None
                    response_text = ""

                    # ä½¿ç”¨æ ‡å‡†Gemini APIå“åº”å¤„ç†
                    print("ğŸ”— è§£æGeminiæ ¼å¼å“åº”")
                    if "candidates" in result and result["candidates"]:
                        candidate = result["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            for part in candidate["content"]["parts"]:
                                # æå–æ–‡æœ¬
                                if "text" in part:
                                    response_text += part["text"]

                                # æå–ç”Ÿæˆçš„å›¾ç‰‡ï¼ˆæ”¯æŒinline_dataå’ŒinlineDataä¸¤ç§æ ¼å¼ï¼‰
                                inline_data = part.get("inline_data") or part.get("inlineData")
                                if inline_data and "data" in inline_data:
                                    try:
                                        import io, base64
                                        image_data = inline_data["data"]
                                        image_bytes = base64.b64decode(image_data)
                                        generated_image = Image.open(io.BytesIO(image_bytes))
                                        print(f"âœ… æˆåŠŸæå–ç”Ÿæˆçš„å›¾åƒ: {generated_image.size}")
                                    except Exception as e:
                                        print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")

                    if generated_image:
                        # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale
                                    upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆç»Ÿä¸€è¯·æ±‚ï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(generated_image), response_text)
                    else:
                        raise Exception("æœªèƒ½ä»å“åº”ä¸­æå–å›¾åƒ")
                else:
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")

            except Exception as e:
                print(f"âŒ ç»Ÿä¸€è¯·æ±‚å‘é€å¤±è´¥: {e}")
                raise ValueError(f"ç»Ÿä¸€è¯·æ±‚å‘é€å¤±è´¥: {e}")

        else:
            # å…¶ä»–é•œåƒç«™å¤„ç†
            print(f"âš ï¸ æœªçŸ¥é•œåƒç«™ç±»å‹ï¼Œå°è¯•ä½¿ç”¨é€šç”¨APIæ ¼å¼")

            # å°è¯•ä½¿ç”¨é€šç”¨æ ¼å¼è°ƒç”¨
            try:
                # æ„å»ºgeneration_config
                generation_config = {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                }

                # Response Modalitiesæ§åˆ¶
                if response_modality == "IMAGE_ONLY":
                    generation_config["responseModalities"] = ["Image"]
                else:
                    generation_config["responseModalities"] = ["Text", "Image"]

                # Aspect Ratioæ§åˆ¶
                if aspect_ratio and aspect_ratio != "1:1":
                    generation_config["imageConfig"] = {
                        "aspectRatio": aspect_ratio
                    }

                if seed and seed != 0:
                    generation_config["seed"] = seed

                # æ„å»ºè¯·æ±‚ä½“
                request_body = {
                    "contents": [{
                        "parts": [{
                            "text": enhanced_prompt
                        }]
                    }],
                    "generationConfig": generation_config
                }

                # å‘é€è¯·æ±‚
                headers = {
                    "Content-Type": "application/json",
                    "x-goog-api-key": api_key
                }

                response = requests.post(
                    api_url,
                    headers=headers,
                    json=request_body,
                    timeout=120,
                    verify=False
                )

                if response.status_code == 200:
                    response_json = response.json()

                    # æå–ç”Ÿæˆçš„å›¾åƒ
                    generated_image = process_generated_image_from_response(response_json)
                    response_text = extract_text_from_response(response_json)

                    if generated_image:
                        # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                            try:
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale
                                    target_w = generated_image.width * scale
                                    target_h = generated_image.height * scale
                                    upscaled_image = smart_upscale(generated_image, target_w, target_h, gigapixel_model)
                                    if upscaled_image:
                                        generated_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆé€šç”¨æ ¼å¼ï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (pil_to_tensor(generated_image), response_text)
                    else:
                        raise Exception("æœªèƒ½ä»å“åº”ä¸­æå–å›¾åƒ")
                else:
                    raise Exception(f"APIè°ƒç”¨å¤±è´¥: {response.status_code} - {response.text}")

            except Exception as e:
                print(f"âŒ é€šç”¨APIè°ƒç”¨å¤±è´¥: {e}")
                raise ValueError(f"é€šç”¨APIè°ƒç”¨å¤±è´¥: {e}")


# ==================== èŠ‚ç‚¹5: å›¾ç”Ÿå›¾é•œåƒç«™èŠ‚ç‚¹ ====================

class KenChenLLMGeminiBananaMirrorImageEditNode:
    """Gemini Banana é•œåƒç«™å›¾ç‰‡ç¼–è¾‘èŠ‚ç‚¹

    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒé€‰æ‹©é¢„é…ç½®çš„é•œåƒç«™ï¼ˆofficial, comfly, customï¼‰
    - è‡ªåŠ¨å¡«å……å¯¹åº”é•œåƒç«™çš„ API URL å’Œ API Key
    - é€‰æ‹© custom æ—¶å¯æ‰‹åŠ¨è¾“å…¥è‡ªå®šä¹‰é•œåƒç«™ä¿¡æ¯
    - æ™ºèƒ½URLæ ¼å¼éªŒè¯å’Œè‡ªåŠ¨è¡¥å…¨
    """

    # è®¾ç½®èŠ‚ç‚¹é¢œè‰²ä¸ºæ©™è‰²/æ£•è‰²ç³»
    @classmethod
    def get_node_color(cls):
        return "#D2691E"  # å·§å…‹åŠ›æ©™è‰²

    @classmethod
    def get_node_bgcolor(cls):
        return "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯

    @classmethod
    def INPUT_TYPES(cls):
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        image_settings = config.get('image_settings', {})
        
        # è·å–é•œåƒç«™é…ç½®
        mirror_sites = config.get('mirror_sites', {})
        mirror_options = list(mirror_sites.keys())
        if not mirror_options:
            mirror_options = ["official", "comfly", "custom"]
        
        # è·å–é»˜è®¤é•œåƒç«™é…ç½®
        default_site = "comfly" if "comfly" in mirror_options else mirror_options[0] if mirror_options else "official"
        default_config = get_mirror_site_config(default_site)
        
        # ğŸš€ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶é¢„è®¾
        aspect_ratios = image_settings.get('aspect_ratios', [
            "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
        ])
        response_modalities = image_settings.get('response_modalities', [
            "TEXT_AND_IMAGE", "IMAGE_ONLY"
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„è¶…é«˜æ¸…é€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "mirror_site": (mirror_options, {"default": default_site}),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "é•œåƒç«™APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç•™ç©ºæ—¶è‡ªåŠ¨è·å–ï¼‰"
                }),
                "image": ("IMAGE",),
                "prompt": ("STRING", {"default": "Can you add a llama next to me?", "multiline": True}),
                # æ”¯æŒå¤šç§AIæ¨¡å‹å’Œå›¾åƒç¼–è¾‘æœåŠ¡: nano-bananaæ”¯æŒComflyå’ŒT8é•œåƒç«™, [All]æ”¯æŒæ‰€æœ‰é•œåƒç«™, API4GPTæ¨¡å‹, OpenRouteræ¨¡å‹
                "model": (["nano-banana [Comfly-T8]", "nano-banana-hd [Comfly-T8]", "gemini-2.5-flash-image [All]", "gemini-2.5-flash-image-preview [All]", "gemini-2.0-flash-preview-image-generation", "gemini-2.5-flash-image-hd [API4GPT]", "gemini-2.5-flash-image-vip [API4GPT]", "google/gemini-2.5-flash-image [OpenRouter]", "google/gemini-2.5-flash-image-preview [OpenRouter]"], {"default": "nano-banana [Comfly-T8]"}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),

                # ğŸ“ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶å‚æ•°
                "aspect_ratio": (aspect_ratios, {
                    "default": image_settings.get('default_aspect_ratio', "1:1"),
                    "tooltip": "å›¾åƒå®½é«˜æ¯” (Geminiå®˜æ–¹APIæ”¯æŒ)"
                }),
                "response_modality": (response_modalities, {
                    "default": image_settings.get('default_response_modality', "TEXT_AND_IMAGE"),
                    "tooltip": "å“åº”æ¨¡å¼ï¼šTEXT_AND_IMAGE=æ–‡å­—+å›¾åƒï¼ŒIMAGE_ONLY=ä»…å›¾åƒ"
                }),

                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),

                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),

                # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {"unique_id": "UNIQUE_ID"}
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    # è®¾ç½®èŠ‚ç‚¹é¢œè‰² - ä½¿ç”¨ComfyUIæ ‡å‡†å±æ€§
    color = "#D2691E"  # å·§å…‹åŠ›æ©™è‰²
    bgcolor = "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯
    groupcolor = "#CD853F"  # æ²™æ£•è‰²

    def __init__(self):
        # å¼ºåˆ¶è®¾ç½®é¢œè‰²å±æ€§
        self.color = "#D2691E"
        self.bgcolor = "#8B4513"
        self.groupcolor = "#CD853F"
    
    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            print(f"âš ï¸ æ— æ³•æ¨é€å¯¹è¯: PromptServer={PromptServer is not None}, unique_id={unique_id}")
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000

            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")

            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt,
                            "response": display_response,
                            "response_id": str(random.randint(100000, 999999)),
                            "timestamp": time.time()
                        }
                    ], ensure_ascii=False)
                },
            }
            print(f"ğŸ’¬ æ¨é€å¯¹è¯æ°”æ³¡åˆ°èŠ‚ç‚¹ {unique_id}")
            PromptServer.instance.send_sync("display_component", render_spec)
            print(f"âœ… å¯¹è¯æ°”æ³¡æ¨é€æˆåŠŸ")
        except Exception as e:
            print(f"âŒ [LLM Agent Assistant] Chat push failed: {e}")
            import traceback
            traceback.print_exc()
            pass
    
    def edit_image(self, mirror_site: str, api_key: str, image: torch.Tensor, prompt: str, model: str,
                    proxy: str, aspect_ratio: str, response_modality: str, quality: str, style: str,
                    detail_level: str, camera_control: str, lighting_control: str, template_selection: str,
                    upscale_factor: str, gigapixel_model: str, temperature: float, top_p: float, top_k: int, max_output_tokens: int, seed: int,
                    custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨é•œåƒç«™APIç¼–è¾‘å›¾ç‰‡"""

        # ğŸ”§ ç¡®ä¿requestsæ¨¡å—å¯ç”¨
        import requests
        
        # ğŸš€ ç«‹å³è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå»é™¤UIæ ‡è¯†
        model = _normalize_model_name(model)
        
        # æ ¹æ®é•œåƒç«™ä»é…ç½®è·å–URLå’ŒAPI Key
        site_config = get_mirror_site_config(mirror_site) if mirror_site else {"url": "", "api_key": ""}
        api_url = site_config.get("url", "").strip()
        if site_config.get("api_key") and not api_key.strip():
            api_key = site_config["api_key"]
            print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™API Key: {api_key[:8]}...")
        if not api_url:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘è¯¥é•œåƒç«™çš„API URL")
        print(f"ğŸ”— è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™URL: {api_url}")
        
        if not validate_api_url(api_url):
            raise ValueError("API URLæ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†å›¾åƒæ§åˆ¶å‚æ•°
        try:
            from .gemini_banana import process_image_controls, enhance_prompt_with_controls
        except ImportError:
            from gemini_banana import process_image_controls, enhance_prompt_with_controls
        controls = process_image_controls(quality, style)

        # ğŸš€ è°ƒè¯•ï¼šæ˜¾ç¤ºå‚æ•°ä¼ é€’è¿‡ç¨‹
        print(f"ğŸ” å‚æ•°ä¼ é€’è°ƒè¯•:")
        print(f"  - èŠ‚ç‚¹aspect_ratioå‚æ•°: {aspect_ratio}")
        print(f"  - èŠ‚ç‚¹qualityå‚æ•°: {quality}")
        print(f"  - èŠ‚ç‚¹styleå‚æ•°: {style}")

        # ä½¿ç”¨enhance_prompt_with_controlså‡½æ•°è¿›è¡Œå®Œæ•´çš„æç¤ºè¯å¢å¼º
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(),
            controls,
            detail_level,
            camera_control,
            lighting_control,
            template_selection,
            quality_enhancement="Auto",  # é»˜è®¤å€¼
            enhance_quality=True,  # é»˜è®¤å€¼
            smart_resize=True,  # é»˜è®¤å€¼
            fill_color="white"  # é»˜è®¤å€¼
        )

        # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
        if custom_additions and custom_additions.strip():
            enhanced_prompt += f"\n\n{custom_additions.strip()}"
            print(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_additions[:100]}...")

        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: è´¨é‡={controls['quality']}, é£æ ¼={controls['style']}")
        
        # è½¬æ¢è¾“å…¥å›¾ç‰‡
        pil_image = tensor_to_pil(image)
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥ç¬¦åˆAPIè¦æ±‚
        pil_image = resize_image_for_api(pil_image)
        
        # è½¬æ¢ä¸ºbase64
        image_base64 = image_to_base64(pil_image, format='JPEG')

        # è®¾ç½®ä»£ç†ï¼ˆç”¨äº requests åº“ï¼‰
        proxies = None
        if proxy and proxy.strip() and "None" not in proxy:
            proxies = {
                'http': proxy.strip(),
                'https': proxy.strip()
            }
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
            print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
        else:
            existing = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
            if existing:
                print(f"ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼Œæ²¿ç”¨ç³»ç»Ÿä»£ç†: {existing}")
            else:
                print("ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼ˆç³»ç»Ÿæ— ä»£ç†ï¼‰")
        
        # æ£€æŸ¥é•œåƒç«™ç±»å‹ - æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        is_nano_banana_official = mirror_site == "nano-bananaå®˜æ–¹"
        is_t8_mirror = "t8star.cn" in api_url or "ai.t8star.cn" in api_url
        is_api4gpt_mirror = "api4gpt.com" in api_url or "[API4GPT]" in model
        is_comfly_mirror = _is_comfly_base(api_url)
        is_openrouter_mirror = "openrouter.ai" in api_url or "[OpenRouter]" in model
        is_openai_mirror = "api.openai.com" in api_url or site_config.get("api_format") == "openai"

        # å¦‚æœæ£€æµ‹åˆ° OpenRouter æ¨¡å‹ä½† URL ä¸æ˜¯ OpenRouterï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter
        if "[OpenRouter]" in model and "openrouter.ai" not in api_url:
            api_url = "https://openrouter.ai/api/v1"
            is_openrouter_mirror = True
            print(f"ğŸ”„ æ£€æµ‹åˆ° OpenRouter æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter API: {api_url}")

        # å¦‚æœæ£€æµ‹åˆ° API4GPT æ¨¡å‹ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ URL
        if "[API4GPT]" in model:
            if "one.api4gpt.com" not in api_url:
                api_url = "https://one.api4gpt.com"
                is_api4gpt_mirror = True
                print(f"ğŸ”„ æ£€æµ‹åˆ° API4GPT æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° API4GPT API: {api_url}")
        # å¦‚æœ URL åŒ…å« api4gpt.com ä½†ä¸æ˜¯ one.api4gpt.comï¼Œä¹Ÿè¦åˆ‡æ¢
        elif "api4gpt.com" in api_url and "one.api4gpt.com" not in api_url:
            api_url = "https://one.api4gpt.com"
            is_api4gpt_mirror = True
            print(f"ğŸ”„ æ£€æµ‹åˆ°é”™è¯¯çš„ API4GPT URLï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æ­£ç¡®çš„ URL: {api_url}")

        # æ„å»ºå®Œæ•´çš„API URLï¼ˆOpenRouteré™¤å¤–ï¼Œå› ä¸ºå®ƒåœ¨å„è‡ªçš„å¤„ç†é€»è¾‘ä¸­æ„å»ºï¼‰
        if not is_openrouter_mirror:
            full_url = build_api_url(api_url, model)
            print(f"ğŸŒ ä½¿ç”¨APIåœ°å€: {full_url}")
        else:
            print(f"ğŸŒ OpenRouteré•œåƒç«™ï¼ŒURLå°†åœ¨OpenRouterå¤„ç†é€»è¾‘ä¸­æ„å»º")
        
        # æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºå¤„ç†é•œåƒç«™ï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        
        # 1. nano-bananaå®˜æ–¹é•œåƒç«™å¤„ç†
        if is_nano_banana_official:
            print("ğŸ”— æ£€æµ‹åˆ°nano-bananaå®˜æ–¹é•œåƒç«™ï¼Œä½¿ç”¨Googleå®˜æ–¹API")

            # æ„å»ºå†…å®¹éƒ¨åˆ†ï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰
            content_parts = [
                {"text": enhanced_prompt},
                {
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": image_base64
                    }
                }
            ]

            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
            }

            # ğŸ¯ Geminiå®˜æ–¹APIï¼šResponse Modalitiesæ§åˆ¶
            if response_modality == "IMAGE_ONLY":
                generation_config["responseModalities"] = ["Image"]
                print("ğŸ“Š å“åº”æ¨¡å¼ï¼šä»…å›¾åƒï¼ˆIMAGE_ONLYï¼‰")
            else:
                generation_config["responseModalities"] = ["Text", "Image"]
                print("ğŸ“Š å“åº”æ¨¡å¼ï¼šæ–‡å­—+å›¾åƒï¼ˆTEXT_AND_IMAGEï¼‰")

            # ğŸ“ Geminiå®˜æ–¹APIï¼šAspect Ratioæ§åˆ¶
            if aspect_ratio and aspect_ratio != "1:1":
                generation_config["imageConfig"] = {
                    "aspectRatio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            try:
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨ï¼ˆå®˜æ–¹APIä¼˜å…ˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST APIï¼‰
                response_json = generate_with_priority_api(
                    api_key=api_key,
                    model=_normalize_model_name(model),
                    content_parts=content_parts,
                    generation_config=generation_config,
                    max_retries=5,
                    proxy=proxy
                )

                if response_json:
                    # æå–ç¼–è¾‘åçš„å›¾åƒ
                    edited_image = process_generated_image_from_response(response_json)

                    # æå–å“åº”æ–‡æœ¬
                    response_text = extract_text_from_response(response_json)

                    if edited_image:
                        # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                            try:
                                # æå–æ”¾å¤§å€æ•°
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                    # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale

                                    # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                    target_w = edited_image.width * scale
                                    target_h = edited_image.height * scale

                                    # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                    upscaled_image = smart_upscale(
                                        edited_image,
                                        target_w,
                                        target_h,
                                        gigapixel_model
                                    )

                                    if upscaled_image:
                                        edited_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                    else:
                                        print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆnano-bananaå®˜æ–¹ï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)
                    else:
                        print("âš ï¸ nano-bananaå®˜æ–¹APIå“åº”ä¸­æœªæ‰¾åˆ°ç¼–è¾‘åçš„å›¾åƒæ•°æ®")
                        # è¿”å›åŸå›¾åƒ
                        return (image, response_text)
                else:
                    raise Exception("nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥")

            except Exception as e:
                print(f"âŒ nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
                raise e
            
        # 2. Comflyé•œåƒç«™å¤„ç†
        elif is_comfly_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°Comflyé•œåƒç«™ï¼Œä½¿ç”¨Comfly APIæ ¼å¼")

            # è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼ˆå»é™¤æ ‡è®°ï¼‰
            normalized_model = _normalize_model_name(model)

            if normalized_model in ["nano-banana", "nano-banana-hd", "fal-ai/nano-banana", "nano-banana/edit", "fal-ai/nano-banana/edit"] and pil_image is not None:
                # æ£€æŸ¥æ˜¯å¦æ˜¯fal-aiæ¨¡å‹
                if normalized_model.startswith("fal-ai/") or normalized_model.endswith("/edit"):
                    # ä½¿ç”¨fal-aiç«¯ç‚¹è¿›è¡Œç¼–è¾‘
                    try:
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, normalized_model, enhanced_prompt, [pil_image], 1, seed, "image_url")
                        edited_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ Comfly fal-ai/nano-banana ç¼–è¾‘æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"Comfly fal-ai/nano-banana ç¼–è¾‘æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            if result['data']:
                                b64 = result['data'][0].get('b64_json')
                                image_url = result['data'][0].get('url', '')
                                response_text = result.get('response_text', "")

                                if image_url and image_url not in response_text:
                                    response_text += f"\nå›¾åƒURL: {image_url}"

                                if b64:
                                    from base64 import b64decode
                                    import io
                                    try:
                                        b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                        edited_image = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                    except Exception as decode_error:
                                        print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                        try:
                                            edited_image = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                        except Exception as e2:
                                            print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                            edited_image = None
                                else:
                                    import re
                                    base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                    matches = re.findall(base64_pattern, response_text)
                                    if matches:
                                        from base64 import b64decode
                                        import io
                                        edited_image = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                        else:
                            print(f"âš ï¸ APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                            raise Exception(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")

                        # å¦‚æœæˆåŠŸç¼–è¾‘ï¼Œåº”ç”¨æ™ºèƒ½æ”¾å¤§
                        if edited_image:
                            # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                            if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                                try:
                                    # æå–æ”¾å¤§å€æ•°
                                    scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                    if scale > 1:
                                        print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                        # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                        try:
                                            from .banana_upscale import smart_upscale
                                        except ImportError:
                                            from banana_upscale import smart_upscale

                                        # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                        target_w = edited_image.width * scale
                                        target_h = edited_image.height * scale

                                        # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                        upscaled_image = smart_upscale(
                                            edited_image,
                                            target_w,
                                            target_h,
                                            gigapixel_model
                                        )

                                        if upscaled_image:
                                            edited_image = upscaled_image
                                            print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                        else:
                                            print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                                except Exception as e:
                                    print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                            image_tensor = pil_to_tensor(edited_image)
                            print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆComfly fal-ai/nano-bananaï¼‰")
                            self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                            return (image_tensor, response_text)
                        else:
                            print("âš ï¸ fal-ai/nano-bananaç¼–è¾‘åæœªè·å¾—æœ‰æ•ˆå›¾åƒ")
                            return (image, response_text)

                    except Exception as e:
                        print(f"âŒ Comfly fal-ai/nano-banana ç¼–è¾‘å¤±è´¥: {e}")
                        raise e
                else:
                    # ä½¿ç”¨åŸæœ‰çš„nano-bananaç«¯ç‚¹è¿›è¡Œç¼–è¾‘
                    try:
                        result = _comfly_nano_banana_edit(api_url, api_key, normalized_model, enhanced_prompt, [pil_image], controls['size'], temperature, top_p, max_output_tokens, seed)
                        edited_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result and result['data']:
                            b64 = result['data'][0].get('b64_json')
                            response_text = result.get('response_text', "")

                            if b64:
                                from base64 import b64decode
                                import io
                                try:
                                    # ä¿®å¤base64å¡«å……é—®é¢˜
                                    b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                    img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                except Exception as decode_error:
                                    print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                    # å°è¯•ç›´æ¥è§£ç 
                                    try:
                                        img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                    except Exception as e2:
                                        print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                        img = None
                                edited_image = img
                            else:
                                # å¦‚æœæ²¡æœ‰base64æ•°æ®ï¼Œå°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å›¾åƒ
                                import re
                                base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                matches = re.findall(base64_pattern, response_text)
                                if matches:
                                    from base64 import b64decode
                                    import io
                                    img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                    edited_image = img

                        # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨æ™ºèƒ½æ”¾å¤§
                        if edited_image:
                            # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                            if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                                try:
                                    # æå–æ”¾å¤§å€æ•°
                                    scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                    if scale > 1:
                                        print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                        # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                        try:
                                            from .banana_upscale import smart_upscale
                                        except ImportError:
                                            from banana_upscale import smart_upscale

                                        # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                        target_w = edited_image.width * scale
                                        target_h = edited_image.height * scale

                                        # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                        upscaled_image = smart_upscale(
                                            edited_image,
                                            target_w,
                                            target_h,
                                            gigapixel_model
                                        )

                                        if upscaled_image:
                                            edited_image = upscaled_image
                                            print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                        else:
                                            print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                                except Exception as e:
                                    print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                            image_tensor = pil_to_tensor(edited_image)
                            print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆComfly nano-bananaï¼‰")
                            self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                            return (image_tensor, response_text)

                    except Exception as e:
                        print(f"âŒ Comfly(nano-banana) ç¼–è¾‘å¤±è´¥: {e}")
                        raise e
            else:
                # énano-bananaæ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼
                generation_config = {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"]
                }

                # ğŸ“ Aspect Ratioæ§åˆ¶
                if aspect_ratio and aspect_ratio != "1:1":
                    generation_config["imageConfig"] = {
                        "aspectRatio": aspect_ratio
                    }
                    print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

                # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
                if seed and seed > 0:
                    generation_config["seed"] = seed

                request_data = {
                "model": normalized_model,  # ä½¿ç”¨è§„èŒƒåŒ–çš„æ¨¡å‹åç§°
                "contents": [{
                    "parts": [
                        {"text": enhanced_prompt},
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": image_base64
                            }
                        }
                    ]
                }],
                "generationConfig": generation_config
            }

            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
                
        # 3. T8é•œåƒç«™å¤„ç†
        elif is_t8_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨T8 APIæ ¼å¼")

            # è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼ˆå»é™¤æ ‡è®°ï¼‰
            normalized_model = _normalize_model_name(model)

            if normalized_model in ["nano-banana", "nano-banana-hd", "fal-ai/nano-banana", "nano-banana/edit", "fal-ai/nano-banana/edit"] and pil_image is not None:
                # æ£€æŸ¥æ˜¯å¦æ˜¯fal-aiæ¨¡å‹
                if normalized_model.startswith("fal-ai/") or normalized_model.endswith("/edit"):
                    # T8é•œåƒç«™ä½¿ç”¨ä¸Comflyç›¸åŒçš„fal-aiç«¯ç‚¹è°ƒç”¨æ–¹å¼
                    try:
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, normalized_model, enhanced_prompt, [pil_image], 1, seed, "image_url")
                        edited_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ T8 fal-ai/nano-banana ç¼–è¾‘æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"T8 fal-ai/nano-banana ç¼–è¾‘æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            # å¤„ç†è¿”å›çš„å›¾åƒæ•°æ®
                            b64 = result['data'][0].get('b64_json')
                            image_url = result['data'][0].get('url', '')
                            response_text = result.get('response_text', "")

                            # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                            if image_url and image_url not in response_text:
                                response_text += f"\nğŸ”— å›¾åƒURL: {image_url}"

                            if b64:
                                from base64 import b64decode
                                import io
                                try:
                                    b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                    edited_image = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                except Exception as e:
                                    print(f"âš ï¸ base64è§£ç å¤±è´¥: {e}")
                                    # å°è¯•ä»URLä¸‹è½½
                                    if image_url:
                                        try:
                                            import requests
                                            response = requests.get(image_url, timeout=30)
                                            if response.status_code == 200:
                                                edited_image = Image.open(io.BytesIO(response.content)).convert('RGB')
                                                print("âœ… æˆåŠŸä»URLä¸‹è½½ç¼–è¾‘åå›¾åƒ")
                                            else:
                                                print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                        except Exception as e:
                                            print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")

                        if edited_image:
                            # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                            try:
                                print("ğŸš€ å¼€å§‹åº”ç”¨T8 fal-aiå›¾åƒç¼–è¾‘å¢å¼ºæŠ€æœ¯...")
                                enhanced_image = _apply_full_enhancements(
                                    edited_image,
                                    controls['size'],
                                    quality,
                                    enhance_quality,
                                    smart_resize
                                )
                                if enhanced_image:
                                    edited_image = enhanced_image
                                    print(f"âœ… T8 fal-aiå›¾åƒç¼–è¾‘å¢å¼ºå®Œæˆ")
                                    try:
                                        print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                                    except Exception:
                                        pass
                                else:
                                    print("âš ï¸ T8 fal-aiå›¾åƒç¼–è¾‘å¢å¼ºè¿”å›ç©ºç»“æœï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                            except Exception as e:
                                print(f"âš ï¸ T8 fal-aiå›¾åƒç¼–è¾‘å¢å¼ºå¤±è´¥: {e}")
                                print("âš ï¸ ä½¿ç”¨åŸå§‹å›¾åƒç»§ç»­å¤„ç†")

                            print("âœ… T8 fal-aiå›¾ç‰‡ç¼–è¾‘å®Œæˆ")
                            return (pil_to_tensor(edited_image), response_text)
                        else:
                            raise Exception("T8 fal-ai/nano-banana æœªèƒ½ç¼–è¾‘å›¾åƒ")

                    except Exception as e:
                        print(f"âŒ T8 fal-ai/nano-banana ç¼–è¾‘å¤±è´¥: {e}")
                        raise e

                # åŸç”Ÿnano-bananaæ¨¡å‹å¤„ç†
                elif _normalize_model_name(model) in ["nano-banana", "nano-banana-hd"]:
                    print("ğŸ”— T8é•œåƒç«™ä½¿ç”¨chat/completionsç«¯ç‚¹ (nano-banana ç›´è¿)")
                try:
                    result = _comfly_nano_banana_edit(full_url, api_key, _normalize_model_name(model), enhanced_prompt, [pil_image], controls['size'], temperature, top_p, max_output_tokens, seed)
                    edited_image = None
                    response_text = ""

                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        response_text = result.get('response_text', "")

                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception:
                                img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                            edited_image = img

                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨æ™ºèƒ½æ”¾å¤§
                    if edited_image:
                        # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                            try:
                                # æå–æ”¾å¤§å€æ•°
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                    # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale

                                    # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                    target_w = edited_image.width * scale
                                    target_h = edited_image.height * scale

                                    # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                    upscaled_image = smart_upscale(
                                        edited_image,
                                        target_w,
                                        target_h,
                                        gigapixel_model
                                    )

                                    if upscaled_image:
                                        edited_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                    else:
                                        print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆT8 nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)

                except Exception as e:
                    print(f"âŒ T8(nano-banana) ç¼–è¾‘å¤±è´¥: {e}")
                    raise e
            else:
                # å…¶ä»–æ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼ï¼ˆä¸Comflyå¯¹é½ï¼‰
                print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨Gemini APIæ ¼å¼")

                generation_config = {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"]
                }

                # ğŸ“ Aspect Ratioæ§åˆ¶
                if aspect_ratio and aspect_ratio != "1:1":
                    generation_config["imageConfig"] = {
                        "aspectRatio": aspect_ratio
                    }
                    print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

                # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
                if seed and seed > 0:
                    generation_config["seed"] = seed

                request_data = {
                    "model": normalized_model,
                    "contents": [{
                        "parts": [
                            {"text": enhanced_prompt},
                            {
                                "inline_data": {
                                    "mime_type": "image/jpeg",
                                    "data": image_base64
                                }
                            }
                        ]
                    }],
                    "generationConfig": generation_config
                }

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key.strip()}"
                }
            
        # 4. API4GPTé•œåƒç«™å¤„ç†ï¼ˆä½¿ç”¨æ ‡å‡† OpenAI å…¼å®¹æ ¼å¼ï¼‰
        elif is_api4gpt_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°API4GPTé•œåƒç«™ï¼Œä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼")

            # API4GPT çš„ nano-banana æœåŠ¡ä½¿ç”¨æ ‡å‡† OpenAI /v1/images/edits ç«¯ç‚¹
            # å‚è€ƒæ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609442

            # å‡†å¤‡ multipart/form-data è¯·æ±‚
            try:
                # å°†å›¾åƒè½¬æ¢ä¸ºå­—èŠ‚æµ
                import io
                image_bytes = io.BytesIO()
                pil_image.save(image_bytes, format='PNG')
                image_bytes.seek(0)

                # æ„å»ºå®Œæ•´çš„ API URL
                full_url = f"{api_url}/v1/images/edits"
                print(f"ğŸ”— ä½¿ç”¨ API4GPT ç«¯ç‚¹: {full_url}")

                # å‡†å¤‡ multipart/form-data
                files = {
                    'image': ('image.png', image_bytes, 'image/png')
                }
                data = {
                    'prompt': enhanced_prompt,
                    'model': _normalize_model_name(model),
                    'n': 1
                }

                headers = {
                    "Authorization": f"Bearer {api_key.strip()}"
                }

                # è°ƒç”¨ API4GPT APIï¼ˆå›¾ç”Ÿå›¾éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                print(f"â±ï¸ å¼€å§‹è°ƒç”¨ API4GPT å›¾åƒç¼–è¾‘ APIï¼ˆå¯èƒ½éœ€è¦1-3åˆ†é’Ÿï¼‰...")
                response = requests.post(
                    full_url,
                    headers=headers,
                    files=files,
                    data=data,
                    proxies=proxies,
                    timeout=300  # å¢åŠ åˆ°5åˆ†é’Ÿï¼Œå› ä¸ºå›¾ç”Ÿå›¾éœ€è¦ä¸Šä¼ å›¾ç‰‡
                )
                response.raise_for_status()

                # è§£æå“åº”ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
                result = response.json()
                print(f"âœ… API4GPT å›¾åƒç¼–è¾‘ APIè°ƒç”¨æˆåŠŸ")
                print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")

                # ä» OpenAI å…¼å®¹æ ¼å¼ä¸­æå–å›¾åƒ URL
                if "data" in result and len(result["data"]) > 0:
                    image_url = result["data"][0].get("url")
                    if image_url:
                        print(f"ğŸ”— ä¸‹è½½ç¼–è¾‘åçš„å›¾åƒ: {image_url}")
                        image_response = requests.get(image_url, proxies=proxies, timeout=60)
                        image_response.raise_for_status()
                        import io
                        edited_image = Image.open(io.BytesIO(image_response.content))
                        print(f"âœ… æˆåŠŸä¸‹è½½API4GPTç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")

                        # æå–å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
                        response_text = result["data"][0].get("revised_prompt", enhanced_prompt)
                    else:
                        raise ValueError("API4GPTå“åº”ä¸­æ²¡æœ‰å›¾åƒURL")
                else:
                    raise ValueError("API4GPTå“åº”æ ¼å¼é”™è¯¯")

                # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                    try:
                        scale = int(upscale_factor.replace("x", "").strip().split()[0])
                        if scale > 1:
                            print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                            try:
                                from .banana_upscale import smart_upscale
                            except ImportError:
                                from banana_upscale import smart_upscale
                            target_w = edited_image.width * scale
                            target_h = edited_image.height * scale
                            upscaled_image = smart_upscale(edited_image, target_w, target_h, gigapixel_model)
                            if upscaled_image:
                                edited_image = upscaled_image
                                print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                    except Exception as e:
                        print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                # è½¬æ¢ä¸ºtensor
                image_tensor = pil_to_tensor(edited_image)
                print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆAPI4GPTï¼‰")
                self._push_chat(enhanced_prompt, response_text or "", unique_id)
                return (image_tensor, response_text)

            except Exception as e:
                print(f"âŒ API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
                raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
        elif is_openrouter_mirror:
            # OpenRouteré•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenRouteré•œåƒç«™ï¼Œä½¿ç”¨OpenRouter APIæ ¼å¼")
            
            # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç¼–è¾‘
            # æ„å»ºOpenAIå…¼å®¹çš„è¯·æ±‚æ ¼å¼
            content = []
            
            # æ·»åŠ å›¾åƒå†…å®¹
            image_url = f"data:image/jpeg;base64,{image_base64}"
            content.append({
                "type": "image_url",
                "image_url": {"url": image_url}
            })
            
            # æ·»åŠ æ–‡æœ¬æŒ‡ä»¤
            enhanced_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual edited image, not just text description.

Task: {enhanced_prompt}

REQUIREMENTS:
1. EDIT the provided image based on my request
2. DO NOT just describe what the edited image should look like
3. RETURN the actual edited image file/data
4. The output MUST be a visual image, not text

Execute the image editing task now and return the edited image."""
            
            content.append({
                "type": "text",
                "text": enhanced_instruction
            })
            
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": content
                }],
                "modalities": ["image", "text"],  # OpenRouter å›¾åƒç”Ÿæˆå¿…éœ€å‚æ•°
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_output_tokens,
                "stream": True  # Required for gemini-2.5-flash-image-preview
            }

            # æ·»åŠ  image_configï¼ˆåŒ…å« aspect_ratioï¼‰
            if aspect_ratio and aspect_ratio != "1:1":
                request_data["image_config"] = {
                    "aspect_ratio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½® OpenRouter å›¾ç”Ÿå›¾å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ  seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                if "image_config" not in request_data:
                    request_data["image_config"] = {}
                request_data["image_config"]["seed"] = seed
                print(f"ğŸ² è®¾ç½® OpenRouter å›¾ç”Ÿå›¾ seed: {seed}")

            # ä½¿ç”¨OpenRouterçš„chat/completionsç«¯ç‚¹
            # å¯¹äºOpenRouterï¼Œapi_urlå·²ç»åŒ…å«äº†/v1ï¼Œæ‰€ä»¥ç›´æ¥æ·»åŠ /chat/completions
            if api_url.endswith('/v1'):
                full_url = f"{api_url}/chat/completions"
            else:
                full_url = f"{api_url}/v1/chat/completions"
            print(f"ğŸ”— ä½¿ç”¨OpenRouter chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç¼–è¾‘: {full_url}")
            
            # è®¾ç½®OpenRouterè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}",
                "HTTP-Referer": "https://github.com/ComfyUI-LLM-Prompt",
                "X-Title": "ComfyUI LLM Prompt Plugin"
            }
        elif is_openai_mirror:
            # OpenAIé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": enhanced_prompt
                }],
                "temperature": temperature,
                "max_tokens": max_output_tokens,
                "stream": True
            }
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        else:
            # æ ‡å‡†Gemini APIæ ¼å¼
            request_data = {
                "contents": [{
                    "parts": [
                        {
                            "text": enhanced_prompt
                        },
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": image_base64
                            }
                        }
                    ]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        
        # æ™ºèƒ½é‡è¯•æœºåˆ¶ - å®Œå…¨ç§»æ¤å‚è€ƒé¡¹ç›®
        max_retries = 5
        timeout = 120
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç¼–è¾‘å›¾ç‰‡... (å°è¯• {attempt + 1}/{max_retries})")
                print(f"ğŸ“ ç¼–è¾‘æŒ‡ä»¤: {enhanced_prompt[:100]}...") # ä½¿ç”¨å¢å¼ºåçš„æç¤ºè¯
                print(f"ğŸ”— é•œåƒç«™: {api_url}")
                
                # å‘é€è¯·æ±‚ - æ·»åŠ SSLé…ç½®ä»¥è§£å†³è¿æ¥é—®é¢˜
                response = requests.post(full_url, headers=headers, json=request_data, timeout=timeout, stream=True, verify=False)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
                    print(f"ğŸ“¡ å“åº”å¤´: {dict(response.headers)}")

                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    # ä¸ºæ‰€æœ‰é•œåƒç«™å®šä¹‰ result å˜é‡
                    if not is_api4gpt_mirror and not is_openrouter_mirror and not is_t8_mirror and not is_openai_mirror:
                        # æ ‡å‡† Gemini API é•œåƒç«™ï¼ˆå¦‚ Comfy APIï¼‰
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£ææ ‡å‡† Gemini API å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ è§£æå“åº”å¤±è´¥: {e}")
                            result = {}
                    elif is_t8_mirror:
                        # T8 é•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ T8 é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ T8 é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ OpenAI é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ OpenAI é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    
                    if is_api4gpt_mirror:
                        # API4GPTé•œåƒç«™å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†API4GPTé•œåƒç«™å“åº”")
                        
                        try:
                            # å°è¯•è§£æJSONå“åº”
                            result = response.json()
                            print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")
                            
                            if api4gpt_service == "nano-banana":
                                # nano-bananaä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
                                response_text, edited_image = parse_openai_compatible_response(result)
                            else:
                                # å…¶ä»–æœåŠ¡ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
                                response_text, edited_image = parse_api4gpt_response(result, api4gpt_service)
                            
                            if edited_image:
                                print(f"âœ… æˆåŠŸæå–API4GPTç¼–è¾‘åçš„å›¾åƒ")
                            else:
                                print("âš ï¸ API4GPTæœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                                edited_image = pil_image
                                if not response_text:
                                    response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                        except Exception as json_error:
                            print(f"âš ï¸ API4GPT JSONè§£æå¤±è´¥: {json_error}")
                            # print(f"ğŸ“‹ åŸå§‹å“åº”å†…å®¹: {response.text[:500]}...")  # æ³¨é‡Šæ‰å†—é•¿çš„è°ƒè¯•ä¿¡æ¯
                            edited_image = pil_image
                            response_text = f"API4GPTå“åº”è§£æå¤±è´¥: {json_error}"
                    elif is_openrouter_mirror:
                        # OpenRouteré•œåƒç«™å“åº”å¤„ç† - ä½¿ç”¨æµå¼å“åº”
                        print("ğŸ”— å¤„ç†OpenRouteré•œåƒç«™æµå¼å“åº”")
                        
                        # å¤„ç†æµå¼å“åº”
                        response_text = process_openrouter_stream(response)
                        
                        # æ£€æŸ¥å“åº”æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                        if "data:image/" in response_text:
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouterè¿”å›çš„å›¾åƒæ•°æ®")
                            try:
                                # ä½¿ç”¨æ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–base64å›¾åƒæ•°æ®
                                import re
                                base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                image_matches = re.findall(base64_pattern, response_text)
                                if image_matches:
                                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾åƒæ•°æ®
                                    image_url = image_matches[0]
                                    print(f"ğŸ¯ æˆåŠŸåŒ¹é…OpenRouterå›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_url)}å­—ç¬¦")
                                    
                                    # æå–base64éƒ¨åˆ†
                                    if ';base64,' in image_url:
                                        import io
                                        base64_data = image_url.split(';base64,', 1)[1]
                                        image_bytes = base64.b64decode(base64_data)
                                        edited_image = Image.open(io.BytesIO(image_bytes))
                                        print(f"âœ… æˆåŠŸæå–OpenRouterç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")
                                        
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(base64_pattern, '[å›¾åƒå·²ç¼–è¾‘]', response_text)
                                    else:
                                        print(f"âš ï¸ å›¾åƒæ•°æ®æ ¼å¼ä¸æ­£ç¡®: {image_url[:100]}...")
                                        edited_image = pil_image
                                        response_text = f"OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                                else:
                                    print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾åƒæ•°æ®")
                                    edited_image = pil_image
                                    response_text = f"OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†æœªæ‰¾åˆ°å›¾åƒæ•°æ®"
                            except Exception as e:
                                print(f"âš ï¸ OpenRouterå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                edited_image = pil_image
                                response_text = f"OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†è§£æå¤±è´¥: {e}"
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡
                        if not edited_image:
                            print("âš ï¸ OpenRouteræœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®ï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = pil_image
                            if not response_text:
                                response_text = "OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    elif is_t8_mirror:
                        # T8é•œåƒç«™Geminiæ ¼å¼å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†T8é•œåƒç«™Geminiæ ¼å¼å“åº”")

                        # æ£€æŸ¥æ˜¯å¦ä¸ºnano-bananaæ¨¡å‹ï¼ˆä½¿ç”¨OpenAIæ ¼å¼ï¼‰
                        if normalized_model in ["nano-banana", "nano-banana-hd"]:
                            # nano-bananaä½¿ç”¨OpenAIæ ¼å¼
                            if "choices" in result and result["choices"]:
                                choice = result["choices"][0]
                                if "message" in choice and "content" in choice["message"]:
                                    content = choice["message"]["content"]
                                    if isinstance(content, str):
                                        response_text = content
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«base64å›¾åƒæ•°æ®
                                        if "![image](data:image/" in content:
                                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°T8é•œåƒç«™è¿”å›çš„å›¾åƒæ•°æ®")
                                            try:
                                                # æå–base64å›¾åƒæ•°æ®
                                                import re, io
                                                image_match = re.search(r'!\[image\]\(data:image/\w+;base64,([^)]+)\)', content)
                                                if image_match:
                                                    image_data = image_match.group(1)
                                                    image_bytes = base64.b64decode(image_data)
                                                    edited_image = Image.open(io.BytesIO(image_bytes))
                                                    print("âœ… æˆåŠŸæå–T8é•œåƒç«™ç¼–è¾‘åçš„å›¾åƒ")
                                                    # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                                    response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç¼–è¾‘]', content)
                                            except Exception as e:
                                                print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                        else:
                            # å…¶ä»–æ¨¡å‹ä½¿ç”¨Geminiæ ¼å¼
                            if "candidates" in result and result["candidates"]:
                                candidate = result["candidates"][0]
                                if "content" in candidate and "parts" in candidate["content"]:
                                    for part in candidate["content"]["parts"]:
                                        # æå–æ–‡æœ¬
                                        if "text" in part:
                                            response_text += part["text"]

                                        # æå–ç¼–è¾‘åçš„å›¾ç‰‡ï¼ˆæ”¯æŒinline_dataå’ŒinlineDataä¸¤ç§æ ¼å¼ï¼‰
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                import io
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print(f"âœ… æˆåŠŸæå–T8é•œåƒç«™ç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")
                                            except Exception as e:
                                                print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")

                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡
                        if not edited_image:
                            print("âš ï¸ T8é•œåƒç«™æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = pil_image
                            if not response_text:
                                response_text = "T8é•œåƒç«™å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
                        request_data = {
                            "model": _normalize_model_name(model),
                            "messages": [{
                                "role": "user",
                                "content": enhanced_prompt
                            }],
                            "temperature": temperature,
                            "max_tokens": max_output_tokens,
                            "stream": True
                        }
                        
                        # è®¾ç½®è¯·æ±‚å¤´
                        headers = {
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {api_key.strip()}"
                        }
                    else:
                        # æ ‡å‡†Gemini APIå“åº”å¤„ç†
                        try:
                            result = response.json()
                        except Exception as e:
                            print(f"âš ï¸ æ ‡å‡†Gemini JSONè§£æå¤±è´¥: {e}")
                            result = {}
                        if "candidates" in result and result["candidates"]:
                            candidate = result["candidates"][0]
                            if "content" in candidate and "parts" in candidate["content"]:
                                for part in candidate["content"]["parts"]:
                                    # æå–æ–‡æœ¬
                                    if "text" in part:
                                        response_text += part["text"]
                                    
                                    # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                    if "inline_data" in part or "inlineData" in part:
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                # è§£ç å›¾ç‰‡æ•°æ®
                                                import io
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                            except Exception as e:
                                                print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = pil_image
                        if not response_text:
                            response_text = "å›¾ç‰‡ç¼–è¾‘è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"

                    # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                    if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                        try:
                            # æå–æ”¾å¤§å€æ•°
                            scale = int(upscale_factor.replace("x", "").strip().split()[0])
                            if scale > 1:
                                print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                try:
                                    from .banana_upscale import smart_upscale
                                except ImportError:
                                    from banana_upscale import smart_upscale

                                # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                target_w = edited_image.width * scale
                                target_h = edited_image.height * scale

                                # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                upscaled_image = smart_upscale(
                                    edited_image,
                                    target_w,
                                    target_h,
                                    gigapixel_model
                                )

                                if upscaled_image:
                                    edited_image = upscaled_image
                                    print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                else:
                                    print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                        except Exception as e:
                            print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                    
                    # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
                    if not response_text:
                        response_text = "å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„ç¼–è¾‘æŒ‡ä»¤ä¿®æ”¹åçš„å›¾åƒã€‚"
                        print("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆ")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
                    # print(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                    print(f"ğŸ“ å“åº”æ–‡æœ¬ç±»å‹: {'åŒ…å«å›¾åƒæ•°æ®' if 'data:image/' in response_text else 'çº¯æ–‡æœ¬å†…å®¹'}")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id) # ä½¿ç”¨å¢å¼ºåçš„æç¤ºè¯
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢é”™è¯¯
                        if response.status_code == 429:
                            error_message = error_detail.get("error", {}).get("message", "")
                            if "quota" in error_message.lower():
                                print("âš ï¸ æ£€æµ‹åˆ°é…é¢é™åˆ¶é”™è¯¯ï¼Œå»ºè®®:")
                                print("   1. ç­‰å¾…æ›´é•¿æ—¶é—´å†è¯•")
                                print("   2. æ£€æŸ¥APIé…é¢è®¾ç½®")
                                print("   3. è€ƒè™‘å‡çº§APIè®¡åˆ’")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    # æ™ºèƒ½ç­‰å¾…
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å›¾ç‰‡ç¼–è¾‘å¤±è´¥: {error_msg}")



    def get_mirror_config(self):
        """è·å–å½“å‰é•œåƒç«™é…ç½®"""
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        mirror_sites = config.get('mirror_sites', {})
        
        # æŸ¥æ‰¾API4GPTé•œåƒç«™é…ç½®
        for site_name, site_config in mirror_sites.items():
            if "api4gpt.com" in site_config.get("url", ""):
                return site_config
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤é…ç½®
        return {
            "url": "https://www.api4gpt.com",
            "api_key": "",
            "api_format": "api4gpt"
        }


class KenChenLLMGeminiBananaMultiImageEditNode:
    """
    Gemini Banana å¤šå›¾åƒç¼–è¾‘èŠ‚ç‚¹

    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒå¤šå¼ è¾“å…¥å›¾åƒï¼ˆæœ€å¤š4å¼ ï¼‰
    - ä¸“ä¸šçš„å›¾åƒç¼–è¾‘æç¤ºè¯
    - æ”¯æŒå°ºå¯¸ã€è´¨é‡ã€é£æ ¼æ§åˆ¶
    - æ™ºèƒ½å›¾åƒç»„åˆç¼–è¾‘
    - æ”¯æŒå¤šä¸ªé•œåƒç«™
    """

    # è®¾ç½®èŠ‚ç‚¹é¢œè‰²ä¸ºæ©™è‰²/æ£•è‰²ç³»
    @classmethod
    def get_node_color(cls):
        return "#D2691E"  # å·§å…‹åŠ›æ©™è‰²

    @classmethod
    def get_node_bgcolor(cls):
        return "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯

    @classmethod
    def INPUT_TYPES(cls):
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        image_settings = config.get('image_settings', {})
        
        # è·å–é•œåƒç«™é…ç½®
        mirror_sites = config.get('mirror_sites', {})
        mirror_options = list(mirror_sites.keys())
        if not mirror_options:
            mirror_options = ["official", "comfly", "custom"]
        
        # è·å–é»˜è®¤é•œåƒç«™é…ç½®
        default_site = "comfly" if "comfly" in mirror_options else mirror_options[0] if mirror_options else "official"
        default_config = get_mirror_site_config(default_site)
        
        # ğŸš€ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶é¢„è®¾
        aspect_ratios = image_settings.get('aspect_ratios', [
            "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
        ])
        response_modalities = image_settings.get('response_modalities', [
            "TEXT_AND_IMAGE", "IMAGE_ONLY"
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„è¶…é«˜æ¸…é€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "mirror_site": (mirror_options, {"default": default_site}),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "é•œåƒç«™APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç•™ç©ºæ—¶è‡ªåŠ¨è·å–ï¼‰"
                }),
                "prompt": ("STRING", {"default": "è¯·æ ¹æ®è¿™äº›å›¾ç‰‡è¿›è¡Œä¸“ä¸šçš„å›¾åƒç¼–è¾‘", "multiline": True}),
                # æ”¯æŒå¤šç§AIæ¨¡å‹å’Œå¤šå›¾åƒç¼–è¾‘æœåŠ¡: nano-bananaæ”¯æŒComflyå’ŒT8é•œåƒç«™, [All]æ”¯æŒæ‰€æœ‰é•œåƒç«™, API4GPTæ¨¡å‹, OpenRouteræ¨¡å‹
                "model": (["nano-banana [Comfly-T8]", "nano-banana-hd [Comfly-T8]", "gemini-2.5-flash-image [All]", "gemini-2.5-flash-image-preview [All]", "gemini-2.0-flash", "gemini-2.5-flash-image-hd [API4GPT]", "gemini-2.5-flash-image-vip [API4GPT]", "google/gemini-2.5-flash-image [OpenRouter]", "google/gemini-2.5-flash-image-preview [OpenRouter]"], {"default": "nano-banana [Comfly-T8]"}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),

                # ğŸ“ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶å‚æ•°
                "aspect_ratio": (aspect_ratios, {
                    "default": image_settings.get('default_aspect_ratio', "1:1"),
                    "tooltip": "å›¾åƒå®½é«˜æ¯” (Geminiå®˜æ–¹APIæ”¯æŒ)"
                }),
                "response_modality": (response_modalities, {
                    "default": image_settings.get('default_response_modality', "TEXT_AND_IMAGE"),
                    "tooltip": "å“åº”æ¨¡å¼ï¼šTEXT_AND_IMAGE=æ–‡å­—+å›¾åƒï¼ŒIMAGE_ONLY=ä»…å›¾åƒ"
                }),

                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),
                
                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),

                # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 8192), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 999999}),
            },
            "optional": {
                "image1": ("IMAGE",),
                "image2": ("IMAGE",),
                "image3": ("IMAGE",),
                "image4": ("IMAGE",),

                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {"unique_id": "UNIQUE_ID"}
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_multiple_images"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    # è®¾ç½®èŠ‚ç‚¹é¢œè‰² - ä½¿ç”¨ComfyUIæ ‡å‡†å±æ€§
    color = "#D2691E"  # å·§å…‹åŠ›æ©™è‰²
    bgcolor = "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯
    groupcolor = "#CD853F"  # æ²™æ£•è‰²

    def __init__(self):
        # å¼ºåˆ¶è®¾ç½®é¢œè‰²å±æ€§
        self.color = "#D2691E"
        self.bgcolor = "#8B4513"
        self.groupcolor = "#CD853F"

    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            print(f"âš ï¸ æ— æ³•æ¨é€å¯¹è¯: PromptServer={PromptServer is not None}, unique_id={unique_id}")
            return
        try:
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": user_prompt,
                            "response": response_text,
                            "response_id": str(random.randint(100000, 999999)),
                            "timestamp": time.time(),
                        }
                    ])
                },
            }
            print(f"ğŸ’¬ æ¨é€å¯¹è¯æ°”æ³¡åˆ°èŠ‚ç‚¹ {unique_id}")
            PromptServer.instance.send_sync("display_component", render_spec)
            print(f"âœ… å¯¹è¯æ°”æ³¡æ¨é€æˆåŠŸ")
        except Exception as e:
            print(f"âŒ [LLM Agent Assistant] Chat push failed: {e}")
            import traceback
            traceback.print_exc()
            pass

    def edit_multiple_images(self, mirror_site: str, api_key: str, prompt: str, model: str,
                           proxy: str, aspect_ratio: str, response_modality: str, quality: str, style: str,
                           detail_level: str, camera_control: str, lighting_control: str, template_selection: str,
                           upscale_factor: str, gigapixel_model: str, temperature: float, top_p: float, top_k: int, max_output_tokens: int, seed: int,
                           image1=None, image2=None, image3=None, image4=None, custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨é•œåƒç«™APIè¿›è¡Œå¤šå›¾åƒç¼–è¾‘"""

        # ğŸ”§ ç¡®ä¿requestsæ¨¡å—å¯ç”¨
        import requests
        
        # ğŸš€ ç«‹å³è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå»é™¤UIæ ‡è¯†
        model = _normalize_model_name(model)
        
        # æ ¹æ®é•œåƒç«™ä»é…ç½®è·å–URLå’ŒAPI Key
        site_config = get_mirror_site_config(mirror_site) if mirror_site else {"url": "", "api_key": ""}
        api_url = site_config.get("url", "").strip()
        if site_config.get("api_key") and not api_key.strip():
            api_key = site_config["api_key"]
            print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™API Key: {api_key[:8]}...")
        if not api_url:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘è¯¥é•œåƒç«™çš„API URL")
        print(f"ğŸ”— è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™URL: {api_url}")
        
        if not validate_api_url(api_url):
            raise ValueError("API URLæ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†å›¾åƒæ§åˆ¶å‚æ•°
        try:
            from .gemini_banana import process_image_controls, enhance_prompt_with_controls
        except ImportError:
            from gemini_banana import process_image_controls, enhance_prompt_with_controls
        controls = process_image_controls(quality, style)

        # ğŸš€ è°ƒè¯•ï¼šæ˜¾ç¤ºå‚æ•°ä¼ é€’è¿‡ç¨‹
        print(f"ğŸ” å‚æ•°ä¼ é€’è°ƒè¯•:")
        print(f"  - èŠ‚ç‚¹aspect_ratioå‚æ•°: {aspect_ratio}")
        print(f"  - èŠ‚ç‚¹qualityå‚æ•°: {quality}")
        print(f"  - èŠ‚ç‚¹styleå‚æ•°: {style}")

        # ä½¿ç”¨enhance_prompt_with_controlså‡½æ•°è¿›è¡Œå®Œæ•´çš„æç¤ºè¯å¢å¼º
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(),
            controls,
            detail_level,
            camera_control,
            lighting_control,
            template_selection,
            quality_enhancement="Auto",  # é»˜è®¤å€¼
            enhance_quality=True,  # é»˜è®¤å€¼
            smart_resize=True,  # é»˜è®¤å€¼
            fill_color="white"  # é»˜è®¤å€¼
        )

        # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
        if custom_additions and custom_additions.strip():
            enhanced_prompt += f"\n\n{custom_additions.strip()}"
            print(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_additions[:100]}...")

        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: aspect_ratio={aspect_ratio}, quality={controls['quality']}, style={controls['style']}")
        
        # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„å›¾åƒ
        all_input_pils = []
        input_images = [image1, image2, image3, image4]
        
        for i, img_tensor in enumerate(input_images):
            if img_tensor is not None:
                try:
                    pil_image = tensor_to_pil(img_tensor)
                    if pil_image:
                        all_input_pils.append(pil_image)
                        print(f"ğŸ“¸ æ·»åŠ è¾“å…¥å›¾åƒ {i+1}: {pil_image.size}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒ {i+1} å¤„ç†å¤±è´¥: {e}")
        
        if not all_input_pils:
            raise ValueError("é”™è¯¯ï¼šè¯·è¾“å…¥è‡³å°‘ä¸€å¼ è¦ç¼–è¾‘çš„å›¾åƒ")
        
        print(f"ğŸ–¼ï¸ æ€»å…±æ”¶é›†åˆ° {len(all_input_pils)} å¼ è¾“å…¥å›¾åƒ")
        
        # æ™ºèƒ½ç”Ÿæˆå¤šå›¾ç¼–è¾‘æç¤ºè¯
        # é¦–å…ˆå¤„ç†å›¾ç‰‡å¼•ç”¨è½¬æ¢ï¼Œç¡®ä¿æ‰€æœ‰æƒ…å†µä¸‹éƒ½èƒ½ä½¿ç”¨
        user_intent = prompt.strip()
        converted_prompt = user_intent
        
        # è½¬æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨ - é€šç”¨åŒ–å¤„ç†
        if len(all_input_pils) >= 1:
                            converted_prompt = converted_prompt.replace("å›¾1", "å·¦è¾¹å›¾ç‰‡")
        if len(all_input_pils) >= 2:
                            converted_prompt = converted_prompt.replace("å›¾2", "å³è¾¹å›¾ç‰‡")
        if len(all_input_pils) >= 3:
            converted_prompt = converted_prompt.replace("å›¾3", "ç¬¬ä¸‰å¼ å›¾ç‰‡")
        if len(all_input_pils) >= 4:
            converted_prompt = converted_prompt.replace("å›¾4", "ç¬¬å››å¼ å›¾ç‰‡")
        
        # æ ¹æ®å›¾ç‰‡æ•°é‡ç”Ÿæˆä¸åŒçš„æç¤ºè¯ - å®Œå…¨é€šç”¨åŒ–
        if len(all_input_pils) == 2:
            # 2å¼ å›¾ç‰‡ï¼šé€šç”¨ç»„åˆç¼–è¾‘
            if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
                full_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

é‡è¦è¯´æ˜ï¼š
- è¯·ä»”ç»†åˆ†æä¸¤å¼ å›¾ç‰‡çš„å†…å®¹å’Œå…³ç³»
- æ ¹æ®ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤ï¼Œå°†ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ åº”ç”¨åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡ä¸­
- ä¿æŒç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ¸å¿ƒç‰¹å¾å’ŒèƒŒæ™¯ç¯å¢ƒ
- ç¡®ä¿ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ ä¸ç¬¬ä¸€å¼ å›¾ç‰‡å®Œç¾èåˆ
- ç¼–è¾‘ç»“æœåº”è¯¥çœ‹èµ·æ¥è‡ªç„¶çœŸå®ï¼Œç¬¦åˆç”¨æˆ·æ„å›¾

{enhanced_prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚æ‰§è¡Œï¼Œç¡®ä¿ç¼–è¾‘ç»“æœå®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚"""
            else:
                # æ ‡å‡†æç¤ºè¯
                full_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

é‡è¦è¯´æ˜ï¼š
- è¯·ä»”ç»†åˆ†æä¸¤å¼ å›¾ç‰‡çš„å†…å®¹å’Œå…³ç³»
- æ ¹æ®ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤ï¼Œå°†ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ åº”ç”¨åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡ä¸­
- ä¿æŒç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ¸å¿ƒç‰¹å¾å’ŒèƒŒæ™¯ç¯å¢ƒ
- ç¡®ä¿ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ ä¸ç¬¬ä¸€å¼ å›¾ç‰‡å®Œç¾èåˆ
- ç¼–è¾‘ç»“æœåº”è¯¥çœ‹èµ·æ¥è‡ªç„¶çœŸå®ï¼Œç¬¦åˆç”¨æˆ·æ„å›¾

{enhanced_prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚æ‰§è¡Œï¼Œç¡®ä¿ç¼–è¾‘ç»“æœå®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚"""
        elif len(all_input_pils) == 1:
            # 1å¼ å›¾ç‰‡ï¼šæ ‡å‡†å›¾åƒç¼–è¾‘
            full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ç¼–è¾‘è¿™å¼ å›¾ç‰‡ï¼š

{enhanced_prompt}

è¯·ä½¿ç”¨ä½ çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚"""
        else:
            # 3-4å¼ å›¾ç‰‡ï¼šå¤æ‚ç»„åˆç¼–è¾‘
            if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
                full_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œå¤šå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

é‡è¦è¯´æ˜ï¼š
- è¯·ä»”ç»†åˆ†ææ‰€æœ‰è¾“å…¥å›¾ç‰‡çš„å†…å®¹å’Œå…³ç³»
- æ ¹æ®ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤ï¼Œå°†ç›¸å…³å›¾ç‰‡ä¸­çš„å…ƒç´ è¿›è¡Œç²¾ç¡®ç»„åˆ
- ä¿æŒç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ¸å¿ƒç‰¹å¾å’ŒèƒŒæ™¯ç¯å¢ƒ
- ç¡®ä¿æ‰€æœ‰ç¼–è¾‘å…ƒç´ ä¸å‚è€ƒå›¾ç‰‡å®Œå…¨ä¸€è‡´
- ç¼–è¾‘ç»“æœåº”è¯¥çœ‹èµ·æ¥è‡ªç„¶çœŸå®ï¼Œç¬¦åˆç”¨æˆ·æ„å›¾

{enhanced_prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚æ‰§è¡Œï¼Œç¡®ä¿ç¼–è¾‘ç»“æœå®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚"""
            else:
                full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ã€‚è¯·æ ¹æ®è¿™äº›å›¾ç‰‡å’Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

{enhanced_prompt}

è¯·ä½¿ç”¨ä½ çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚ç¡®ä¿ç¼–è¾‘åçš„å›¾åƒç¬¦åˆæ‰€æœ‰è¦æ±‚ã€‚"""
        
        # è½¬æ¢æ‰€æœ‰è¾“å…¥å›¾ç‰‡ - ä¿®å¤å…³é”®é—®é¢˜
        # ç¡®ä¿æ‰€æœ‰å›¾ç‰‡éƒ½è¢«ä¼ é€’ç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´ä¿¡æ¯
        all_image_parts = []
        
        for i, pil_image in enumerate(all_input_pils):
            # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥ç¬¦åˆAPIè¦æ±‚
            pil_image = resize_image_for_api(pil_image)
            # è½¬æ¢ä¸ºbase64
            image_base64 = image_to_base64(pil_image, format='JPEG')
            all_image_parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            })
            print(f"ğŸ“¸ å‡†å¤‡ä¼ é€’å›¾åƒ {i+1}: {pil_image.size}")
        
        # è®¾ç½®ä»£ç†ï¼ˆç”¨äº requests åº“ï¼‰
        proxies = None
        if proxy and proxy.strip() and "None" not in proxy:
            proxies = {
                'http': proxy.strip(),
                'https': proxy.strip()
            }
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
            print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
        else:
            existing = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
            if existing:
                print(f"ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼Œæ²¿ç”¨ç³»ç»Ÿä»£ç†: {existing}")
            else:
                print("ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼ˆç³»ç»Ÿæ— ä»£ç†ï¼‰")
        
        # æ£€æŸ¥é•œåƒç«™ç±»å‹ - æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        is_nano_banana_official = mirror_site == "nano-bananaå®˜æ–¹"
        is_t8_mirror = "t8star.cn" in api_url or "ai.t8star.cn" in api_url
        is_api4gpt_mirror = "api4gpt.com" in api_url or "[API4GPT]" in model
        is_comfly_mirror = _is_comfly_base(api_url)
        is_openrouter_mirror = "openrouter.ai" in api_url or "[OpenRouter]" in model
        is_openai_mirror = "api.openai.com" in api_url or site_config.get("api_format") == "openai"

        # å¦‚æœæ£€æµ‹åˆ° OpenRouter æ¨¡å‹ä½† URL ä¸æ˜¯ OpenRouterï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter
        if "[OpenRouter]" in model and "openrouter.ai" not in api_url:
            api_url = "https://openrouter.ai/api/v1"
            is_openrouter_mirror = True
            print(f"ğŸ”„ æ£€æµ‹åˆ° OpenRouter æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° OpenRouter API: {api_url}")

        # å¦‚æœæ£€æµ‹åˆ° API4GPT æ¨¡å‹ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„ URL
        if "[API4GPT]" in model:
            if "one.api4gpt.com" not in api_url:
                api_url = "https://one.api4gpt.com"
                is_api4gpt_mirror = True
                print(f"ğŸ”„ æ£€æµ‹åˆ° API4GPT æ¨¡å‹ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ° API4GPT API: {api_url}")
        # å¦‚æœ URL åŒ…å« api4gpt.com ä½†ä¸æ˜¯ one.api4gpt.comï¼Œä¹Ÿè¦åˆ‡æ¢
        elif "api4gpt.com" in api_url and "one.api4gpt.com" not in api_url:
            api_url = "https://one.api4gpt.com"
            is_api4gpt_mirror = True
            print(f"ğŸ”„ æ£€æµ‹åˆ°é”™è¯¯çš„ API4GPT URLï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°æ­£ç¡®çš„ URL: {api_url}")

        # æ„å»ºå®Œæ•´çš„API URLï¼ˆOpenRouteré™¤å¤–ï¼Œå› ä¸ºå®ƒåœ¨å„è‡ªçš„å¤„ç†é€»è¾‘ä¸­æ„å»ºï¼‰
        if not is_openrouter_mirror:
            full_url = build_api_url(api_url, model)
            print(f"ğŸŒ ä½¿ç”¨APIåœ°å€: {full_url}")
        else:
            print(f"ğŸŒ OpenRouteré•œåƒç«™ï¼ŒURLå°†åœ¨OpenRouterå¤„ç†é€»è¾‘ä¸­æ„å»º")
        
        # æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºå¤„ç†é•œåƒç«™ï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        
        # 1. nano-bananaå®˜æ–¹é•œåƒç«™å¤„ç†
        if is_nano_banana_official:
            print("ğŸ”— æ£€æµ‹åˆ°nano-bananaå®˜æ–¹é•œåƒç«™ï¼Œä½¿ç”¨Googleå®˜æ–¹API")

            # æ„å»ºå†…å®¹éƒ¨åˆ†ï¼ˆæ–‡æœ¬ + å¤šå›¾åƒï¼‰
            content_parts = [{"text": full_prompt}] + all_image_parts

            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
            }

            # ğŸ¯ Geminiå®˜æ–¹APIï¼šResponse Modalitiesæ§åˆ¶
            if response_modality == "IMAGE_ONLY":
                generation_config["responseModalities"] = ["Image"]
                print("ğŸ“Š å“åº”æ¨¡å¼ï¼šä»…å›¾åƒï¼ˆIMAGE_ONLYï¼‰")
            else:
                generation_config["responseModalities"] = ["Text", "Image"]
                print("ğŸ“Š å“åº”æ¨¡å¼ï¼šæ–‡å­—+å›¾åƒï¼ˆTEXT_AND_IMAGEï¼‰")

            # ğŸ“ Geminiå®˜æ–¹APIï¼šAspect Ratioæ§åˆ¶
            if aspect_ratio and aspect_ratio != "1:1":
                generation_config["imageConfig"] = {
                    "aspectRatio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            try:
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨ï¼ˆå®˜æ–¹APIä¼˜å…ˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST APIï¼‰
                response_json = generate_with_priority_api(
                    api_key=api_key,
                    model=_normalize_model_name(model),
                    content_parts=content_parts,
                    generation_config=generation_config,
                    max_retries=5,
                    proxy=proxy
                )

                if response_json:
                    # æå–ç¼–è¾‘åçš„å›¾åƒ
                    edited_image = process_generated_image_from_response(response_json)

                    # æå–å“åº”æ–‡æœ¬
                    response_text = extract_text_from_response(response_json)

                    if edited_image:
                        # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                            try:
                                # æå–æ”¾å¤§å€æ•°
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                    # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale

                                    # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                    target_w = edited_image.width * scale
                                    target_h = edited_image.height * scale

                                    # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                    upscaled_image = smart_upscale(
                                        edited_image,
                                        target_w,
                                        target_h,
                                        gigapixel_model
                                    )

                                    if upscaled_image:
                                        edited_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                    else:
                                        print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆnano-bananaå®˜æ–¹ï¼‰")
                        self._push_chat(enhanced_prompt, response_text or "", unique_id)
                        return (image_tensor, response_text)
                    else:
                        print("âš ï¸ nano-bananaå®˜æ–¹APIå“åº”ä¸­æœªæ‰¾åˆ°ç¼–è¾‘åçš„å›¾åƒæ•°æ®")
                        # è¿”å›ç¬¬ä¸€å¼ è¾“å…¥å›¾åƒ
                        if all_input_pils:
                            return (pil_to_tensor(all_input_pils[0]), response_text)
                        else:
                            # åˆ›å»ºé»˜è®¤å›¾åƒ
                            default_image = Image.new('RGB', (1024, 1024), color='black')
                            return (pil_to_tensor(default_image), response_text)
                else:
                    raise Exception("nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥")

            except Exception as e:
                print(f"âŒ nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
                raise e
            
        # 2. Comflyé•œåƒç«™å¤„ç†
        elif is_comfly_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°Comflyé•œåƒç«™ï¼Œä½¿ç”¨Comfly APIæ ¼å¼")

            # è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼ˆå»é™¤æ ‡è®°ï¼‰
            normalized_model = _normalize_model_name(model)

            if normalized_model in ["nano-banana", "nano-banana-hd", "fal-ai/nano-banana", "nano-banana/edit", "fal-ai/nano-banana/edit"] and all_input_pils:
                # æ£€æŸ¥æ˜¯å¦æ˜¯fal-aiæ¨¡å‹
                if normalized_model.startswith("fal-ai/") or normalized_model.endswith("/edit"):
                    # ä½¿ç”¨fal-aiç«¯ç‚¹è¿›è¡Œå¤šå›¾ç¼–è¾‘
                    try:
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, normalized_model, enhanced_prompt, all_input_pils, 1, seed, "image_url")
                        edited_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ Comfly fal-ai/nano-banana å¤šå›¾ç¼–è¾‘æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"Comfly fal-ai/nano-banana å¤šå›¾ç¼–è¾‘æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            if result['data']:
                                b64 = result['data'][0].get('b64_json')
                                image_url = result['data'][0].get('url', '')
                                response_text = result.get('response_text', "")

                                if image_url and image_url not in response_text:
                                    response_text += f"\nå›¾åƒURL: {image_url}"

                                if b64:
                                    from base64 import b64decode
                                    import io
                                    try:
                                        b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                        edited_image = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                    except Exception as decode_error:
                                        print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                        try:
                                            edited_image = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                        except Exception as e2:
                                            print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                            edited_image = None
                                else:
                                    import re
                                    base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                    matches = re.findall(base64_pattern, response_text)
                                    if matches:
                                        from base64 import b64decode
                                        import io
                                        edited_image = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                        else:
                            print(f"âš ï¸ APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")
                            raise Exception(f"APIå“åº”æ ¼å¼å¼‚å¸¸: {result}")

                        # å¦‚æœæˆåŠŸç¼–è¾‘ï¼Œåº”ç”¨å°ºå¯¸ä¸è´¨é‡å¢å¼ºåå†è¿”å›
                        if edited_image:
                            try:
                                edited_image = _apply_full_enhancements(
                                    edited_image,
                                    controls['size'],
                                    quality,
                                    enhance_quality,
                                    smart_resize
                                )
                                image_tensor = pil_to_tensor(edited_image)
                                print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆComfly fal-ai/nano-bananaï¼‰")
                                self._push_chat(enhanced_prompt, response_text or "", unique_id)
                                return (image_tensor, response_text)
                            except Exception as enhance_error:
                                print(f"âš ï¸ å›¾åƒå¢å¼ºå¤±è´¥: {enhance_error}")
                                image_tensor = pil_to_tensor(edited_image)
                                return (image_tensor, response_text)
                        else:
                            print("âš ï¸ fal-ai/nano-bananaå¤šå›¾ç¼–è¾‘åæœªè·å¾—æœ‰æ•ˆå›¾åƒ")
                            if all_input_pils:
                                return (pil_to_tensor(all_input_pils[0]), response_text)
                            else:
                                dummy_image = Image.new('RGB', (512, 512), color='white')
                                return (pil_to_tensor(dummy_image), response_text)

                    except Exception as e:
                        print(f"âŒ Comfly fal-ai/nano-banana å¤šå›¾ç¼–è¾‘å¤±è´¥: {e}")
                        raise e
                else:
                    # ä½¿ç”¨åŸæœ‰çš„nano-bananaç«¯ç‚¹è¿›è¡Œå¤šå›¾ç¼–è¾‘
                    try:
                        # ä½¿ç”¨æ‰€æœ‰è¾“å…¥å›¾åƒ
                        result = _comfly_nano_banana_edit(api_url, api_key, normalized_model, enhanced_prompt, all_input_pils, controls['size'], temperature, top_p, max_output_tokens, seed)
                        edited_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result and result['data']:
                            b64 = result['data'][0].get('b64_json')
                            response_text = result.get('response_text', "")

                            if b64:
                                from base64 import b64decode
                                import io
                                try:
                                    # ä¿®å¤base64å¡«å……é—®é¢˜
                                    b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                    img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                except Exception as decode_error:
                                    print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                    # å°è¯•ç›´æ¥è§£ç 
                                    try:
                                        img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                    except Exception as e2:
                                        print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                        img = None
                                edited_image = img
                            else:
                                # å¦‚æœæ²¡æœ‰base64æ•°æ®ï¼Œå°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å›¾åƒ
                                import re
                                base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                matches = re.findall(base64_pattern, response_text)
                                if matches:
                                    from base64 import b64decode
                                    import io
                                    img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                    edited_image = img

                        # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨æ™ºèƒ½æ”¾å¤§
                        if edited_image:
                            # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                            if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                                try:
                                    # æå–æ”¾å¤§å€æ•°
                                    scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                    if scale > 1:
                                        print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                        # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                        try:
                                            from .banana_upscale import smart_upscale
                                        except ImportError:
                                            from banana_upscale import smart_upscale

                                        # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                        target_w = edited_image.width * scale
                                        target_h = edited_image.height * scale

                                        # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                        upscaled_image = smart_upscale(
                                            edited_image,
                                            target_w,
                                            target_h,
                                            gigapixel_model
                                        )

                                        if upscaled_image:
                                            edited_image = upscaled_image
                                            print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                        else:
                                            print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                                except Exception as e:
                                    print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                            image_tensor = pil_to_tensor(edited_image)
                            print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆComfly nano-bananaï¼‰")
                            self._push_chat(enhanced_prompt, response_text or "", unique_id)
                            return (image_tensor, response_text)

                    except Exception as e:
                        print(f"âŒ Comfly(nano-banana) å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {e}")
                        raise e
            else:
                # énano-bananaæ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼
                generation_config = {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"]
                }

                # ğŸ“ Aspect Ratioæ§åˆ¶
                if aspect_ratio and aspect_ratio != "1:1":
                    generation_config["imageConfig"] = {
                        "aspectRatio": aspect_ratio
                    }
                    print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

                # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
                if seed and seed > 0:
                    generation_config["seed"] = seed

                request_data = {
                    "model": normalized_model,
                    "contents": [{
                        "parts": [{"text": full_prompt}] + all_image_parts
                    }],
                    "generationConfig": generation_config
                }

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key.strip()}"
                }
                
        # 3. T8é•œåƒç«™å¤„ç†
        elif is_t8_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨T8 APIæ ¼å¼")

            # è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼ˆå»é™¤æ ‡è®°ï¼‰
            normalized_model = _normalize_model_name(model)

            if normalized_model in ["nano-banana", "nano-banana-hd", "fal-ai/nano-banana", "nano-banana/edit", "fal-ai/nano-banana/edit"] and all_input_pils:
                # æ£€æŸ¥æ˜¯å¦æ˜¯fal-aiæ¨¡å‹
                if normalized_model.startswith("fal-ai/") or normalized_model.endswith("/edit"):
                    # T8é•œåƒç«™ä½¿ç”¨ä¸Comflyç›¸åŒçš„fal-aiç«¯ç‚¹è°ƒç”¨æ–¹å¼
                    try:
                        result = _comfly_fal_ai_nano_banana(api_url, api_key, normalized_model, enhanced_prompt, all_input_pils, 1, seed, "image_url")
                        edited_image = None
                        response_text = ""

                        if isinstance(result, dict) and 'data' in result:
                            if not result['data']:
                                response_text = result.get('response_text', '')
                                print(f"âš ï¸ T8 fal-ai/nano-banana å¤šå›¾ç¼–è¾‘æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                                # print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                                print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)} å­—ç¬¦")
                                raise Exception(f"T8 fal-ai/nano-banana å¤šå›¾ç¼–è¾‘æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                            # å¤„ç†è¿”å›çš„å›¾åƒæ•°æ®
                            b64 = result['data'][0].get('b64_json')
                            image_url = result['data'][0].get('url', '')
                            response_text = result.get('response_text', "")

                            # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                            if image_url and image_url not in response_text:
                                response_text += f"\nğŸ”— å›¾åƒURL: {image_url}"

                            if b64:
                                from base64 import b64decode
                                import io
                                try:
                                    b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                    edited_image = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                except Exception as e:
                                    print(f"âš ï¸ base64è§£ç å¤±è´¥: {e}")
                                    # å°è¯•ä»URLä¸‹è½½
                                    if image_url:
                                        try:
                                            import requests
                                            response = requests.get(image_url, timeout=30)
                                            if response.status_code == 200:
                                                edited_image = Image.open(io.BytesIO(response.content)).convert('RGB')
                                                print("âœ… æˆåŠŸä»URLä¸‹è½½å¤šå›¾ç¼–è¾‘åå›¾åƒ")
                                            else:
                                                print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                        except Exception as e:
                                            print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")

                        # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨å…¨é‡å¢å¼ºåå†è¿”å›
                        if edited_image:
                            # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                            try:
                                edited_image = _apply_full_enhancements(
                                    edited_image,
                                    controls['size'],
                                    quality,
                                    enhance_quality,
                                    smart_resize
                                )
                                try:
                                    print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                                except Exception:
                                    pass
                            except Exception:
                                pass

                            image_tensor = pil_to_tensor(edited_image)
                            print("âœ… T8 fal-aiå¤šå›¾ç¼–è¾‘å®Œæˆ")
                            self._push_chat(enhanced_prompt, response_text or "", unique_id)
                            return (image_tensor, response_text)
                        else:
                            raise Exception("T8 fal-ai/nano-banana æœªèƒ½ç¼–è¾‘å¤šå›¾")

                    except Exception as e:
                        print(f"âŒ T8 fal-ai/nano-banana å¤šå›¾ç¼–è¾‘å¤±è´¥: {e}")
                        raise e

                # åŸç”Ÿnano-bananaæ¨¡å‹å¤„ç†
                elif _normalize_model_name(model) in ["nano-banana", "nano-banana-hd"]:
                    print("ğŸ”— T8é•œåƒç«™ä½¿ç”¨chat/completionsç«¯ç‚¹ (nano-banana ç›´è¿)")
                try:
                    # ä½¿ç”¨æ‰€æœ‰è¾“å…¥å›¾åƒ
                    result = _comfly_nano_banana_edit(full_url, api_key, _normalize_model_name(model), enhanced_prompt, all_input_pils, controls['size'], temperature, top_p, max_output_tokens, seed)
                    edited_image = None
                    response_text = ""

                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        response_text = result.get('response_text', "")

                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception:
                                img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                            edited_image = img

                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨æ™ºèƒ½æ”¾å¤§
                    if edited_image:
                        # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                        if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                            try:
                                # æå–æ”¾å¤§å€æ•°
                                scale = int(upscale_factor.replace("x", "").strip().split()[0])
                                if scale > 1:
                                    print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                    # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                    try:
                                        from .banana_upscale import smart_upscale
                                    except ImportError:
                                        from banana_upscale import smart_upscale

                                    # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                    target_w = edited_image.width * scale
                                    target_h = edited_image.height * scale

                                    # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                    upscaled_image = smart_upscale(
                                        edited_image,
                                        target_w,
                                        target_h,
                                        gigapixel_model
                                    )

                                    if upscaled_image:
                                        edited_image = upscaled_image
                                        print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                    else:
                                        print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                            except Exception as e:
                                print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆT8 nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, response_text or "", unique_id)
                        return (image_tensor, response_text)

                except Exception as e:
                    print(f"âŒ T8(nano-banana) å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {e}")
                    raise e
            else:
                # å…¶ä»–æ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼ï¼ˆä¸Comflyå¯¹é½ï¼‰
                print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨Gemini APIæ ¼å¼")

                generation_config = {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"]
                }

                # ğŸ“ Aspect Ratioæ§åˆ¶
                if aspect_ratio and aspect_ratio != "1:1":
                    generation_config["imageConfig"] = {
                        "aspectRatio": aspect_ratio
                    }
                    print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

                # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
                if seed and seed > 0:
                    generation_config["seed"] = seed

                # æ„å»ºpartsï¼šæ–‡æœ¬ + æ‰€æœ‰å›¾åƒ
                parts = [{"text": enhanced_prompt}]
                for image_part in all_image_parts:
                    parts.append(image_part)

                request_data = {
                    "model": normalized_model,
                    "contents": [{
                        "parts": parts
                    }],
                    "generationConfig": generation_config
                }

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key.strip()}"
                }
            
        # 4. API4GPTé•œåƒç«™å¤„ç†ï¼ˆä½¿ç”¨æ ‡å‡† OpenAI å…¼å®¹æ ¼å¼ï¼‰
        elif is_api4gpt_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°API4GPTé•œåƒç«™ï¼Œä½¿ç”¨ OpenAI å…¼å®¹æ ¼å¼")

            # API4GPT çš„ nano-banana æœåŠ¡ä½¿ç”¨æ ‡å‡† OpenAI /v1/images/edits ç«¯ç‚¹
            # å‚è€ƒæ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609442
            # API4GPT æ”¯æŒå¤šå¼ å›¾ç‰‡ä¸Šä¼ 
            print(f"ğŸ“¤ API4GPT å‡†å¤‡ä¸Šä¼  {len(all_input_pils)} å¼ å›¾ç‰‡è¿›è¡Œç¼–è¾‘")

            if all_input_pils:
                try:
                    # å°†æ‰€æœ‰å›¾åƒè½¬æ¢ä¸ºå­—èŠ‚æµ
                    import io

                    # æ„å»ºå®Œæ•´çš„ API URL
                    full_url = f"{api_url}/v1/images/edits"
                    print(f"ğŸ”— ä½¿ç”¨ API4GPT ç«¯ç‚¹: {full_url}")

                    # å‡†å¤‡å›¾ç‰‡æ•°æ®ï¼ˆä¿å­˜ä¸ºbytesï¼Œæ–¹ä¾¿é‡è¯•æ—¶é‡ç”¨ï¼‰
                    image_data_list = []
                    for idx, pil_img in enumerate(all_input_pils):
                        img_bytes = io.BytesIO()
                        pil_img.save(img_bytes, format='PNG')
                        image_data_list.append({
                            'name': f'image_{idx}.png',
                            'data': img_bytes.getvalue(),  # è·å–bytesæ•°æ®
                            'size': pil_img.size
                        })
                        print(f"  ğŸ“ å‡†å¤‡å›¾ç‰‡ {idx+1}: {pil_img.size}")

                    # æ„å»ºè¯·æ±‚æ•°æ®
                    data = {
                        'prompt': full_prompt,
                        'model': _normalize_model_name(model),
                        'n': 1
                    }

                    # å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥åˆ›å»ºfilesåˆ—è¡¨ï¼ˆæ¯æ¬¡è¯·æ±‚éƒ½éœ€è¦æ–°çš„BytesIOå¯¹è±¡ï¼‰
                    def create_files_list():
                        files = []
                        for img_data in image_data_list:
                            img_bytes = io.BytesIO(img_data['data'])
                            files.append(('image', (img_data['name'], img_bytes, 'image/png')))
                        return files

                    headers = {
                        "Authorization": f"Bearer {api_key.strip()}"
                    }

                    # è°ƒç”¨ API4GPT APIï¼ˆå¤šå›¾ç¼–è¾‘éœ€è¦æ›´é•¿æ—¶é—´ï¼‰
                    print(f"â±ï¸ å¼€å§‹è°ƒç”¨ API4GPT å¤šå›¾ç¼–è¾‘ APIï¼ˆä¸Šä¼  {len(image_data_list)} å¼ å›¾ç‰‡ï¼Œå¯èƒ½éœ€è¦3-5åˆ†é’Ÿï¼‰...")
                    print(f"ğŸ“Š è¯·æ±‚å‚æ•°: prompté•¿åº¦={len(full_prompt)}, model={_normalize_model_name(model)}, n=1")

                    # å¤šå›¾ä¸Šä¼ æ—¶ï¼Œå¦‚æœä½¿ç”¨ä»£ç†å¯èƒ½å¯¼è‡´è¶…æ—¶ï¼Œå°è¯•å¤šç§ç­–ç•¥
                    response = None
                    last_error = None

                    # ç­–ç•¥1ï¼šä½¿ç”¨ä»£ç†å°è¯•ï¼ˆå¦‚æœé…ç½®äº†ä»£ç†ï¼‰
                    if proxies:
                        try:
                            print(f"ğŸ“¡ ç­–ç•¥1: ä½¿ç”¨ä»£ç†ä¸Šä¼ ï¼ˆ{proxies.get('http', 'N/A')}ï¼‰")
                            files = create_files_list()  # åˆ›å»ºæ–°çš„filesåˆ—è¡¨
                            response = requests.post(
                                full_url,
                                headers=headers,
                                files=files,
                                data=data,
                                proxies=proxies,
                                timeout=300
                            )
                            response.raise_for_status()
                            print(f"âœ… ä½¿ç”¨ä»£ç†ä¸Šä¼ æˆåŠŸ")
                        except (requests.exceptions.ProxyError, requests.exceptions.ConnectionError) as e:
                            print(f"âš ï¸ ä»£ç†ä¸Šä¼ å¤±è´¥: {e}")
                            last_error = e
                            response = None

                    # ç­–ç•¥2ï¼šå¦‚æœä»£ç†å¤±è´¥ï¼Œå°è¯•ç›´è¿
                    if response is None:
                        try:
                            print(f"ğŸ“¡ ç­–ç•¥2: å°è¯•ç›´è¿ä¸Šä¼ ï¼ˆä¸ä½¿ç”¨ä»£ç†ï¼‰")
                            files = create_files_list()  # åˆ›å»ºæ–°çš„filesåˆ—è¡¨
                            response = requests.post(
                                full_url,
                                headers=headers,
                                files=files,
                                data=data,
                                proxies=None,  # ä¸ä½¿ç”¨ä»£ç†
                                timeout=300
                            )
                            response.raise_for_status()
                            print(f"âœ… ç›´è¿ä¸Šä¼ æˆåŠŸ")
                        except Exception as e:
                            print(f"âŒ ç›´è¿ä¸Šä¼ ä¹Ÿå¤±è´¥: {e}")
                            last_error = e
                            response = None

                    # å¦‚æœä¸¤ç§ç­–ç•¥éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
                    if response is None:
                        raise last_error

                    print(f"ğŸ“¡ APIå“åº”çŠ¶æ€ç : {response.status_code}")

                    # è§£æå“åº”ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
                    result = response.json()
                    print(f"âœ… API4GPT å¤šå›¾ç‰‡ç¼–è¾‘ APIè°ƒç”¨æˆåŠŸ")
                    print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")

                    # ä» OpenAI å…¼å®¹æ ¼å¼ä¸­æå–å›¾åƒ URL
                    if "data" in result and len(result["data"]) > 0:
                        image_url = result["data"][0].get("url")
                        if image_url:
                            print(f"ğŸ”— ä¸‹è½½ç¼–è¾‘åçš„å›¾åƒ: {image_url}")
                            image_response = requests.get(image_url, proxies=proxies, timeout=60)
                            image_response.raise_for_status()
                            import io
                            edited_image = Image.open(io.BytesIO(image_response.content))
                            print(f"âœ… æˆåŠŸä¸‹è½½API4GPTç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")

                            # æå–å“åº”æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
                            response_text = result["data"][0].get("revised_prompt", full_prompt)
                        else:
                            raise ValueError("API4GPTå“åº”ä¸­æ²¡æœ‰å›¾åƒURL")
                    else:
                        raise ValueError("API4GPTå“åº”æ ¼å¼é”™è¯¯")

                    # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                    if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                        try:
                            scale = int(upscale_factor.replace("x", "").strip().split()[0])
                            if scale > 1:
                                print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")
                                try:
                                    from .banana_upscale import smart_upscale
                                except ImportError:
                                    from banana_upscale import smart_upscale
                                target_w = edited_image.width * scale
                                target_h = edited_image.height * scale
                                upscaled_image = smart_upscale(edited_image, target_w, target_h, gigapixel_model)
                                if upscaled_image:
                                    edited_image = upscaled_image
                                    print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                        except Exception as e:
                            print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}")

                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆAPI4GPTï¼‰")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)

                except requests.exceptions.Timeout as e:
                    print(f"âŒ API4GPT APIè°ƒç”¨è¶…æ—¶: {e}")
                    print(f"ğŸ’¡ å»ºè®®ï¼šå¤šå›¾ç¼–è¾‘éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘å›¾ç‰‡æ•°é‡")
                    raise ValueError(f"API4GPT APIè°ƒç”¨è¶…æ—¶ï¼ˆ{len(all_input_pils)}å¼ å›¾ç‰‡ï¼‰: {e}")
                except requests.exceptions.RequestException as e:
                    print(f"âŒ API4GPT APIç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
                    if hasattr(e, 'response') and e.response is not None:
                        print(f"ğŸ“‹ å“åº”çŠ¶æ€ç : {e.response.status_code}")
                        try:
                            error_detail = e.response.json()
                            print(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {error_detail}")
                        except:
                            print(f"ğŸ“‹ å“åº”å†…å®¹: {e.response.text[:500]}")
                    raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
                except Exception as e:
                    print(f"âŒ API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
            else:
                raise ValueError("æ²¡æœ‰å¯ç”¨çš„è¾“å…¥å›¾åƒè¿›è¡Œç¼–è¾‘")
        elif is_openrouter_mirror:
            # OpenRouteré•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenRouteré•œåƒç«™ï¼Œä½¿ç”¨OpenRouter APIæ ¼å¼")
            
            # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹è¿›è¡Œå¤šå›¾åƒç¼–è¾‘
            # æ„å»ºOpenAIå…¼å®¹çš„è¯·æ±‚æ ¼å¼
            content = []
            
            # æ·»åŠ æ‰€æœ‰è¾“å…¥å›¾åƒ
            for i, pil_image in enumerate(all_input_pils):
                # è½¬æ¢ä¸ºbase64
                image_base64 = image_to_base64(pil_image, format='JPEG')
                image_url = f"data:image/jpeg;base64,{image_base64}"
                
                # æ·»åŠ å›¾ç‰‡æ ‡è¯†
                content.append({
                    "type": "text",
                    "text": f"[è¿™æ˜¯ç¬¬{i+1}å¼ å›¾ç‰‡]"
                })
                
                # æ·»åŠ å›¾ç‰‡
                content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            
            # æ·»åŠ æ–‡æœ¬æŒ‡ä»¤
            enhanced_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual edited image, not just text description.

Task: {full_prompt}

Image References:
- When I mention "ç¬¬1å¼ å›¾ç‰‡", I mean the first image provided above
- When I mention "ç¬¬2å¼ å›¾ç‰‡", I mean the second image provided above
- When I mention "ç¬¬3å¼ å›¾ç‰‡", I mean the third image provided above
- When I mention "ç¬¬4å¼ å›¾ç‰‡", I mean the fourth image provided above

REQUIREMENTS:
1. EDIT the provided images based on my request
2. DO NOT just describe what the edited image should look like
3. RETURN the actual edited image file/data
4. The output MUST be a visual image, not text

Execute the multi-image editing task now and return the edited image."""
            
            content.append({
                "type": "text",
                "text": enhanced_instruction
            })
            
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": content
                }],
                "modalities": ["image", "text"],  # OpenRouter å›¾åƒç”Ÿæˆå¿…éœ€å‚æ•°
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_output_tokens,
                "stream": True  # Required for gemini-2.5-flash-image-preview
            }

            # æ·»åŠ  image_configï¼ˆåŒ…å« aspect_ratioï¼‰
            if aspect_ratio and aspect_ratio != "1:1":
                request_data["image_config"] = {
                    "aspect_ratio": aspect_ratio
                }
                print(f"ğŸ“ è®¾ç½® OpenRouter å¤šå›¾ç¼–è¾‘å®½é«˜æ¯”: {aspect_ratio}")

            # æ·»åŠ  seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                if "image_config" not in request_data:
                    request_data["image_config"] = {}
                request_data["image_config"]["seed"] = seed
                print(f"ğŸ² è®¾ç½® OpenRouter å¤šå›¾ç¼–è¾‘ seed: {seed}")

            # ä½¿ç”¨OpenRouterçš„chat/completionsç«¯ç‚¹
            # å¯¹äºOpenRouterï¼Œapi_urlå·²ç»åŒ…å«äº†/v1ï¼Œæ‰€ä»¥ç›´æ¥æ·»åŠ /chat/completions
            if api_url.endswith('/v1'):
                full_url = f"{api_url}/chat/completions"
            else:
                full_url = f"{api_url}/v1/chat/completions"
            print(f"ğŸ”— ä½¿ç”¨OpenRouter chat/completionsç«¯ç‚¹è¿›è¡Œå¤šå›¾åƒç¼–è¾‘: {full_url}")
            
            # è®¾ç½®OpenRouterè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}",
                "HTTP-Referer": "https://github.com/ComfyUI-LLM-Prompt",
                "X-Title": "ComfyUI LLM Prompt Plugin"
            }
        elif is_openai_mirror:
            # OpenAIé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": enhanced_prompt
                }],
                "temperature": temperature,
                "max_tokens": max_output_tokens,
                "stream": True
            }
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        else:
            # æ ‡å‡†Gemini APIæ ¼å¼
            request_data = {
                "contents": [{
                    "parts": [
                        {
                            "text": full_prompt
                        }
                    ] + all_image_parts  # æ·»åŠ æ‰€æœ‰å›¾ç‰‡
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        
        # æ™ºèƒ½é‡è¯•æœºåˆ¶
        max_retries = 5
        timeout = 120
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç¼–è¾‘å›¾ç‰‡... (å°è¯• {attempt + 1}/{max_retries})")
                print(f"ğŸ“ ç¼–è¾‘æŒ‡ä»¤: {enhanced_prompt[:100]}...")
                print(f"ğŸ”— é•œåƒç«™: {api_url}")
                
                # å‘é€è¯·æ±‚ - æ·»åŠ SSLé…ç½®ä»¥è§£å†³è¿æ¥é—®é¢˜
                response = requests.post(full_url, headers=headers, json=request_data, timeout=timeout, verify=False)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€å’Œå†…å®¹
                if response.status_code != 200:
                    print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
                    print(f"ğŸ“¡ å“åº”å¤´: {dict(response.headers)}")

                # æ£€æŸ¥å“åº”å†…å®¹
                if not response.text.strip():
                    print("âš ï¸ APIè¿”å›ç©ºå“åº”")
                    raise ValueError("APIè¿”å›ç©ºå“åº”")
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    # ä¸ºæ‰€æœ‰é•œåƒç«™å®šä¹‰ result å˜é‡
                    if not is_api4gpt_mirror and not is_openrouter_mirror and not is_t8_mirror and not is_openai_mirror:
                        # æ ‡å‡† Gemini API é•œåƒç«™ï¼ˆå¦‚ Comfy APIï¼‰
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£ææ ‡å‡† Gemini API å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ è§£æå“åº”å¤±è´¥: {e}")
                            result = {}
                    elif is_t8_mirror:
                        # T8 é•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ T8 é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ T8 é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ OpenAI é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ OpenAI é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    
                    if is_api4gpt_mirror:
                        # API4GPTé•œåƒç«™å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†API4GPTé•œåƒç«™å“åº”")
                        
                        try:
                            # å°è¯•è§£æJSONå“åº”
                            result = response.json()
                            print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")
                            
                            if api4gpt_service == "nano-banana":
                                # nano-bananaä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
                                response_text, edited_image = parse_openai_compatible_response(result)
                            else:
                                # å…¶ä»–æœåŠ¡ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
                                response_text, edited_image = parse_api4gpt_response(result, api4gpt_service)
                            
                            if edited_image:
                                print(f"âœ… æˆåŠŸæå–API4GPTç¼–è¾‘åçš„å›¾åƒ")
                            else:
                                print("âš ï¸ API4GPTæœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                                edited_image = all_input_pils[0]
                                if not response_text:
                                    response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                        except Exception as json_error:
                            print(f"âš ï¸ API4GPT JSONè§£æå¤±è´¥: {json_error}")
                            # print(f"ğŸ“‹ åŸå§‹å“åº”å†…å®¹: {response.text[:500]}...")  # æ³¨é‡Šæ‰å†—é•¿çš„è°ƒè¯•ä¿¡æ¯
                            raise ValueError(f"API4GPTå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {json_error}")
                            
                    elif is_openrouter_mirror:
                        # OpenRouteré•œåƒç«™å“åº”å¤„ç† - ä½¿ç”¨æµå¼å“åº”
                        print("ğŸ”— å¤„ç†OpenRouteré•œåƒç«™æµå¼å“åº”")
                        
                        # å¤„ç†æµå¼å“åº”
                        response_text = process_openrouter_stream(response)
                        
                        # æ£€æŸ¥å“åº”æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                        if "data:image/" in response_text:
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouterè¿”å›çš„å›¾åƒæ•°æ®")
                            try:
                                # ä½¿ç”¨æ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–base64å›¾åƒæ•°æ®
                                import re
                                base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                image_matches = re.findall(base64_pattern, response_text)
                                if image_matches:
                                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾åƒæ•°æ®
                                    image_url = image_matches[0]
                                    print(f"ğŸ¯ æˆåŠŸåŒ¹é…OpenRouterå›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_url)}å­—ç¬¦")
                                    
                                    # æå–base64éƒ¨åˆ†
                                    if ';base64,' in image_url:
                                        import io
                                        base64_data = image_url.split(';base64,', 1)[1]
                                        image_bytes = base64.b64decode(base64_data)
                                        edited_image = Image.open(io.BytesIO(image_bytes))
                                        print(f"âœ… æˆåŠŸæå–OpenRouterç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")
                                        
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(base64_pattern, '[å›¾åƒå·²ç¼–è¾‘]', response_text)
                                    else:
                                        print(f"âš ï¸ å›¾åƒæ•°æ®æ ¼å¼ä¸æ­£ç¡®: {image_url[:100]}...")
                                        edited_image = all_input_pils[0]
                                        response_text = f"OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                                else:
                                    print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾åƒæ•°æ®")
                                    edited_image = all_input_pils[0]
                                    response_text = f"OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†æœªæ‰¾åˆ°å›¾åƒæ•°æ®"
                            except Exception as e:
                                print(f"âš ï¸ OpenRouterå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                edited_image = all_input_pils[0]
                                response_text = f"OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†è§£æå¤±è´¥: {e}"
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡
                        if not edited_image:
                            print("âš ï¸ OpenRouteræœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®ï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = all_input_pils[0]
                            if not response_text:
                                response_text = "OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    elif is_t8_mirror:
                        # T8é•œåƒç«™Geminiæ ¼å¼å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†T8é•œåƒç«™Geminiæ ¼å¼å“åº”")

                        # æ£€æŸ¥æ˜¯å¦ä¸ºnano-bananaæ¨¡å‹ï¼ˆä½¿ç”¨OpenAIæ ¼å¼ï¼‰
                        if normalized_model in ["nano-banana", "nano-banana-hd"]:
                            # nano-bananaä½¿ç”¨OpenAIæ ¼å¼
                            if "choices" in result and result["choices"]:
                                choice = result["choices"][0]
                                if "message" in choice and "content" in choice["message"]:
                                    content = choice["message"]["content"]
                                    if isinstance(content, str):
                                        response_text = content
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«base64å›¾åƒæ•°æ®
                                        if "![image](data:image/" in content:
                                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°T8é•œåƒç«™è¿”å›çš„å›¾åƒæ•°æ®")
                                            try:
                                                # æå–base64å›¾åƒæ•°æ®
                                                import re, io
                                                image_match = re.search(r'!\[image\]\(data:image/\w+;base64,([^)]+)\)', content)
                                                if image_match:
                                                    image_data = image_match.group(1)
                                                    image_bytes = base64.b64decode(image_data)
                                                    edited_image = Image.open(io.BytesIO(image_bytes))
                                                    print("âœ… æˆåŠŸæå–T8é•œåƒç«™ç¼–è¾‘åçš„å›¾åƒ")
                                                    # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                                    response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç¼–è¾‘]', content)
                                            except Exception as e:
                                                print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                        else:
                            # å…¶ä»–æ¨¡å‹ä½¿ç”¨Geminiæ ¼å¼
                            if "candidates" in result and result["candidates"]:
                                candidate = result["candidates"][0]
                                if "content" in candidate and "parts" in candidate["content"]:
                                    for part in candidate["content"]["parts"]:
                                        # æå–æ–‡æœ¬
                                        if "text" in part:
                                            response_text += part["text"]

                                        # æå–ç¼–è¾‘åçš„å›¾ç‰‡ï¼ˆæ”¯æŒinline_dataå’ŒinlineDataä¸¤ç§æ ¼å¼ï¼‰
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                import io
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print(f"âœ… æˆåŠŸæå–T8é•œåƒç«™ç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")
                                            except Exception as e:
                                                print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")

                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡
                        if not edited_image:
                            print("âš ï¸ T8é•œåƒç«™æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = all_input_pils[0]
                            if not response_text:
                                response_text = "T8é•œåƒç«™å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
                        request_data = {
                            "model": _normalize_model_name(model),
                            "messages": [{
                                "role": "user",
                                "content": enhanced_prompt
                            }],
                            "temperature": temperature,
                            "max_tokens": max_output_tokens,
                            "stream": True
                        }
                        
                        # è®¾ç½®è¯·æ±‚å¤´
                        headers = {
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {api_key.strip()}"
                        }
                    else:
                        # æ ‡å‡†Gemini APIå“åº”å¤„ç†
                        try:
                            result = response.json()
                        except Exception as e:
                            print(f"âš ï¸ æ ‡å‡†Gemini JSONè§£æå¤±è´¥: {e}")
                            result = {}
                        if "candidates" in result and result["candidates"]:
                            candidate = result["candidates"][0]
                            if "content" in candidate and "parts" in candidate["content"]:
                                for part in candidate["content"]["parts"]:
                                    # æå–æ–‡æœ¬
                                    if "text" in part:
                                        response_text += part["text"]
                                    
                                    # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                    if "inline_data" in part or "inlineData" in part:
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                # è§£ç å›¾ç‰‡æ•°æ®
                                                import io
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                            except Exception as e:
                                                print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = all_input_pils[0]
                        if not response_text:
                            response_text = "å›¾ç‰‡ç¼–è¾‘è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"

                    # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                    if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                        try:
                            # æå–æ”¾å¤§å€æ•°
                            scale = int(upscale_factor.replace("x", "").strip().split()[0])
                            if scale > 1:
                                print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                try:
                                    from .banana_upscale import smart_upscale
                                except ImportError:
                                    from banana_upscale import smart_upscale

                                # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                target_w = edited_image.width * scale
                                target_h = edited_image.height * scale

                                # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                upscaled_image = smart_upscale(
                                    edited_image,
                                    target_w,
                                    target_h,
                                    gigapixel_model
                                )

                                if upscaled_image:
                                    edited_image = upscaled_image
                                    print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                else:
                                    print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                        except Exception as e:
                            print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                    
                    # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
                    if not response_text:
                        response_text = "å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„æŒ‡ä»¤å’Œå‚è€ƒå›¾åƒç”Ÿæˆçš„ç¼–è¾‘ç»“æœã€‚"
                        print("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆ")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
                    # print(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")  # æ³¨é‡Šæ‰å¯èƒ½åŒ…å«base64æ•°æ®çš„è¾“å‡º
                    print(f"ğŸ“ å“åº”æ–‡æœ¬ç±»å‹: {'åŒ…å«å›¾åƒæ•°æ®' if 'data:image/' in response_text else 'çº¯æ–‡æœ¬å†…å®¹'}")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    # æ™ºèƒ½ç­‰å¾…
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {error_msg}")


def parse_openai_compatible_response(response_data):
    """è§£æOpenAIå…¼å®¹æ ¼å¼çš„å“åº”æ•°æ®"""
    response_text = ""
    generated_image = None
    
    try:
        if "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            if "message" in choice and "content" in choice["message"]:
                content = choice["message"]["content"]
                
                if isinstance(content, str):
                    response_text = content
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®ï¼ˆbase64æˆ–URLï¼‰
                    if "![image](" in content:
                        print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenAIå…¼å®¹æ ¼å¼è¿”å›çš„å›¾åƒæ•°æ®")
                        try:
                            import re
                            # æå–æ‰€æœ‰å›¾åƒæ ‡è®°
                            image_matches = re.findall(r'!\[image\]\(([^)]+)\)', content)
                            
                            for image_url in image_matches:
                                if image_url.startswith("data:image/"):
                                    # å¤„ç†base64å›¾åƒæ•°æ®
                                    try:
                                        image_data = image_url.split(",")[1]
                                        image_bytes = base64.b64decode(image_data)
                                        generated_image = Image.open(io.BytesIO(image_bytes))
                                        print("âœ… æˆåŠŸæå–base64å›¾åƒæ•°æ®")
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç”Ÿæˆ]', content)
                                        break
                                    except Exception as e:
                                        print(f"âš ï¸ base64å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                elif image_url.startswith("http"):
                                    # å¤„ç†å¤–éƒ¨å›¾åƒURL
                                    try:
                                        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å›¾åƒ: {image_url}")
                                        response = requests.get(image_url, timeout=30)
                                        if response.status_code == 200:
                                            image_bytes = response.content
                                            generated_image = Image.open(io.BytesIO(image_bytes))
                                            print("âœ… æˆåŠŸä¸‹è½½å¹¶æå–å¤–éƒ¨å›¾åƒ")
                                            # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤URL
                                            response_text = re.sub(r'!\[image\]\([^)]+\)', '[å›¾åƒå·²ç”Ÿæˆ]', content)
                                            break
                                        else:
                                            print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                    except Exception as e:
                                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
                                else:
                                    print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {image_url}")
                        except Exception as e:
                            print(f"âš ï¸ å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                            
                elif isinstance(content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                    for item in content:
                        if item.get("type") == "text":
                            response_text += item.get("text", "")
                        elif item.get("type") == "image_url":
                            # å¤„ç†å›¾åƒURLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒURLå“åº”")
                            image_url = item.get("image_url", {}).get("url", "")
                            if image_url.startswith("data:image/"):
                                try:
                                    # æå–base64æ•°æ®
                                    image_data = image_url.split(",")[1]
                                    image_bytes = base64.b64decode(image_data)
                                    generated_image = Image.open(io.BytesIO(image_bytes))
                                    print("âœ… æˆåŠŸæå–base64å›¾åƒæ•°æ®")
                                except Exception as e:
                                    print(f"âš ï¸ base64å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                            elif image_url.startswith("http"):
                                try:
                                    # ä¸‹è½½å¤–éƒ¨å›¾åƒURL
                                    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å›¾åƒ: {image_url}")
                                    response = requests.get(image_url, timeout=30)
                                    if response.status_code == 200:
                                        image_bytes = response.content
                                        generated_image = Image.open(io.BytesIO(image_bytes))
                                        print("âœ… æˆåŠŸä¸‹è½½å¹¶æå–å¤–éƒ¨å›¾åƒ")
                                    else:
                                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                except Exception as e:
                                    print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
                            else:
                                print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾åƒURLæ ¼å¼: {image_url}")
        
        if not response_text:
            response_text = "OpenAIå…¼å®¹æ ¼å¼å“åº”å¤„ç†å®Œæˆ"
            
    except Exception as e:
        print(f"âš ï¸ OpenAIå…¼å®¹æ ¼å¼å“åº”è§£æå¤±è´¥: {e}")
        response_text = f"å“åº”è§£æå¤±è´¥: {e}"
    
    return response_text, generated_image


# èŠ‚ç‚¹æ˜ å°„ - ä¿æŒä¸å‚è€ƒé¡¹ç›®ä¸€è‡´çš„å‘½åé£æ ¼
NODE_CLASS_MAPPINGS = {
    "KenChenLLMGeminiBananaMirrorImageGenNode": KenChenLLMGeminiBananaMirrorImageGenNode,
    "KenChenLLMGeminiBananaMirrorImageEditNode": KenChenLLMGeminiBananaMirrorImageEditNode,
    "GeminiBananaMirrorMultiImageEdit": KenChenLLMGeminiBananaMultiImageEditNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "KenChenLLMGeminiBananaMirrorImageGenNode": "ğŸŒ Gemini Banana é•œåƒå›¾åƒç”Ÿæˆ",
    "KenChenLLMGeminiBananaMirrorImageEditNode": "ğŸŒ Gemini Banana é•œåƒå›¾ç‰‡ç¼–è¾‘",
    "GeminiBananaMirrorMultiImageEdit": "ğŸŒ Gemini Banana é•œåƒå¤šå›¾åƒç¼–è¾‘",
}

# å¼ºåˆ¶è®¾ç½®èŠ‚ç‚¹é¢œè‰²
def setup_node_colors():
    """ä¸ºé•œåƒèŠ‚ç‚¹è®¾ç½®æ©™è‰²ä¸»é¢˜"""
    orange_color = "#D2691E"  # å·§å…‹åŠ›æ©™è‰²
    brown_bgcolor = "#8B4513"  # æ·±æ£•è‰²èƒŒæ™¯
    sand_groupcolor = "#CD853F"  # æ²™æ£•è‰²

    for node_class in [KenChenLLMGeminiBananaMirrorImageGenNode,
                       KenChenLLMGeminiBananaMirrorImageEditNode,
                       KenChenLLMGeminiBananaMultiImageEditNode]:
        # è®¾ç½®ç±»çº§åˆ«çš„é¢œè‰²å±æ€§
        node_class.color = orange_color
        node_class.bgcolor = brown_bgcolor
        node_class.groupcolor = sand_groupcolor

        # ç¡®ä¿å®ä¾‹ä¹Ÿæœ‰è¿™äº›é¢œè‰²
        original_init = getattr(node_class, '__init__', None)

        def colored_init(self, *args, **kwargs):
            if original_init:
                original_init(self, *args, **kwargs)
            self.color = orange_color
            self.bgcolor = brown_bgcolor
            self.groupcolor = sand_groupcolor

        node_class.__init__ = colored_init

# åº”ç”¨é¢œè‰²è®¾ç½®
setup_node_colors()
print("ğŸ¨ å·²ä¸ºé•œåƒèŠ‚ç‚¹è®¾ç½®æ©™è‰²ä¸»é¢˜")

