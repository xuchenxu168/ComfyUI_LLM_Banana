"""
Gemini Banana é•œåƒç«™èŠ‚ç‚¹
æ”¯æŒè‡ªå®šä¹‰APIåœ°å€ï¼Œé€‚é…å›½å†…é•œåƒç«™å’Œä»£ç†æœåŠ¡
"""

import torch
import numpy as np
from PIL import Image
import io
import base64
import requests
import json
import time
import random
from typing import Optional, Tuple, Dict, Any
import re
import os
try:
    from server import PromptServer
except ImportError:
    # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼ŒPromptServerå¯èƒ½ä¸å¯ç”¨
    class PromptServer:
        @staticmethod
        def instance():
            return None
import sys
from io import BytesIO
from PIL import Image, ImageFilter

# ğŸš€ AIæ”¾å¤§æ¨¡å‹é›†æˆ
def detect_available_upscale_models():
    """
    è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„AIæ”¾å¤§æ¨¡å‹
    """
    available_models = []
    
    # æ£€æµ‹ Real-ESRGAN
    try:
        import realesrgan
        available_models.append("Real-ESRGAN")
        print(f"âœ… æ£€æµ‹åˆ° Real-ESRGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ Real-ESRGAN æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ ESRGAN
    try:
        import esrgan
        available_models.append("ESRGAN")
        print(f"âœ… æ£€æµ‹åˆ° ESRGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ ESRGAN æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ Waifu2x
    try:
        import waifu2x
        available_models.append("Waifu2x")
        print(f"âœ… æ£€æµ‹åˆ° Waifu2x æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ Waifu2x æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ GFPGAN
    try:
        import gfpgan
        available_models.append("GFPGAN")
        print(f"âœ… æ£€æµ‹åˆ° GFPGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ GFPGAN æ¨¡å‹æœªå®‰è£…")
    
    print(f"ğŸ” å¯ç”¨AIæ”¾å¤§æ¨¡å‹: {available_models}")
    return available_models

def ai_upscale_with_realesrgan(image, target_width, target_height):
    """
    ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰ï¼Œä¼˜å…ˆ2xï¼Œå¤±è´¥å›é€€LANCZOSã€‚
    """
    try:
        try:
            from .banana_upscale import smart_upscale as _smart
        except ImportError:
            from banana_upscale import smart_upscale as _smart
        res = _smart(image, target_width, target_height)
        if res is not None:
            return res
        print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
        return image.resize((target_width, target_height), Image.Resampling.LANCZOS)
    except Exception as e:
        print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥ï¼Œå›é€€é‡é‡‡æ ·: {e}")
        return image.resize((target_width, target_height), Image.Resampling.LANCZOS)
def ai_upscale_with_esrgan(image, target_width, target_height):
    """
    ä½¿ç”¨ ESRGAN è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§
    """
    try:
        print(f"ğŸš€ ä½¿ç”¨ ESRGAN è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§...")
        
        # ESRGAN å®ç°ä»£ç 
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ESRGANå®ç°æ¥ç¼–å†™
        
        print(f"âœ… ESRGAN AIæ”¾å¤§å®Œæˆ")
        return image  # ä¸´æ—¶è¿”å›åŸå›¾
        
    except Exception as e:
        print(f"âŒ ESRGAN æ”¾å¤§å¤±è´¥: {e}")
        raise e

def ai_upscale_with_waifu2x(image, target_width, target_height):
    """
    ä½¿ç”¨ Waifu2x è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§
    """
    try:
        print(f"ğŸš€ ä½¿ç”¨ Waifu2x è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§...")
        
        # Waifu2x å®ç°ä»£ç 
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„Waifu2xå®ç°æ¥ç¼–å†™
        
        print(f"âœ… Waifu2x AIæ”¾å¤§å®Œæˆ")
        return image  # ä¸´æ—¶è¿”å›åŸå›¾
        
    except Exception as e:
        print(f"âŒ Waifu2x æ”¾å¤§å¤±è´¥: {e}")
        raise e

def smart_ai_upscale(image, target_width, target_height):
	"""
	ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰
	"""
	try:
		try:
			from .banana_upscale import smart_upscale as _smart
		except ImportError:
			from banana_upscale import smart_upscale as _smart
		return _smart(image, target_width, target_height)
	except Exception as e:
		print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥: {e}")
		return None


def detect_image_foreground_subject(image):
    """
    ğŸ¯ æ™ºèƒ½æ£€æµ‹å›¾åƒå‰æ™¯ä¸»ä½“ï¼ˆäººç‰©ã€ç‰©ä½“ç­‰ï¼‰
    è¿”å›ä¸»ä½“è¾¹ç•Œæ¡† (x, y, width, height) å’Œä¸­å¿ƒç‚¹
    """
    try:
        import cv2
        import numpy as np

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if hasattr(image, 'convert'):
            if image.mode != 'RGB':
                image = image.convert('RGB')
            img_array = np.array(image)
        else:
            img_array = image

        # è½¬æ¢ä¸ºBGRæ ¼å¼ï¼ˆOpenCVä½¿ç”¨BGRï¼‰
        if len(img_array.shape) == 3:
            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        else:
            img_bgr = img_array

        height, width = img_bgr.shape[:2]
        print(f"ğŸ” å›¾åƒå°ºå¯¸: {width}x{height}")

        # ğŸ¯ æ–¹æ³•1ï¼šå…¨å›¾äººè„¸æ£€æµ‹ï¼ˆæœ€ä¼˜å…ˆï¼‰
        try:
            face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, 1.1, 4)

            if len(faces) > 0:
                # æ‰¾åˆ°æœ€å¤§çš„äººè„¸
                largest_face = max(faces, key=lambda f: f[2] * f[3])
                x, y, w, h = largest_face

                # ğŸš€ å…³é”®ä¿®å¤ï¼šä»¥äººè„¸ä¸ºä¸­å¿ƒï¼Œæ‰©å±•åˆ°åˆç†çš„äººç‰©åŒºåŸŸ
                # äººè„¸é€šå¸¸å äººç‰©é«˜åº¦çš„1/8åˆ°1/6
                estimated_person_height = h * 6  # å‡è®¾äººè„¸æ˜¯äººç‰©é«˜åº¦çš„1/6
                estimated_person_width = w * 3   # å‡è®¾äººç‰©å®½åº¦æ˜¯äººè„¸å®½åº¦çš„3å€

                # è®¡ç®—äººç‰©åŒºåŸŸï¼ˆä»¥äººè„¸ä¸­å¿ƒä¸ºåŸºå‡†ï¼‰
                face_center_x = x + w // 2
                face_center_y = y + h // 2

                # äººç‰©åŒºåŸŸçš„å·¦ä¸Šè§’ï¼ˆäººè„¸é€šå¸¸åœ¨äººç‰©ä¸Šéƒ¨1/4å¤„ï¼‰
                person_x = max(0, face_center_x - estimated_person_width // 2)
                person_y = max(0, face_center_y - estimated_person_height // 4)

                # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
                person_w = min(estimated_person_width, width - person_x)
                person_h = min(estimated_person_height, height - person_y)

                subject_center_x = person_x + person_w // 2
                subject_center_y = person_y + person_h // 2

                print(f"ğŸ¯ äººè„¸æ£€æµ‹è¯†åˆ«åˆ°ä¸»ä½“: äººè„¸({x}, {y}, {w}x{h}), äººç‰©åŒºåŸŸ({person_x}, {person_y}, {person_w}x{person_h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                return (person_x, person_y, person_w, person_h), (subject_center_x, subject_center_y)
        except Exception as e:
            print(f"âš ï¸ äººè„¸æ£€æµ‹å¤±è´¥: {e}")

        # ğŸ¯ æ–¹æ³•2ï¼šå…¨å›¾è‚¤è‰²æ£€æµ‹ï¼ˆæ”¹è¿›ç‰ˆï¼‰
        try:
            hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)

            # æ‰©å±•è‚¤è‰²èŒƒå›´ï¼ŒåŒ…å«æ›´å¤šè‚¤è‰²ç±»å‹
            lower_skin1 = np.array([0, 20, 70])    # åçº¢è‚¤è‰²
            upper_skin1 = np.array([20, 255, 255])

            lower_skin2 = np.array([0, 10, 60])    # è¾ƒæµ…è‚¤è‰²
            upper_skin2 = np.array([25, 255, 255])

            # åˆ›å»ºè‚¤è‰²æ©ç 
            skin_mask1 = cv2.inRange(hsv, lower_skin1, upper_skin1)
            skin_mask2 = cv2.inRange(hsv, lower_skin2, upper_skin2)
            skin_mask = cv2.bitwise_or(skin_mask1, skin_mask2)

            # å½¢æ€å­¦æ“ä½œ
            kernel = np.ones((7,7), np.uint8)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)
            skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)

            # æ‰¾åˆ°è‚¤è‰²åŒºåŸŸ
            contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            if contours:
                # è¿‡æ»¤å¤ªå°çš„åŒºåŸŸ
                min_area = (width * height) * 0.005  # æœ€å°é¢ç§¯é˜ˆå€¼
                valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]

                if valid_contours:
                    # æ‰¾åˆ°æœ€å¤§çš„è‚¤è‰²åŒºåŸŸ
                    largest_contour = max(valid_contours, key=cv2.contourArea)
                    x, y, w, h = cv2.boundingRect(largest_contour)

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šåˆç†æ‰©å±•è¾¹ç•Œæ¡†
                    # è‚¤è‰²åŒºåŸŸé€šå¸¸åªæ˜¯äººç‰©çš„ä¸€éƒ¨åˆ†ï¼Œéœ€è¦é€‚å½“æ‰©å±•
                    expand_factor_w = 1.5  # å®½åº¦æ‰©å±•ç³»æ•°
                    expand_factor_h = 1.8  # é«˜åº¦æ‰©å±•ç³»æ•°

                    expanded_w = int(w * expand_factor_w)
                    expanded_h = int(h * expand_factor_h)

                    # é‡æ–°è®¡ç®—ä½ç½®ï¼Œä¿æŒä¸­å¿ƒä¸å˜
                    expanded_x = max(0, x - (expanded_w - w) // 2)
                    expanded_y = max(0, y - (expanded_h - h) // 4)  # å‘ä¸Šæ‰©å±•æ›´å¤š

                    # ç¡®ä¿ä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
                    expanded_w = min(expanded_w, width - expanded_x)
                    expanded_h = min(expanded_h, height - expanded_y)

                    subject_center_x = expanded_x + expanded_w // 2
                    subject_center_y = expanded_y + expanded_h // 2

                    print(f"ğŸ¯ è‚¤è‰²æ£€æµ‹è¯†åˆ«åˆ°ä¸»ä½“: åŸå§‹({x}, {y}, {w}x{h}), æ‰©å±•å({expanded_x}, {expanded_y}, {expanded_w}x{expanded_h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                    return (expanded_x, expanded_y, expanded_w, expanded_h), (subject_center_x, subject_center_y)

        except Exception as e:
            print(f"âš ï¸ è‚¤è‰²æ£€æµ‹å¤±è´¥: {e}")
        
        # ğŸ¯ æ–¹æ³•3ï¼šæ”¹è¿›çš„è¾¹ç¼˜æ£€æµ‹ï¼ˆä¸“æ³¨äºäººç‰©è½®å»“ï¼‰
        try:
            # è½¬æ¢ä¸ºç°åº¦å›¾
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

            # é«˜æ–¯æ¨¡ç³Šå‡å°‘å™ªå£°
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)

            # ä½¿ç”¨æ›´ä¿å®ˆçš„è¾¹ç¼˜æ£€æµ‹å‚æ•°
            edges = cv2.Canny(blurred, 30, 100)

            # å½¢æ€å­¦æ“ä½œè¿æ¥è¾¹ç¼˜
            kernel = np.ones((5,5), np.uint8)
            edges = cv2.morphologyEx(edges, cv2.MORPH_CLOSE, kernel)

            # æ‰¾åˆ°è½®å»“
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

            if contours:
                # è¿‡æ»¤å°è½®å»“ï¼Œæ‰¾åˆ°ä¸»è¦ç‰©ä½“
                min_area = (width * height) * 0.02  # æé«˜æœ€å°é¢ç§¯é˜ˆå€¼
                valid_contours = [c for c in contours if cv2.contourArea(c) > min_area]

                if valid_contours:
                    # æ‰¾åˆ°æœ€å¤§çš„è½®å»“
                    largest_contour = max(valid_contours, key=cv2.contourArea)
                    x, y, w, h = cv2.boundingRect(largest_contour)

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ£€æµ‹åˆ°çš„æ˜¯åˆç†çš„äººç‰©åŒºåŸŸ
                    aspect_ratio = w / h
                    if 0.3 <= aspect_ratio <= 2.0:  # äººç‰©çš„å®½é«˜æ¯”é€šå¸¸åœ¨è¿™ä¸ªèŒƒå›´å†…
                        subject_center_x = x + w // 2
                        subject_center_y = y + h // 2

                        print(f"ğŸ¯ è¾¹ç¼˜æ£€æµ‹è¯†åˆ«åˆ°å‰æ™¯ä¸»ä½“: ä½ç½®({x}, {y}), å°ºå¯¸({w}x{h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                        return (x, y, w, h), (subject_center_x, subject_center_y)

        except Exception as e:
            print(f"âš ï¸ è¾¹ç¼˜æ£€æµ‹ç®—æ³•å¤±è´¥: {e}")

        # ğŸ¯ æ–¹æ³•4ï¼šå®‰å…¨çš„ä¿å®ˆç­–ç•¥ï¼ˆç¡®ä¿ä¸»ä½“ä¸ä¸¢å¤±ï¼‰
        try:
            print(f"ğŸ” ä½¿ç”¨å®‰å…¨ä¿å®ˆç­–ç•¥...")

            # ğŸš€ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´ä¿å®ˆçš„ä¸»ä½“ä½ç½®ä¼°è®¡
            # åŸºäºå›¾åƒçš„é»„é‡‘åˆ†å‰²ç‚¹å’Œå¸¸è§äººç‰©ä½ç½®

            # åˆ†æå›¾åƒçš„äº®åº¦åˆ†å¸ƒï¼Œæ‰¾åˆ°å¯èƒ½çš„ä¸»ä½“åŒºåŸŸ
            gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

            # å°†å›¾åƒåˆ†ä¸º9ä¸ªåŒºåŸŸï¼ˆ3x3ç½‘æ ¼ï¼‰
            h_step = height // 3
            w_step = width // 3

            max_variance = 0
            best_region = None

            for i in range(3):
                for j in range(3):
                    region_y = i * h_step
                    region_x = j * w_step
                    region = gray[region_y:region_y+h_step, region_x:region_x+w_step]

                    # è®¡ç®—åŒºåŸŸçš„æ–¹å·®ï¼ˆé«˜æ–¹å·®é€šå¸¸è¡¨ç¤ºæœ‰æ›´å¤šç»†èŠ‚ï¼Œå¯èƒ½æ˜¯ä¸»ä½“ï¼‰
                    variance = np.var(region)
                    if variance > max_variance:
                        max_variance = variance
                        best_region = (region_x, region_y, w_step, h_step)

            if best_region:
                x, y, w, h = best_region
                subject_center_x = x + w // 2
                subject_center_y = y + h // 2

                print(f"ğŸ¯ æ–¹å·®åˆ†æè¯†åˆ«åˆ°ä¸»ä½“åŒºåŸŸ: ä½ç½®({x}, {y}), å°ºå¯¸({w}x{h}), ä¸­å¿ƒ({subject_center_x}, {subject_center_y})")
                return (x, y, w, h), (subject_center_x, subject_center_y)

        except Exception as e:
            print(f"âš ï¸ æ–¹å·®åˆ†æå¤±è´¥: {e}")

        # ğŸ¯ æœ€ç»ˆå®‰å…¨ç­–ç•¥ï¼šåŸºäºå›¾åƒä¸­å¿ƒçš„ä¿å®ˆä¼°è®¡
        print(f"ğŸ” ä½¿ç”¨æœ€ç»ˆå®‰å…¨ç­–ç•¥ï¼šå›¾åƒä¸­å¿ƒåŒºåŸŸ...")

        # ğŸš€ å…³é”®ä¿®å¤ï¼šä½¿ç”¨å›¾åƒä¸­å¿ƒåŒºåŸŸä½œä¸ºä¸»ä½“ä½ç½®
        # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸»ä½“ä¸ä¼šå®Œå…¨ä¸¢å¤±
        safe_x = width // 4
        safe_y = height // 4
        safe_w = width // 2
        safe_h = height // 2

        safe_center_x = safe_x + safe_w // 2  # å›¾åƒä¸­å¿ƒ
        safe_center_y = safe_y + safe_h // 2  # å›¾åƒä¸­å¿ƒ

        print(f"ğŸ¯ å®‰å…¨ç­–ç•¥ä¸»ä½“ä½ç½®: ({safe_x}, {safe_y}), å°ºå¯¸({safe_w}x{safe_h}), ä¸­å¿ƒ({safe_center_x}, {safe_center_y})")
        return (safe_x, safe_y, safe_w, safe_h), (safe_center_x, safe_center_y)
        
    except ImportError:
        print("âš ï¸ OpenCVæœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œæ™ºèƒ½ä¸»ä½“æ£€æµ‹")
        # ğŸš€ å…³é”®ä¿®å¤ï¼šè¿”å›å®‰å…¨çš„å›¾åƒä¸­å¿ƒä½ç½®
        width, height = image.size
        safe_center_x = width // 2   # å›¾åƒä¸­å¿ƒ
        safe_center_y = height // 2  # å›¾åƒä¸­å¿ƒ
        safe_w = width // 2
        safe_h = height // 2
        safe_x = width // 4
        safe_y = height // 4
        print(f"ğŸ¯ é»˜è®¤å®‰å…¨ä½ç½®: ({safe_x}, {safe_y}), å°ºå¯¸({safe_w}x{safe_h}), ä¸­å¿ƒ({safe_center_x}, {safe_center_y})")
        return (safe_x, safe_y, safe_w, safe_h), (safe_center_x, safe_center_y)
    except Exception as e:
        print(f"âŒ æ™ºèƒ½ä¸»ä½“æ£€æµ‹å¤±è´¥: {e}")
        # ğŸš€ å…³é”®ä¿®å¤ï¼šè¿”å›å®‰å…¨çš„å›¾åƒä¸­å¿ƒä½ç½®
        width, height = image.size
        safe_center_x = width // 2   # å›¾åƒä¸­å¿ƒ
        safe_center_y = height // 2  # å›¾åƒä¸­å¿ƒ
        safe_w = width // 2
        safe_h = height // 2
        safe_x = width // 4
        safe_y = height // 4
        print(f"ğŸ¯ å¼‚å¸¸å¤„ç†å®‰å…¨ä½ç½®: ({safe_x}, {safe_y}), å°ºå¯¸({safe_w}x{safe_h}), ä¸­å¿ƒ({safe_center_x}, {safe_center_y})")
        return (safe_x, safe_y, safe_w, safe_h), (safe_center_x, safe_center_y)

def _log_info(message):
    try:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] INFO: {repr(message)}")

def _log_warning(message):
    try:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] WARNING: {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] WARNING: {repr(message)}")

def _log_error(message):
    try:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] ERROR: {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana-Mirror] ERROR: {repr(message)}")

def tensor_to_pil(tensor):
    """Convert tensor to PIL Image"""
    if len(tensor.shape) == 4:
        tensor = tensor.squeeze(0)
    if tensor.shape[0] == 3:
        tensor = tensor.permute(1, 2, 0)
    tensor = (tensor * 255).clamp(0, 255).byte()
    return Image.fromarray(tensor.cpu().numpy())

def pil_to_tensor(image):
    """Convert PIL Image to tensor"""
    if image.mode != 'RGB':
        image = image.convert('RGB')
    image_array = np.array(image).astype(np.float32) / 255.0
    tensor = torch.from_numpy(image_array).unsqueeze(0)
    return tensor

def enhance_image_quality(image: Image.Image, quality: str = "hd", adaptive_mode: str = "disabled") -> Image.Image:
    """
    ğŸš€ å…¨é¢AIç”»è´¨å¢å¼ºç³»ç»Ÿ
    æ™ºèƒ½è¯†åˆ«å›¾åƒç±»å‹å¹¶åº”ç”¨æœ€é€‚åˆçš„å¢å¼ºæŠ€æœ¯
    """
    print(f"ğŸ¨ å¼€å§‹å…¨é¢AIç”»è´¨å¢å¼ºï¼Œè´¨é‡çº§åˆ«: {quality}")

    # ä¿å­˜åŸå›¾ç”¨äºè¿‡åº¦å¢å¼ºæ£€æµ‹
    original_image = image.copy()

    # ğŸš€ ç¬¬ä¸€æ­¥ï¼šæ™ºèƒ½å›¾åƒç±»å‹è¯†åˆ«
    image_type = _analyze_image_type(image)
    print(f"ğŸ” å›¾åƒç±»å‹è¯†åˆ«: {image_type}")

    # ğŸš€ ç¬¬äºŒæ­¥ï¼šæ ¹æ®å›¾åƒç±»å‹é€‰æ‹©æœ€ä½³å¢å¼ºç­–ç•¥
    if image_type == "portrait":
        # äººåƒå›¾åƒï¼šä¼˜å…ˆäººè„¸ä¿®å¤
        enhanced_image = _apply_ai_face_restoration(image)
        if enhanced_image:
            print(f"âœ… AIäººè„¸ä¿®å¤å®Œæˆ")
            image = enhanced_image

    elif image_type == "landscape":
        # é£æ™¯å›¾åƒï¼šä½¿ç”¨é£æ™¯ä¸“ç”¨å¢å¼º
        enhanced_image = _apply_landscape_enhancement(image, quality)
        if enhanced_image:
            print(f"âœ… é£æ™¯ä¸“ç”¨å¢å¼ºå®Œæˆ")
            image = enhanced_image

    elif image_type == "architecture":
        # å»ºç­‘å›¾åƒï¼šä½¿ç”¨å»ºç­‘ä¸“ç”¨å¢å¼º
        enhanced_image = _apply_architecture_enhancement(image, quality)
        if enhanced_image:
            print(f"âœ… å»ºç­‘ä¸“ç”¨å¢å¼ºå®Œæˆ")
            image = enhanced_image

    elif image_type == "artwork":
        # è‰ºæœ¯ä½œå“ï¼šä½¿ç”¨è‰ºæœ¯ä¸“ç”¨å¢å¼º
        enhanced_image = _apply_artwork_enhancement(image, quality)
        if enhanced_image:
            print(f"âœ… è‰ºæœ¯ä¸“ç”¨å¢å¼ºå®Œæˆ")
            image = enhanced_image

    else:
        # é€šç”¨å›¾åƒï¼šä½¿ç”¨é€šç”¨AIå¢å¼º
        print(f"ğŸ” ä½¿ç”¨é€šç”¨AIå¢å¼ºç­–ç•¥")

    # ğŸš€ ç¬¬ä¸‰æ­¥ï¼šé€šç”¨AIè¶…åˆ†è¾¨ç‡å¢å¼ºï¼ˆé€‚ç”¨äºæ‰€æœ‰ç±»å‹ï¼‰
    if quality == "ultra_hd":
        sr_enhanced = _apply_ai_super_resolution(image, image_type)
        if sr_enhanced:
            print(f"âœ… AIè¶…åˆ†è¾¨ç‡å¢å¼ºå®Œæˆ")
            image = sr_enhanced
        else:
            print(f"ğŸ” AIè¶…åˆ†è¾¨ç‡ä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿå¢å¼º")

    # ğŸš€ ç¬¬å››æ­¥ï¼šè‡ªé€‚åº”ä¼ ç»Ÿå¢å¼ºï¼ˆæ ¹æ®å›¾åƒç±»å‹è°ƒæ•´å‚æ•°ï¼‰
    if quality in ["hd", "ultra_hd"]:
        image = _apply_adaptive_traditional_enhancement(image, quality, image_type)

    # ğŸš€ ç¬¬äº”æ­¥ï¼šæ™ºèƒ½è¿‡åº¦å¢å¼ºæ£€æµ‹å’Œä¿®æ­£
    final_image = _check_and_correct_over_enhancement(original_image, image)  # ä¼ å…¥åŸå›¾å’Œå¢å¼ºåçš„å›¾
    if final_image != image:
        print(f"ğŸ”§ æ£€æµ‹åˆ°è¿‡åº¦å¢å¼ºï¼Œå·²è‡ªåŠ¨ä¿®æ­£")
        image = final_image

    print(f"ğŸ¨ å…¨é¢ç”»è´¨å¢å¼ºæµç¨‹å®Œæˆ")
    return image


def _apply_ai_face_restoration(image: Image.Image) -> Optional[Image.Image]:
    """
    ğŸš€ AIäººè„¸ä¿®å¤ï¼šä¼˜å…ˆä¸“ä¸šæ¨¡å‹ï¼Œå›é€€åˆ°æ™ºèƒ½å¢å¼º
    """
    try:
        # é¦–å…ˆæ£€æµ‹æ˜¯å¦æœ‰äººè„¸
        faces = _detect_faces_with_locations(image)
        if not faces:
            return None

        print(f"ğŸ¯ æ£€æµ‹åˆ°{len(faces)}ä¸ªäººè„¸ï¼Œå¼€å§‹AIä¿®å¤...")

        # å°è¯•CodeFormerï¼ˆæœ€æ–°æŠ€æœ¯ï¼‰
        restored = _try_codeformer_restoration(image)
        if restored:
            print(f"âœ… CodeFormeräººè„¸ä¿®å¤æˆåŠŸ")
            return restored

        # å›é€€åˆ°GFPGAN
        restored = _try_gfpgan_restoration(image)
        if restored:
            print(f"âœ… GFPGANäººè„¸ä¿®å¤æˆåŠŸ")
            return restored

        # ğŸš€ å®ç”¨çš„å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼ºäººè„¸åŒºåŸŸ
        enhanced = _enhance_face_regions_with_ai_upscale(image, faces)
        if enhanced:
            print(f"âœ… AIæ”¾å¤§äººè„¸å¢å¼ºæˆåŠŸ")
            return enhanced

        print(f"âš ï¸ æ‰€æœ‰AIäººè„¸ä¿®å¤æ–¹æ³•ä¸å¯ç”¨")
        return None

    except Exception as e:
        print(f"âŒ AIäººè„¸ä¿®å¤å¤±è´¥: {e}")
        return None


def _apply_ai_super_resolution(image: Image.Image, image_type: str = "general") -> Optional[Image.Image]:
    """
    ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºï¼šæ ¹æ®å›¾åƒç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥
    """
    try:
        # ğŸš€ æ ¹æ®å›¾åƒç±»å‹è°ƒæ•´æ”¾å¤§ç­–ç•¥
        current_size = max(image.size)

        if image_type == "portrait":
            # äººåƒï¼šéå¸¸ä¿å®ˆï¼Œä¿æŒè‡ªç„¶
            target_scale = 1.15 if current_size < 1024 else 1.08
        elif image_type == "landscape":
            # é£æ™¯ï¼šæ¸©å’Œæ”¾å¤§ï¼Œä¿æŒçœŸå®æ„Ÿ
            target_scale = 1.25 if current_size < 1024 else 1.12
        elif image_type == "architecture":
            # å»ºç­‘ï¼šé€‚åº¦æ”¾å¤§ï¼Œä¿æŒæè´¨è´¨æ„Ÿ
            target_scale = 1.2 if current_size < 1024 else 1.1
        elif image_type == "artwork":
            # è‰ºæœ¯ä½œå“ï¼šæœ€å°æ”¾å¤§ï¼Œä¿æŒåŸæœ‰é£æ ¼
            target_scale = 1.1 if current_size < 1024 else 1.05
        else:
            # é€šç”¨ï¼šä¿å®ˆç­–ç•¥
            target_scale = 1.2 if current_size < 1024 else 1.1

        target_w = int(image.width * target_scale)
        target_h = int(image.height * target_scale)

        print(f"ğŸš€ å°è¯•AIè¶…åˆ†è¾¨ç‡å¢å¼º: {image.size} -> ({target_w}, {target_h})")

        # ä½¿ç”¨ç°æœ‰çš„æ™ºèƒ½æ”¾å¤§ç³»ç»Ÿ
        enhanced = smart_ai_upscale(image, target_w, target_h)
        if enhanced:
            # å¦‚æœæ”¾å¤§æˆåŠŸï¼Œç¼©å›åŸå°ºå¯¸ä»¥ä¿æŒç»†èŠ‚æå‡
            final = enhanced.resize(image.size, Image.Resampling.LANCZOS)
            print(f"âœ… AIè¶…åˆ†è¾¨ç‡å¢å¼ºæˆåŠŸ")
            return final

        return None

    except Exception as e:
        print(f"âŒ AIè¶…åˆ†è¾¨ç‡å¢å¼ºå¤±è´¥: {e}")
        return None


def _analyze_image_type(image: Image.Image) -> str:
    """
    ğŸš€ æ™ºèƒ½å›¾åƒç±»å‹è¯†åˆ«
    åˆ†æå›¾åƒå†…å®¹å¹¶è¿”å›æœ€é€‚åˆçš„å¢å¼ºç­–ç•¥ç±»å‹
    """
    try:
        import cv2
        import numpy as np

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        img_array = np.array(image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)

        # æ£€æµ‹äººè„¸
        faces = _detect_faces_with_locations(image)
        if faces:
            face_area = sum(w * h for x, y, w, h in faces)
            total_area = image.width * image.height
            face_ratio = face_area / total_area

            if face_ratio > 0.05:  # äººè„¸å æ¯”è¶…è¿‡5%
                return "portrait"

        # åˆ†æé¢œè‰²åˆ†å¸ƒå’Œçº¹ç†ç‰¹å¾
        hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)

        # æ£€æµ‹å¤©ç©ºï¼ˆè“è‰²åŒºåŸŸï¼‰
        blue_mask = cv2.inRange(hsv, np.array([100, 50, 50]), np.array([130, 255, 255]))
        blue_ratio = np.sum(blue_mask > 0) / (image.width * image.height)

        # æ£€æµ‹ç»¿è‰²æ¤è¢«
        green_mask = cv2.inRange(hsv, np.array([40, 40, 40]), np.array([80, 255, 255]))
        green_ratio = np.sum(green_mask > 0) / (image.width * image.height)

        # æ£€æµ‹å»ºç­‘ç‰¹å¾ï¼ˆç›´çº¿å’Œè¾¹ç¼˜ï¼‰
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)
        edges = cv2.Canny(gray, 50, 150)

        # æ£€æµ‹ç›´çº¿ï¼ˆå»ºç­‘ç‰¹å¾ï¼‰
        lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=100, minLineLength=50, maxLineGap=10)
        line_count = len(lines) if lines is not None else 0

        # æ ¹æ®ç‰¹å¾åˆ¤æ–­å›¾åƒç±»å‹
        if blue_ratio > 0.15 and green_ratio > 0.2:
            return "landscape"  # é£æ™¯ï¼šæœ‰å¤©ç©ºå’Œæ¤è¢«
        elif line_count > 20:
            return "architecture"  # å»ºç­‘ï¼šæœ‰å¾ˆå¤šç›´çº¿
        elif blue_ratio < 0.05 and green_ratio < 0.1 and line_count < 10:
            # æ£€æµ‹è‰ºæœ¯ä½œå“ç‰¹å¾ï¼ˆè‰²å½©ä¸°å¯Œåº¦ï¼‰
            colors = img_array.reshape(-1, 3)
            unique_colors = len(np.unique(colors.view(np.dtype((np.void, colors.dtype.itemsize*colors.shape[1])))))
            color_diversity = unique_colors / (image.width * image.height)

            if color_diversity > 0.3:
                return "artwork"  # è‰ºæœ¯ä½œå“ï¼šè‰²å½©ä¸°å¯Œ

        return "general"  # é€šç”¨å›¾åƒ

    except Exception as e:
        print(f"âš ï¸ å›¾åƒç±»å‹è¯†åˆ«å¤±è´¥: {e}")
        return "general"


def _apply_landscape_enhancement(image: Image.Image, quality: str) -> Optional[Image.Image]:
    """
    ğŸš€ è‡ªç„¶é£æ™¯ä¸“ç”¨å¢å¼º - ä¿æŒçœŸå®æ„Ÿ
    """
    try:
        print(f"ğŸŒ„ åº”ç”¨è‡ªç„¶é£æ™¯å¢å¼º...")
        from PIL import ImageEnhance, ImageFilter
        import numpy as np

        enhanced = image.copy()

        # ğŸš€ è‡ªç„¶é£æ™¯å¢å¼ºç­–ç•¥ï¼ˆä¿æŒçœŸå®æ„Ÿï¼‰ï¼š
        # 1. è½»å¾®å¢å¼ºå¯¹æ¯”åº¦ï¼ˆé¿å…è¿‡åº¦HDRæ•ˆæœï¼‰
        # 2. æ¸©å’Œæå‡é¥±å’Œåº¦ï¼ˆä¿æŒè‡ªç„¶è‰²å½©ï¼‰
        # 3. ç»†èŠ‚å¢å¼ºä½†ä¸è¿‡åº¦é”åŒ–

        # è½»å¾®å¯¹æ¯”åº¦å¢å¼ºï¼ˆé¿å…å‡HDRæ•ˆæœï¼‰
        contrast_enhancer = ImageEnhance.Contrast(enhanced)
        enhanced = contrast_enhancer.enhance(1.06)  # é™ä½åˆ°æ›´è‡ªç„¶çš„æ°´å¹³

        # æ¸©å’Œé¥±å’Œåº¦å¢å¼ºï¼ˆä¿æŒè‡ªç„¶ï¼‰
        color_enhancer = ImageEnhance.Color(enhanced)
        enhanced = color_enhancer.enhance(1.04)  # æ›´æ¸©å’Œçš„é¥±å’Œåº¦

        # è‡ªç„¶çš„ç»†èŠ‚å¢å¼º
        if quality == "ultra_hd":
            # ä½¿ç”¨éå¸¸æ¸©å’Œçš„é”åŒ–
            enhanced = enhanced.filter(ImageFilter.UnsharpMask(radius=0.8, percent=80, threshold=5))
        else:
            sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = sharpness_enhancer.enhance(1.08)  # æ¸©å’Œé”åŒ–

        # ä¿æŒè‡ªç„¶äº®åº¦
        brightness_enhancer = ImageEnhance.Brightness(enhanced)
        enhanced = brightness_enhancer.enhance(1.01)  # å¾®è°ƒå³å¯

        print(f"âœ… è‡ªç„¶é£æ™¯å¢å¼ºå®Œæˆ")
        return enhanced

    except Exception as e:
        print(f"âŒ é£æ™¯å¢å¼ºå¤±è´¥: {e}")
        return None


def _apply_architecture_enhancement(image: Image.Image, quality: str) -> Optional[Image.Image]:
    """
    ğŸš€ å»ºç­‘å›¾åƒä¸“ç”¨å¢å¼º - ä¿æŒçœŸå®æ„Ÿ
    """
    try:
        print(f"ğŸ¢ åº”ç”¨å»ºç­‘ä¸“ç”¨å¢å¼º...")
        from PIL import ImageEnhance, ImageFilter

        enhanced = image.copy()

        # ğŸš€ å»ºç­‘å¢å¼ºç­–ç•¥ï¼ˆä¿æŒçœŸå®æ„Ÿï¼‰ï¼š
        # 1. é€‚åº¦å¼ºåŒ–è¾¹ç¼˜ï¼ˆé¿å…è¿‡åº¦é”åŒ–ï¼‰
        # 2. æ¸©å’Œå¢å¼ºå¯¹æ¯”åº¦ï¼ˆä¿æŒè‡ªç„¶å…‰å½±ï¼‰
        # 3. ä¿æŒæè´¨è´¨æ„Ÿ

        # é€‚åº¦è¾¹ç¼˜å¢å¼ºï¼ˆé¿å…è¿‡åº¦é”åŒ–ï¼‰
        if quality == "ultra_hd":
            # ä½¿ç”¨æ¸©å’Œçš„é”åŒ–å‚æ•°
            enhanced = enhanced.filter(ImageFilter.UnsharpMask(radius=1.2, percent=100, threshold=3))
        else:
            sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = sharpness_enhancer.enhance(1.15)  # é™ä½é”åŒ–å¼ºåº¦

        # æ¸©å’Œå¯¹æ¯”åº¦å¢å¼ºï¼ˆä¿æŒè‡ªç„¶å…‰å½±ï¼‰
        contrast_enhancer = ImageEnhance.Contrast(enhanced)
        enhanced = contrast_enhancer.enhance(1.08)  # é™ä½å¯¹æ¯”åº¦å¢å¼º

        # ä¿æŒè‡ªç„¶é¥±å’Œåº¦
        color_enhancer = ImageEnhance.Color(enhanced)
        enhanced = color_enhancer.enhance(1.03)  # æ›´æ¸©å’Œçš„é¥±å’Œåº¦

        # ä¿æŒè‡ªç„¶äº®åº¦
        brightness_enhancer = ImageEnhance.Brightness(enhanced)
        enhanced = brightness_enhancer.enhance(1.01)  # å¾®è°ƒå³å¯

        print(f"âœ… å»ºç­‘ä¸“ç”¨å¢å¼ºå®Œæˆ")
        return enhanced

    except Exception as e:
        print(f"âŒ å»ºç­‘å¢å¼ºå¤±è´¥: {e}")
        return None


def _apply_artwork_enhancement(image: Image.Image, quality: str) -> Optional[Image.Image]:
    """
    ğŸš€ è‰ºæœ¯ä½œå“ä¸“ç”¨å¢å¼º - ä¿æŒåŸæœ‰é£æ ¼
    """
    try:
        print(f"ğŸ¨ åº”ç”¨è‰ºæœ¯ä¸“ç”¨å¢å¼º...")
        from PIL import ImageEnhance, ImageFilter

        enhanced = image.copy()

        # ğŸš€ è‰ºæœ¯ä½œå“å¢å¼ºç­–ç•¥ï¼ˆä¿æŒåŸæœ‰é£æ ¼ï¼‰ï¼š
        # 1. æœ€å°åŒ–è‰²å½©æ”¹åŠ¨
        # 2. è½»å¾®ç»†èŠ‚å¢å¼º
        # 3. å®Œå…¨ä¿æŒè‰ºæœ¯é£æ ¼

        # éå¸¸æ¸©å’Œçš„è‰²å½©å¢å¼º
        color_enhancer = ImageEnhance.Color(enhanced)
        enhanced = color_enhancer.enhance(1.03)  # é™ä½åˆ°æœ€å°

        # è½»å¾®çš„å¯¹æ¯”åº¦å¢å¼º
        contrast_enhancer = ImageEnhance.Contrast(enhanced)
        enhanced = contrast_enhancer.enhance(1.04)  # é™ä½å¯¹æ¯”åº¦å¢å¼º

        # æœ€æ¸©å’Œçš„é”åŒ–
        if quality == "ultra_hd":
            # ä½¿ç”¨ææ¸©å’Œçš„é”åŒ–å‚æ•°
            enhanced = enhanced.filter(ImageFilter.UnsharpMask(radius=0.6, percent=60, threshold=8))
        else:
            sharpness_enhancer = ImageEnhance.Sharpness(enhanced)
            enhanced = sharpness_enhancer.enhance(1.06)  # ææ¸©å’Œé”åŒ–

        # ä¿æŒåŸæœ‰äº®åº¦
        brightness_enhancer = ImageEnhance.Brightness(enhanced)
        enhanced = brightness_enhancer.enhance(1.005)  # å‡ ä¹ä¸æ”¹å˜

        print(f"âœ… è‰ºæœ¯ä¸“ç”¨å¢å¼ºå®Œæˆ")
        return enhanced

    except Exception as e:
        print(f"âŒ è‰ºæœ¯å¢å¼ºå¤±è´¥: {e}")
        return None


def _apply_adaptive_traditional_enhancement(image: Image.Image, quality: str, image_type: str) -> Image.Image:
    """
    ğŸš€ è‡ªé€‚åº”ä¼ ç»Ÿå¢å¼ºï¼šæ ¹æ®å›¾åƒç±»å‹è°ƒæ•´å‚æ•°
    """
    try:
        print(f"ğŸ¨ åº”ç”¨è‡ªé€‚åº”ä¼ ç»Ÿå¢å¼º (ç±»å‹: {image_type}, è´¨é‡: {quality})...")

        # å¦‚æœå·²ç»åº”ç”¨äº†ä¸“ç”¨å¢å¼ºï¼Œä½¿ç”¨æ›´æ¸©å’Œçš„å‚æ•°
        if image_type in ["landscape", "architecture", "artwork"]:
            return _apply_gentle_traditional_enhancement(image, quality)
        else:
            # å¯¹äºäººåƒå’Œé€šç”¨å›¾åƒï¼Œä½¿ç”¨æ ‡å‡†å¢å¼º
            return _apply_advanced_traditional_enhancement(image, quality)

    except Exception as e:
        print(f"âŒ è‡ªé€‚åº”ä¼ ç»Ÿå¢å¼ºå¤±è´¥: {e}")
        return image


def _apply_gentle_traditional_enhancement(image: Image.Image, quality: str) -> Image.Image:
    """
    ğŸš€ ææ¸©å’Œçš„ä¼ ç»Ÿå¢å¼ºï¼šç”¨äºå·²ç»åº”ç”¨ä¸“ç”¨å¢å¼ºçš„å›¾åƒ
    """
    try:
        from PIL import ImageEnhance

        print(f"ğŸ¨ åº”ç”¨ææ¸©å’Œä¼ ç»Ÿå¢å¼º...")

        # ææ¸©å’Œçš„é”åŒ–
        enhancer = ImageEnhance.Sharpness(image)
        sharpness_factor = 1.02 if quality == "hd" else 1.04  # é™ä½åˆ°ææ¸©å’Œ
        image = enhancer.enhance(sharpness_factor)

        # æè½»å¾®çš„å¯¹æ¯”åº¦è°ƒæ•´
        enhancer = ImageEnhance.Contrast(image)
        contrast_factor = 1.015 if quality == "hd" else 1.025  # é™ä½åˆ°ææ¸©å’Œ
        image = enhancer.enhance(contrast_factor)

        # æå¾®è°ƒé¥±å’Œåº¦
        enhancer = ImageEnhance.Color(image)
        color_factor = 1.01 if quality == "hd" else 1.015  # é™ä½åˆ°ææ¸©å’Œ
        image = enhancer.enhance(color_factor)

        # äº®åº¦å‡ ä¹ä¸å˜
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.003)  # å‡ ä¹ä¸æ”¹å˜

        print(f"âœ… ææ¸©å’Œä¼ ç»Ÿå¢å¼ºå®Œæˆ")
        return image

    except Exception as e:
        print(f"âŒ æ¸©å’Œä¼ ç»Ÿå¢å¼ºå¤±è´¥: {e}")
        return image


def _check_and_correct_over_enhancement(original: Image.Image, enhanced: Image.Image) -> Image.Image:
    """
    ğŸš€ æ™ºèƒ½æ£€æµ‹è¿‡åº¦å¢å¼ºå¹¶è‡ªåŠ¨ä¿®æ­£
    """
    try:
        import numpy as np
        from PIL import ImageStat

        print(f"ğŸ” æ£€æµ‹è¿‡åº¦å¢å¼º...")

        # è½¬æ¢ä¸ºnumpyæ•°ç»„è¿›è¡Œåˆ†æ
        orig_array = np.array(original)
        enh_array = np.array(enhanced)

        # æ£€æµ‹è¿‡åº¦é¥±å’Œ
        orig_sat = np.std(orig_array)
        enh_sat = np.std(enhanced)
        saturation_increase = enh_sat / orig_sat if orig_sat > 0 else 1.0

        # æ£€æµ‹è¿‡åº¦å¯¹æ¯”åº¦
        orig_contrast = np.std(orig_array.astype(float))
        enh_contrast = np.std(enh_array.astype(float))
        contrast_increase = enh_contrast / orig_contrast if orig_contrast > 0 else 1.0

        # æ£€æµ‹è¿‡åº¦é”åŒ–ï¼ˆè¾¹ç¼˜æ£€æµ‹ï¼‰
        try:
            import cv2
            orig_gray = cv2.cvtColor(orig_array, cv2.COLOR_RGB2GRAY)
            enh_gray = cv2.cvtColor(enh_array, cv2.COLOR_RGB2GRAY)

            orig_edges = cv2.Canny(orig_gray, 50, 150)
            enh_edges = cv2.Canny(enh_gray, 50, 150)

            orig_edge_count = np.sum(orig_edges > 0)
            enh_edge_count = np.sum(enh_edges > 0)
            edge_increase = enh_edge_count / orig_edge_count if orig_edge_count > 0 else 1.0
        except:
            edge_increase = 1.0

        print(f"ğŸ” å¢å¼ºåˆ†æ: é¥±å’Œåº¦x{saturation_increase:.2f}, å¯¹æ¯”åº¦x{contrast_increase:.2f}, è¾¹ç¼˜x{edge_increase:.2f}")

        # åˆ¤æ–­æ˜¯å¦è¿‡åº¦å¢å¼º
        over_enhanced = False
        blend_ratio = 1.0  # 1.0 = å®Œå…¨ä½¿ç”¨å¢å¼ºå›¾åƒï¼Œ0.0 = å®Œå…¨ä½¿ç”¨åŸå›¾

        if saturation_increase > 1.15:  # é¥±å’Œåº¦å¢åŠ è¶…è¿‡15%
            over_enhanced = True
            blend_ratio *= 0.7
            print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦é¥±å’Œ")

        if contrast_increase > 1.2:  # å¯¹æ¯”åº¦å¢åŠ è¶…è¿‡20%
            over_enhanced = True
            blend_ratio *= 0.8
            print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦å¯¹æ¯”")

        if edge_increase > 1.5:  # è¾¹ç¼˜å¢åŠ è¶…è¿‡50%
            over_enhanced = True
            blend_ratio *= 0.6
            print(f"âš ï¸ æ£€æµ‹åˆ°è¿‡åº¦é”åŒ–")

        if over_enhanced:
            # æ··åˆåŸå›¾å’Œå¢å¼ºå›¾åƒä»¥å‡å°‘è¿‡åº¦å¢å¼º
            print(f"ğŸ”§ åº”ç”¨ä¿®æ­£æ··åˆ (æ¯”ä¾‹: {blend_ratio:.2f})")

            orig_array = orig_array.astype(float)
            enh_array = enh_array.astype(float)

            corrected_array = (enh_array * blend_ratio + orig_array * (1 - blend_ratio)).astype(np.uint8)
            corrected_image = Image.fromarray(corrected_array)

            print(f"âœ… è¿‡åº¦å¢å¼ºä¿®æ­£å®Œæˆ")
            return corrected_image
        else:
            print(f"âœ… å¢å¼ºæ•ˆæœè‡ªç„¶ï¼Œæ— éœ€ä¿®æ­£")
            return enhanced

    except Exception as e:
        print(f"âŒ è¿‡åº¦å¢å¼ºæ£€æµ‹å¤±è´¥: {e}")
        return enhanced


def _detect_faces_in_image(image: Image.Image) -> bool:
    """
    å¿«é€Ÿæ£€æµ‹å›¾åƒä¸­æ˜¯å¦æœ‰äººè„¸
    """
    faces = _detect_faces_with_locations(image)
    return len(faces) > 0


def _detect_faces_with_locations(image: Image.Image) -> list:
    """
    æ£€æµ‹å›¾åƒä¸­çš„äººè„¸å¹¶è¿”å›ä½ç½®ä¿¡æ¯
    è¿”å›æ ¼å¼: [(x, y, w, h), ...]
    """
    try:
        import cv2
        import numpy as np

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        img_array = np.array(image)
        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)
        gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

        # ä½¿ç”¨Haarçº§è”åˆ†ç±»å™¨æ£€æµ‹äººè„¸
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(30, 30))

        return faces.tolist() if len(faces) > 0 else []

    except Exception as e:
        print(f"âš ï¸ äººè„¸æ£€æµ‹å¤±è´¥: {e}")
        return []


def _enhance_face_regions_with_ai_upscale(image: Image.Image, faces: list) -> Optional[Image.Image]:
    """
    ğŸš€ ä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼ºäººè„¸åŒºåŸŸ
    è¿™æ˜¯ä¸€ä¸ªå®ç”¨çš„äººè„¸å¢å¼ºæ–¹æ¡ˆï¼Œåˆ©ç”¨ç°æœ‰çš„AIæ”¾å¤§æŠ€æœ¯
    """
    try:
        if not faces:
            return None

        print(f"ğŸ¯ ä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼º{len(faces)}ä¸ªäººè„¸åŒºåŸŸ...")

        # åˆ›å»ºå›¾åƒå‰¯æœ¬
        enhanced_image = image.copy()

        for i, (x, y, w, h) in enumerate(faces):
            try:
                # æ‰©å±•äººè„¸åŒºåŸŸï¼ˆåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼‰
                padding = max(w, h) // 4
                expanded_x = max(0, x - padding)
                expanded_y = max(0, y - padding)
                expanded_w = min(image.width - expanded_x, w + 2 * padding)
                expanded_h = min(image.height - expanded_y, h + 2 * padding)

                # æå–äººè„¸åŒºåŸŸ
                face_region = image.crop((expanded_x, expanded_y, expanded_x + expanded_w, expanded_y + expanded_h))

                # ä½¿ç”¨AIæ”¾å¤§æŠ€æœ¯å¢å¼ºäººè„¸åŒºåŸŸ
                # è®¡ç®—åˆé€‚çš„æ”¾å¤§å€æ•°
                face_size = max(expanded_w, expanded_h)
                if face_size < 128:
                    scale_factor = 2.0  # å°äººè„¸éœ€è¦æ›´å¤šå¢å¼º
                elif face_size < 256:
                    scale_factor = 1.5  # ä¸­ç­‰äººè„¸é€‚åº¦å¢å¼º
                else:
                    scale_factor = 1.2  # å¤§äººè„¸è½»å¾®å¢å¼º

                target_w = int(expanded_w * scale_factor)
                target_h = int(expanded_h * scale_factor)

                print(f"ğŸš€ å¢å¼ºäººè„¸{i+1}: {expanded_w}x{expanded_h} -> {target_w}x{target_h}")

                # ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§
                enhanced_face = smart_ai_upscale(face_region, target_w, target_h)

                if enhanced_face:
                    # ç¼©å›åŸå°ºå¯¸ï¼Œä¿ç•™å¢å¼ºæ•ˆæœ
                    enhanced_face_resized = enhanced_face.resize((expanded_w, expanded_h), Image.Resampling.LANCZOS)

                    # å°†å¢å¼ºåçš„äººè„¸åŒºåŸŸç²˜è´´å›åŸå›¾
                    enhanced_image.paste(enhanced_face_resized, (expanded_x, expanded_y))
                    print(f"âœ… äººè„¸{i+1}å¢å¼ºå®Œæˆ")
                else:
                    print(f"âš ï¸ äººè„¸{i+1}AIæ”¾å¤§å¤±è´¥ï¼Œè·³è¿‡")

            except Exception as e:
                print(f"âŒ äººè„¸{i+1}å¢å¼ºå¤±è´¥: {e}")
                continue

        print(f"âœ… æ‰€æœ‰äººè„¸åŒºåŸŸAIå¢å¼ºå®Œæˆ")
        return enhanced_image

    except Exception as e:
        print(f"âŒ AIäººè„¸åŒºåŸŸå¢å¼ºå¤±è´¥: {e}")
        return None


def _try_codeformer_restoration(image: Image.Image) -> Optional[Image.Image]:
    """
    å°è¯•ä½¿ç”¨CodeFormerè¿›è¡Œäººè„¸ä¿®å¤
    """
    try:
        # ğŸš€ å°è¯•ä½¿ç”¨ComfyUIçš„FaceRestoreèŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰ComfyUIçš„äººè„¸ä¿®å¤èŠ‚ç‚¹
            import comfy.model_management
            from comfy.utils import load_torch_file

            # æŸ¥æ‰¾CodeFormeræ¨¡å‹
            models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'models')
            possible_paths = [
                os.path.join(models_dir, 'facerestore_models', 'codeformer-v0.1.0.pth'),
                os.path.join(models_dir, 'face_restore', 'codeformer-v0.1.0.pth'),
                os.path.join(models_dir, 'upscale_models', 'codeformer-v0.1.0.pth'),
            ]

            codeformer_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    codeformer_path = path
                    break

            if codeformer_path:
                print(f"ğŸš€ æ‰¾åˆ°CodeFormeræ¨¡å‹: {codeformer_path}")
                # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„CodeFormerè°ƒç”¨
                # ç”±äºéœ€è¦ç‰¹å®šçš„ä¾èµ–å’Œå®ç°ï¼Œæš‚æ—¶è·³è¿‡
                print(f"ğŸ” CodeFormeré›†æˆéœ€è¦é¢å¤–ä¾èµ–ï¼Œè·³è¿‡")
                return None
            else:
                print(f"ğŸ” CodeFormeræ¨¡å‹æœªæ‰¾åˆ°")
                return None

        except ImportError:
            print(f"ğŸ” ComfyUIäººè„¸ä¿®å¤æ¨¡å—ä¸å¯ç”¨")
            return None

    except Exception as e:
        print(f"âŒ CodeFormerä¿®å¤å¤±è´¥: {e}")
        return None


def _try_gfpgan_restoration(image: Image.Image) -> Optional[Image.Image]:
    """
    å°è¯•ä½¿ç”¨GFPGANè¿›è¡Œäººè„¸ä¿®å¤
    """
    try:
        # ğŸš€ å°è¯•ä½¿ç”¨ComfyUIçš„FaceRestoreèŠ‚ç‚¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            import comfy.model_management
            from comfy.utils import load_torch_file

            # æŸ¥æ‰¾GFPGANæ¨¡å‹
            models_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))), 'models')
            possible_paths = [
                os.path.join(models_dir, 'facerestore_models', 'GFPGANv1.4.pth'),
                os.path.join(models_dir, 'face_restore', 'GFPGANv1.4.pth'),
                os.path.join(models_dir, 'upscale_models', 'GFPGANv1.4.pth'),
            ]

            gfpgan_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    gfpgan_path = path
                    break

            if gfpgan_path:
                print(f"ğŸš€ æ‰¾åˆ°GFPGANæ¨¡å‹: {gfpgan_path}")
                # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„GFPGANè°ƒç”¨
                # ç”±äºéœ€è¦ç‰¹å®šçš„ä¾èµ–å’Œå®ç°ï¼Œæš‚æ—¶è·³è¿‡
                print(f"ğŸ” GFPGANé›†æˆéœ€è¦é¢å¤–ä¾èµ–ï¼Œè·³è¿‡")
                return None
            else:
                print(f"ğŸ” GFPGANæ¨¡å‹æœªæ‰¾åˆ°")
                return None

        except ImportError:
            print(f"ğŸ” ComfyUIäººè„¸ä¿®å¤æ¨¡å—ä¸å¯ç”¨")
            return None

    except Exception as e:
        print(f"âŒ GFPGANä¿®å¤å¤±è´¥: {e}")
        return None


def _apply_advanced_traditional_enhancement(image: Image.Image, quality: str) -> Image.Image:
    """
    ğŸš€ è‡ªç„¶ä¼ ç»Ÿç”»è´¨å¢å¼ºç®—æ³• - ä¿æŒçœŸå®æ„Ÿ
    """
    try:
        from PIL import ImageEnhance, ImageFilter
        import numpy as np

        print(f"ğŸ¨ åº”ç”¨è‡ªç„¶ä¼ ç»Ÿå¢å¼ºç®—æ³•...")

        # ğŸš€ ç¬¬ä¸€æ­¥ï¼šè½»å¾®å»å™ªï¼ˆä»…åœ¨ultra_hdæ¨¡å¼ä¸‹ï¼‰
        if quality == "ultra_hd":
            # éå¸¸è½»å¾®çš„å»å™ª
            denoised = image.filter(ImageFilter.GaussianBlur(radius=0.3))
            # ä¸åŸå›¾æ··åˆï¼ˆæ›´ä¿å®ˆçš„æ¯”ä¾‹ï¼‰
            original_array = np.array(image)
            denoised_array = np.array(denoised)
            mixed_array = (original_array * 0.9 + denoised_array * 0.1).astype(np.uint8)
            image = Image.fromarray(mixed_array)
            print(f"âœ… è½»å¾®å»å™ªå®Œæˆ")

        # ğŸš€ ç¬¬äºŒæ­¥ï¼šè‡ªç„¶é”åŒ–ï¼ˆé™ä½å¼ºåº¦ï¼‰
        enhancer = ImageEnhance.Sharpness(image)
        if quality == "hd":
            sharpness_factor = 1.06  # éå¸¸æ¸©å’Œçš„é”åŒ–
        else:  # ultra_hd
            sharpness_factor = 1.1   # æ¸©å’Œé”åŒ–
        image = enhancer.enhance(sharpness_factor)
        print(f"âœ… è‡ªç„¶é”åŒ–å®Œæˆ (factor: {sharpness_factor})")

        # ğŸš€ ç¬¬ä¸‰æ­¥ï¼šæ¸©å’Œå¯¹æ¯”åº¦å¢å¼º
        enhancer = ImageEnhance.Contrast(image)
        if quality == "hd":
            contrast_factor = 1.04  # éå¸¸æ¸©å’Œ
        else:  # ultra_hd
            contrast_factor = 1.06  # æ¸©å’Œå¯¹æ¯”åº¦
        image = enhancer.enhance(contrast_factor)
        print(f"âœ… æ¸©å’Œå¯¹æ¯”åº¦å¢å¼ºå®Œæˆ (factor: {contrast_factor})")

        # ğŸš€ ç¬¬å››æ­¥ï¼šè‡ªç„¶è‰²å½©ä¼˜åŒ–
        enhancer = ImageEnhance.Color(image)
        if quality == "hd":
            color_factor = 1.02  # éå¸¸æ¸©å’Œçš„é¥±å’Œåº¦
        else:  # ultra_hd
            color_factor = 1.03  # æ¸©å’Œé¥±å’Œåº¦
        image = enhancer.enhance(color_factor)
        print(f"âœ… è‡ªç„¶è‰²å½©ä¼˜åŒ–å®Œæˆ (factor: {color_factor})")

        # ğŸš€ ç¬¬äº”æ­¥ï¼šäº®åº¦ä¿æŒï¼ˆå‡ ä¹ä¸å˜ï¼‰
        enhancer = ImageEnhance.Brightness(image)
        brightness_factor = 1.005  # å‡ ä¹ä¸æ”¹å˜äº®åº¦
        image = enhancer.enhance(brightness_factor)
        print(f"âœ… äº®åº¦ä¿æŒå®Œæˆ (factor: {brightness_factor})")

        print(f"ğŸ¨ è‡ªç„¶ä¼ ç»Ÿå¢å¼ºç®—æ³•å®Œæˆ")
        return image

    except Exception as e:
        print(f"âŒ ä¼ ç»Ÿå¢å¼ºå¤±è´¥: {e}")
        return image

def _apply_full_enhancements(base_image: Image.Image, target_size_str: str, quality: str, enhance_quality: bool, smart_resize_enabled: bool) -> Image.Image:
    """å°ºå¯¸é€‚é… + ä¸»ä½“æ£€æµ‹è£å‰ª + ç”»è´¨å¢å¼ºï¼Œä¾›nano-bananaå¤ç”¨"""
    print(f"ğŸ”§ å¼€å§‹å›¾åƒå¢å¼ºå¤„ç†...")
    print(f"ğŸ”§ è¾“å…¥å‚æ•°: target_size={target_size_str}, quality={quality}, enhance_quality={enhance_quality}, smart_resize={smart_resize_enabled}")
    print(f"ğŸ”§ åŸå§‹å›¾åƒå°ºå¯¸: {base_image.size}")

    image = base_image
    try:
        if 'x' in target_size_str:
            target_width, target_height = map(int, target_size_str.split('x'))
        else:
            target_width, target_height = image.size
        print(f"ğŸ”§ ç›®æ ‡å°ºå¯¸: {target_width}x{target_height}")
    except Exception as e:
        target_width, target_height = image.size
        print(f"âš ï¸ è§£æç›®æ ‡å°ºå¯¸å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å°ºå¯¸: {e}")

    try:
        if smart_resize_enabled and (image.size != (target_width, target_height)):
            print(f"ğŸ¯ å¯ç”¨æ™ºèƒ½è°ƒæ•´å°ºå¯¸...")

            # ğŸš€ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥AIæ¨¡å‹æ˜¯å¦å·²ç»ç”Ÿæˆäº†åˆé€‚çš„å°ºå¯¸
            current_aspect = image.size[0] / image.size[1]
            target_aspect = target_width / target_height
            aspect_diff = abs(current_aspect - target_aspect) / target_aspect

            print(f"ğŸ” å®½é«˜æ¯”åˆ†æ: å½“å‰={current_aspect:.3f}, ç›®æ ‡={target_aspect:.3f}, å·®å¼‚={aspect_diff:.3f}")

            # å¦‚æœå®½é«˜æ¯”å·®å¼‚å¾ˆå°ï¼ˆ<10%ï¼‰ï¼Œä½¿ç”¨æ¸©å’Œçš„è°ƒæ•´ç­–ç•¥
            if aspect_diff < 0.1:
                print(f"ğŸ¯ å®½é«˜æ¯”æ¥è¿‘ç›®æ ‡ï¼Œä½¿ç”¨æ¸©å’Œè°ƒæ•´ç­–ç•¥...")
                # ç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸ï¼Œä¸è¿›è¡Œæ¿€è¿›è£å‰ª
                image = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
                print(f"âœ… æ¸©å’Œè°ƒæ•´å®Œæˆ: {image.size}")
            elif image.size[0] / image.size[1] != target_width / target_height:
                print(f"ğŸ¯ æ£€æµ‹åˆ°å®½é«˜æ¯”ä¸åŒ¹é…ï¼Œå¯ç”¨æ™ºèƒ½ä¸»ä½“æ£€æµ‹...")
                scale_x = target_width / image.size[0]
                scale_y = target_height / image.size[1]
                scale = max(scale_x, scale_y)
                enlarged_w = max(1, int(image.size[0] * scale))
                enlarged_h = max(1, int(image.size[1] * scale))
                print(f"ğŸ¯ æ”¾å¤§åˆ°: {enlarged_w}x{enlarged_h}")

                try:
                    print(f"ğŸš€ å°è¯•AIæ™ºèƒ½æ”¾å¤§...")
                    ai_up = smart_ai_upscale(image, enlarged_w, enlarged_h)
                    enlarged = ai_up if ai_up is not None else image.resize((enlarged_w, enlarged_h), Image.Resampling.LANCZOS)
                    if ai_up is not None:
                        print(f"âœ… AIæ™ºèƒ½æ”¾å¤§æˆåŠŸ")
                    else:
                        print(f"âš ï¸ AIæ™ºèƒ½æ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ”¾å¤§")
                except Exception as e:
                    print(f"âŒ AIæ™ºèƒ½æ”¾å¤§å¼‚å¸¸: {e}")
                    enlarged = image.resize((enlarged_w, enlarged_h), Image.Resampling.LANCZOS)

                try:
                    print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½ä¸»ä½“æ£€æµ‹...")
                    subject_bbox, subject_center = detect_image_foreground_subject(enlarged)
                    cx, cy = subject_center
                    print(f"ğŸ¯ æ£€æµ‹åˆ°ä¸»ä½“ä¸­å¿ƒ: ({cx}, {cy})")

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼Œç¡®ä¿ä¸»ä½“ä¸ä¼šè¢«è£å‰ªæ‰
                    subject_x, subject_y, subject_w, subject_h = subject_bbox

                    # æ£€æŸ¥ä¸»ä½“æ˜¯å¦åˆç†ï¼ˆä¸èƒ½å¤ªå°æˆ–å¤ªå¤§ï¼‰
                    enlarged_area = enlarged.width * enlarged.height
                    subject_area = subject_w * subject_h
                    subject_ratio = subject_area / enlarged_area

                    print(f"ğŸ” ä¸»ä½“åŒºåŸŸæ£€æŸ¥: ä¸»ä½“({subject_x}, {subject_y}, {subject_w}x{subject_h}), å æ¯”{subject_ratio:.3f}")

                    # ğŸš€ å…³é”®ä¿®å¤ï¼šå¦‚æœä¸»ä½“å æ¯”è¿‡å¤§ï¼Œè¯´æ˜AIå·²ç»ç”Ÿæˆäº†åˆé€‚çš„æ„å›¾ï¼Œä½¿ç”¨æ¸©å’Œç­–ç•¥
                    if subject_ratio > 0.6:
                        print(f"ğŸ¯ ä¸»ä½“å æ¯”è¾ƒå¤§({subject_ratio:.3f})ï¼ŒAIæ„å›¾è‰¯å¥½ï¼Œä½¿ç”¨æ¸©å’Œè£å‰ªç­–ç•¥")
                        # ä½¿ç”¨å›¾åƒä¸­å¿ƒè¿›è¡Œæ¸©å’Œè£å‰ª
                        crop_x = max(0, (enlarged.width - target_width) // 2)
                        crop_y = max(0, (enlarged.height - target_height) // 2)
                        image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                        print(f"âœ… æ¸©å’Œè£å‰ªå®Œæˆ: ({crop_x}, {crop_y})")
                    elif subject_ratio < 0.01:
                        print(f"âš ï¸ ä¸»ä½“å æ¯”è¿‡å°({subject_ratio:.3f})ï¼Œæ£€æµ‹å¯èƒ½æœ‰è¯¯ï¼Œä½¿ç”¨ä¸­å¿ƒè£å‰ª")
                        # ä½¿ç”¨å›¾åƒä¸­å¿ƒä½œä¸ºè£å‰ªä¸­å¿ƒ
                        crop_x = max(0, (enlarged.width - target_width) // 2)
                        crop_y = max(0, (enlarged.height - target_height) // 2)
                        image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                        print(f"âš ï¸ ä½¿ç”¨ä¸­å¿ƒè£å‰ª: ({crop_x}, {crop_y})")
                    else:
                        # æ­£å¸¸çš„æ™ºèƒ½ä¸»ä½“å±…ä¸­è£å‰ª
                        cx, cy = subject_center

                        # è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“åœ¨è£å‰ªåŒºåŸŸå†…
                        crop_x = int(cx - target_width / 2)
                        crop_y = int(cy - target_height / 2)

                        # ğŸš€ å…³é”®ä¿®å¤ï¼šç¡®ä¿è£å‰ªåŒºåŸŸåŒ…å«ä¸»ä½“
                        # æ£€æŸ¥ä¸»ä½“æ˜¯å¦ä¼šè¢«è£å‰ªæ‰
                        crop_right = crop_x + target_width
                        crop_bottom = crop_y + target_height
                        subject_right = subject_x + subject_w
                        subject_bottom = subject_y + subject_h

                        # å¦‚æœä¸»ä½“ä¼šè¢«è£å‰ªæ‰ï¼Œè°ƒæ•´è£å‰ªä½ç½®
                        if subject_x < crop_x:  # ä¸»ä½“å·¦è¾¹è¢«è£å‰ª
                            crop_x = max(0, subject_x - target_width // 10)  # ç•™10%è¾¹è·
                        if subject_y < crop_y:  # ä¸»ä½“ä¸Šè¾¹è¢«è£å‰ª
                            crop_y = max(0, subject_y - target_height // 10)
                        if subject_right > crop_right:  # ä¸»ä½“å³è¾¹è¢«è£å‰ª
                            crop_x = min(enlarged.width - target_width, subject_right - target_width + target_width // 10)
                        if subject_bottom > crop_bottom:  # ä¸»ä½“ä¸‹è¾¹è¢«è£å‰ª
                            crop_y = min(enlarged.height - target_height, subject_bottom - target_height + target_height // 10)

                        # æœ€ç»ˆè¾¹ç•Œæ£€æŸ¥
                        crop_x = max(0, min(crop_x, enlarged.width - target_width))
                        crop_y = max(0, min(crop_y, enlarged.height - target_height))

                        print(f"ğŸ¯ æ™ºèƒ½è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
                        print(f"ğŸ” ä¸»ä½“ä¿æŠ¤æ£€æŸ¥: ä¸»ä½“({subject_x}, {subject_y}) -> ({subject_right}, {subject_bottom})")

                        image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                        print(f"âœ… æ™ºèƒ½ä¸»ä½“å±…ä¸­è£å‰ªå®Œæˆ")

                except Exception as e:
                    print(f"âŒ æ™ºèƒ½ä¸»ä½“æ£€æµ‹å¤±è´¥ï¼Œä½¿ç”¨å®‰å…¨ä¸­å¿ƒè£å‰ª: {e}")
                    import traceback
                    print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    # ğŸš€ å…³é”®ä¿®å¤ï¼šä½¿ç”¨æ›´å®‰å…¨çš„ä¸­å¿ƒè£å‰ª
                    crop_x = max(0, (enlarged.width - target_width) // 2)
                    crop_y = max(0, (enlarged.height - target_height) // 2)
                    image = enlarged.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                    print(f"âš ï¸ ä½¿ç”¨å®‰å…¨ä¸­å¿ƒè£å‰ª: ({crop_x}, {crop_y})")
            else:
                print(f"ğŸ¯ å®½é«˜æ¯”åŒ¹é…ï¼Œç›´æ¥ç¼©æ”¾")
                image = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        else:
            if not smart_resize_enabled:
                print(f"ğŸ”§ æ™ºèƒ½è°ƒæ•´å·²ç¦ç”¨ï¼Œä½¿ç”¨å¡«å……æ¨¡å¼")
            else:
                print(f"ğŸ”§ å°ºå¯¸å·²åŒ¹é…ï¼Œä½¿ç”¨å¡«å……æ¨¡å¼")
            image = smart_resize_with_padding(image, (target_width, target_height))
    except Exception as e:
        print(f"âŒ å°ºå¯¸è°ƒæ•´å¤±è´¥: {e}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

    try:
        if enhance_quality and quality in ['hd', 'ultra_hd']:
            print(f"ğŸ¨ å¼€å§‹ç”»è´¨å¢å¼º (è´¨é‡çº§åˆ«: {quality})...")
            enhanced = enhance_image_quality(image, quality, "disabled")
            if enhanced:
                image = enhanced
                print(f"âœ… ç”»è´¨å¢å¼ºå®Œæˆ")
            else:
                print(f"âš ï¸ ç”»è´¨å¢å¼ºè¿”å›None")
        else:
            print(f"ğŸ”§ è·³è¿‡ç”»è´¨å¢å¼º (enhance_quality={enhance_quality}, quality={quality})")
    except Exception as e:
        print(f"âŒ ç”»è´¨å¢å¼ºå¤±è´¥: {e}")

    print(f"ğŸ”§ å›¾åƒå¢å¼ºå¤„ç†å®Œæˆï¼Œæœ€ç»ˆå°ºå¯¸: {image.size}")
    return image

def image_to_base64(image, format='JPEG'):
    """Convert PIL Image to base64 string"""
    buffer = io.BytesIO()
    if format.upper() == 'JPEG' and image.mode in ('RGBA', 'LA', 'P'):
        background = Image.new('RGB', image.size, (255, 255, 255))
        if image.mode == 'P':
            image = image.convert('RGBA')
        background.paste(image, mask=image.split()[-1] if image.mode in ('RGBA', 'LA') else None)
        image = background
    image.save(buffer, format=format, quality=95)
    return base64.b64encode(buffer.getvalue()).decode('utf-8')

def _make_chat_summary(response_text: str) -> str:
    try:
        import re
        url_pattern = r'https?://\S+\.(?:jpg|jpeg|png|gif|webp)'
        m = re.search(url_pattern, response_text or '')
        if m:
            return f"å·²ç”Ÿæˆå›¾åƒ: {m.group(0)}"
        # markdown image
        md_pat = r"!\[.*?\]\((.*?)\)"
        m2 = re.search(md_pat, response_text or '')
        if m2:
            return f"å·²ç”Ÿæˆå›¾åƒ: {m2.group(1)}"
        # base64
        if 'data:image/' in (response_text or ''):
            return "å·²ç”Ÿæˆå›¾åƒï¼ˆå†…åµŒbase64ï¼‰"
    except Exception:
        pass
    return "å·²ç”Ÿæˆå›¾åƒ"

def validate_api_key(api_key):
    """Validate API key format"""
    return api_key and len(api_key.strip()) > 10

def format_error_message(error):
    """Format error message"""
    return str(error)

def generate_with_official_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None):
    """ä½¿ç”¨å®˜æ–¹google.genaiåº“è°ƒç”¨API"""
    try:
        # å°è¯•å¯¼å…¥å®˜æ–¹åº“
        from google import genai
        from google.genai import types

        print(f"ğŸš€ ä½¿ç”¨å®˜æ–¹google.genaiåº“è°ƒç”¨æ¨¡å‹: {model}")

        # åˆ›å»ºå®¢æˆ·ç«¯
        client = genai.Client(api_key=api_key)

        # è½¬æ¢generation_configæ ¼å¼
        official_config = types.GenerateContentConfig(
            temperature=generation_config.get('temperature', 0.7),
            top_p=generation_config.get('topP', 0.95),
            top_k=generation_config.get('topK', 40),
            max_output_tokens=generation_config.get('maxOutputTokens', 8192),
            response_modalities=['Text', 'Image'] if 'IMAGE' in generation_config.get('responseModalities', []) else ['Text']
        )

        # è½¬æ¢content_partsæ ¼å¼ï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ï¼Œä¸gemini_bananaæ¨¡å—ä¿æŒä¸€è‡´ï¼‰
        official_parts = []
        for part in content_parts:
            if 'text' in part:
                official_parts.append({"text": part['text']})
            elif 'inline_data' in part:
                official_parts.append({
                    "inline_data": {
                        "mime_type": part['inline_data']['mime_type'],
                        "data": part['inline_data']['data']
                    }
                })

        # è°ƒç”¨APIï¼ˆä½¿ç”¨å­—å…¸æ ¼å¼ï¼‰
        response = client.models.generate_content(
            model=model,
            contents=[{"parts": official_parts}],
            config=official_config
        )

        # è½¬æ¢å“åº”æ ¼å¼ä¸ºREST APIå…¼å®¹æ ¼å¼
        result = {
            "candidates": [{
                "content": {
                    "parts": []
                }
            }]
        }

        # å¤„ç†å“åº”å†…å®¹
        if hasattr(response, 'candidates') and response.candidates:
            for candidate in response.candidates:
                if hasattr(candidate, 'content') and candidate.content and hasattr(candidate.content, 'parts'):
                    for part in candidate.content.parts:
                        # å¤„ç†æ–‡æœ¬å†…å®¹
                        if hasattr(part, 'text') and part.text:
                            result["candidates"][0]["content"]["parts"].append({
                                "text": part.text
                            })
                        # å¤„ç†å›¾åƒå†…å®¹
                        elif hasattr(part, 'inline_data') and part.inline_data:
                            try:
                                # ç¡®ä¿inline_dataä¸ä¸ºNoneä¸”æœ‰å¿…è¦çš„å±æ€§
                                if hasattr(part.inline_data, 'mime_type') and hasattr(part.inline_data, 'data'):
                                    # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
                                    data = part.inline_data.data
                                    if hasattr(data, 'decode'):
                                        # å¦‚æœæ˜¯bytesï¼Œè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²
                                        import base64
                                        data = base64.b64encode(data).decode('utf-8')
                                    elif isinstance(data, str):
                                        # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                                        data = data

                                    result["candidates"][0]["content"]["parts"].append({
                                        "inline_data": {
                                            "mime_type": part.inline_data.mime_type,
                                            "data": data
                                        }
                                    })
                            except Exception as e:
                                print(f"âš ï¸ å¤„ç†å›¾åƒå“åº”æ—¶å‡ºé”™: {e}")

        # å¦‚æœæ²¡æœ‰ä»candidatesä¸­è·å–åˆ°å†…å®¹ï¼Œå°è¯•ä½¿ç”¨response.text
        if not result["candidates"][0]["content"]["parts"] and hasattr(response, 'text') and response.text:
            result["candidates"][0]["content"]["parts"].append({
                "text": response.text
            })

        print("âœ… å®˜æ–¹APIè°ƒç”¨æˆåŠŸ")
        return result

    except ImportError:
        print("âš ï¸ google.genaiåº“æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨å®˜æ–¹API")
        return None
    except Exception as e:
        print(f"âŒ å®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
        # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return None

def generate_with_rest_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None, base_url=None):
    """ä½¿ç”¨REST APIè°ƒç”¨Gemini"""
    import requests

    if not base_url:
        base_url = "https://generativelanguage.googleapis.com"

    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "contents": [{
            "parts": content_parts
        }],
        "generationConfig": generation_config
    }

    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key.strip()
    }

    # è®¾ç½®ä»£ç†
    proxies = None
    if proxy and proxy.strip() and "None" not in proxy:
        proxies = {
            'http': proxy.strip(),
            'https': proxy.strip()
        }

    for attempt in range(max_retries):
        try:
            print(f"ğŸŒ REST APIè°ƒç”¨ (å°è¯• {attempt + 1}/{max_retries})")

            response = requests.post(
                f"{base_url}/v1beta/models/{model}:generateContent",
                headers=headers,
                json=request_data,
                timeout=120,
                proxies=proxies
            )

            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ REST APIé”™è¯¯ {response.status_code}: {response.text}")
                if attempt == max_retries - 1:
                    raise Exception(f"REST APIè°ƒç”¨å¤±è´¥: {response.status_code}")

        except Exception as e:
            print(f"âŒ REST APIè°ƒç”¨å¼‚å¸¸: {e}")
            if attempt == max_retries - 1:
                raise e

        # é‡è¯•å»¶è¿Ÿ
        import time
        time.sleep(2 ** attempt)

    return None

def generate_with_priority_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None, base_url=None):
    """ä¼˜å…ˆä½¿ç”¨å®˜æ–¹APIï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST API"""

    # é¦–å…ˆå°è¯•å®˜æ–¹API
    print("ğŸ¯ ä¼˜å…ˆå°è¯•å®˜æ–¹google.genai API")
    result = generate_with_official_api(api_key, model, content_parts, generation_config, max_retries, proxy)

    if result is not None:
        print("âœ… å®˜æ–¹APIè°ƒç”¨æˆåŠŸ")
        return result

    # å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°REST API
    print("ğŸ”„ å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°REST API")
    return generate_with_rest_api(api_key, model, content_parts, generation_config, max_retries, proxy, base_url)

def extract_text_from_response(response_json):
    """ä»å“åº”ä¸­æå–æ–‡æœ¬å†…å®¹"""
    try:
        if not response_json or "candidates" not in response_json:
            return "Response received but no candidates"

        candidates = response_json["candidates"]
        if not candidates:
            return "Response received but no candidates"

        candidate = candidates[0]
        if "content" not in candidate:
            return "Response received but no content"

        content = candidate["content"]
        if "parts" not in content:
            return "Response received but no parts"

        parts = content["parts"]
        text_parts = []

        for part in parts:
            if "text" in part:
                text_parts.append(part["text"])

        if text_parts:
            return "\n".join(text_parts)
        else:
            return "Response received but no text content"

    except Exception as e:
        print(f"âŒ æå–æ–‡æœ¬å“åº”å¤±è´¥: {e}")
        return f"Error extracting text: {str(e)}"

def process_generated_image_from_response(response_json):
    """ä»å“åº”ä¸­æå–ç”Ÿæˆçš„å›¾åƒ"""
    try:
        if not response_json or "candidates" not in response_json:
            return None

        candidates = response_json["candidates"]
        if not candidates:
            return None

        candidate = candidates[0]
        if "content" not in candidate:
            return None

        content = candidate["content"]
        if "parts" not in content:
            return None

        parts = content["parts"]

        for part in parts:
            if "inline_data" in part:
                inline_data = part["inline_data"]
                if "data" in inline_data:
                    # è§£ç base64å›¾åƒæ•°æ®
                    import base64
                    import io
                    from PIL import Image

                    image_data = inline_data["data"]
                    image_bytes = base64.b64decode(image_data)
                    image = Image.open(io.BytesIO(image_bytes))
                    print("âœ… æˆåŠŸæå–ç”Ÿæˆçš„å›¾åƒ")
                    return image

        return None

    except Exception as e:
        print(f"âŒ æå–å›¾åƒå¤±è´¥: {e}")
        return None

def _normalize_model_name(model: str) -> str:
    """Strip any trailing bracketed labels (e.g., ' [OpenRouter]', ' [Comflyâ€‘T8]'), robust to hyphen variants."""
    try:
        if not model:
            return model
        import re
        # Remove any ' [....]' suffix at end, including unicode hyphen variants inside
        return re.sub(r"\s*\[[^\]]+\]\s*$", "", model)
    except Exception:
        return model

def _is_comfly_base(url: str) -> bool:
    try:
        return isinstance(url, str) and ("ai.comfly.chat" in url or "comfly.chat" in url)
    except Exception:
        return False

def _parse_size_str(size_str: str, default: str = "1024x1024") -> str:
    try:
        s = size_str.strip().lower()
        if "x" in s:
            w, h = s.split("x", 1)
            w = int(w); h = int(h)
            return f"{w}x{h}"
    except Exception:
        pass
    return default

def _comfly_nano_banana_generate(api_url: str, api_key: str, model: str, prompt: str, size: str, temperature: float = 1.0, top_p: float = 0.95, max_tokens: int = 32768, seed: int = 0) -> dict:
    """Call images/generations endpoint for nano-banana image generation (supports both Comfly and T8 mirror sites)."""
    import requests, json, os, re, time

    # è°ƒè¯•ä¿¡æ¯ - å·²å…³é—­
    # print(f"[DEBUG] _comfly_nano_banana_generate called with:")
    # print(f"[DEBUG]   api_url: {api_url}")
    # print(f"[DEBUG]   model: {model}")
    # print(f"[DEBUG]   prompt: {prompt[:100]}...")
    # print(f"[DEBUG]   size: {size}")
    # print(f"[DEBUG]   seed: {seed}")
    # å¯¹äºå›¾åƒç”Ÿæˆï¼Œä½¿ç”¨ /v1/images/generations ç«¯ç‚¹
    if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
        # T8é•œåƒç«™çš„æ­£ç¡®URL
        url = "https://ai.t8star.cn/v1/images/generations"
    elif api_url.endswith('/v1/chat/completions'):
        url = api_url.replace('/v1/chat/completions', '/v1/images/generations')
    else:
        # Comflyæˆ–å…¶ä»–å…¼å®¹çš„é•œåƒç«™
        url = "https://ai.comfly.chat/v1/images/generations"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # æ„å»ºå›¾åƒç”Ÿæˆçš„è¯·æ±‚æ ¼å¼ï¼ˆå®Œå…¨æŒ‰ç…§Comflyé¡¹ç›®çš„æ ¼å¼ï¼‰
    payload = {
        "prompt": prompt,
        "model": str(model)
    }

    # æ·»åŠ å¯é€‰å‚æ•°ï¼ˆæŒ‰ç…§Comflyé¡¹ç›®çš„æ–¹å¼ï¼‰
    if size:
        # å¤„ç†ç‰¹æ®Šå°ºå¯¸æ ¼å¼ï¼ŒT8é•œåƒç«™éœ€è¦å…·ä½“çš„å°ºå¯¸æ ¼å¼
        if size == "Original size":
            # é»˜è®¤ä½¿ç”¨1024x1024ä½œä¸ºåŸå§‹å°ºå¯¸
            payload["size"] = "1024x1024"
            # print(f"[DEBUG] è½¬æ¢ 'Original size' -> '1024x1024'")
        else:
            payload["size"] = size

    # ä½¿ç”¨urlæ ¼å¼è€Œä¸æ˜¯b64_jsonï¼ˆæ ¹æ®ç”¨æˆ·åé¦ˆï¼‰
    payload["response_format"] = "url"

    if seed > 0:
        payload["seed"] = seed

    # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°å®é™…çš„è¯·æ±‚payload - å·²å…³é—­
    # print(f"[DEBUG] Request payload: {json.dumps(payload, indent=2)}")
    # print(f"[DEBUG] Target URL: {url}")

    # å‘é€å›¾åƒç”Ÿæˆè¯·æ±‚ï¼ˆéæµå¼ï¼‰
    session = requests.Session()
    max_retries = 1
    backoff_seconds = 1

    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            try:
                proxy = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy") or os.environ.get("HTTP_PROXY") or os.environ.get("http_proxy")
                if proxy:
                    print(f"Use Proxy: {proxy}")
            except Exception:
                pass

            print(f"[ComflyNanoBananaMirror] (generate) POST attempt {attempt}/{max_retries} â†’ {url}")

            response = session.post(
                url,
                headers=headers,
                json=payload,
                timeout=(20, 120),
                allow_redirects=True,
            )
            print(f"[ComflyNanoBananaMirror] HTTP status: {response.status_code}")
            try:
                print(f"[ComflyNanoBananaMirror] Content-Type: {response.headers.get('Content-Type','')}")
            except Exception:
                pass
            if response.status_code != 200:
                try:
                    print(f"[ComflyNanoBananaMirror] Error body: {response.text[:500]}")
                except Exception:
                    pass
            response.raise_for_status()

            # å¤„ç†å›¾åƒç”ŸæˆAPIçš„JSONå“åº”
            try:
                result = response.json()
                print(f"[ComflyNanoBananaMirror] Response received: {list(result.keys())}")

                # æ£€æŸ¥æ˜¯å¦æœ‰å›¾åƒæ•°æ®
                if 'data' in result and result['data']:
                    # OpenAI Images APIæ ¼å¼
                    image_data = result['data'][0]

                    # ä¼˜å…ˆå¤„ç†URLæ ¼å¼ï¼ˆæ ¹æ®ç”¨æˆ·åé¦ˆï¼‰
                    if 'url' in image_data:
                        image_url = image_data['url']
                        # print(f"[DEBUG] ç”Ÿæˆçš„å›¾åƒURL: {image_url}")

                        # ä¸‹è½½å›¾åƒå¹¶è½¬æ¢ä¸ºbase64
                        import base64
                        from io import BytesIO
                        img_response = session.get(image_url, timeout=30)
                        img_response.raise_for_status()
                        b64_data = base64.b64encode(img_response.content).decode('utf-8')

                        response_text = f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œæ¨¡å‹: {model}\nå›¾åƒURL: {image_url}"
                        # print(f"[DEBUG] å“åº”æ–‡æœ¬: {response_text}")

                        return {
                            "data": [{
                                "b64_json": b64_data,
                                "url": image_url,
                                "revised_prompt": image_data.get('revised_prompt', prompt)
                            }],
                            "response_text": response_text
                        }
                    elif 'b64_json' in image_data:
                        # å¤„ç†base64æ•°æ®ï¼Œå¯èƒ½åŒ…å«data URLå‰ç¼€
                        b64_data = image_data['b64_json']
                        if b64_data.startswith('data:image/'):
                            # æå–çº¯base64éƒ¨åˆ†
                            b64_data = b64_data.split(',', 1)[1] if ',' in b64_data else b64_data

                        return {
                            "data": [{
                                "b64_json": b64_data,
                                "url": "",
                                "revised_prompt": image_data.get('revised_prompt', prompt)
                            }],
                            "response_text": f"å›¾åƒç”ŸæˆæˆåŠŸï¼Œæ¨¡å‹: {model}"
                        }

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾åƒæ•°æ®ï¼Œè¿”å›ç©ºç»“æœ
                print(f"[ComflyNanoBananaMirror] No image data found in response: {result}")
                return {
                    "data": [],
                    "response_text": f"å›¾åƒç”Ÿæˆå¤±è´¥ï¼Œå“åº”: {str(result)[:200]}"
                }

            except json.JSONDecodeError as e:
                print(f"[ComflyNanoBananaMirror] JSON decode error: {e}")
                return {
                    "data": [],
                    "response_text": f"å“åº”è§£æå¤±è´¥: {response.text[:200]}"
                }

        except requests.exceptions.HTTPError as http_err:
            status = getattr(http_err.response, 'status_code', None)
            last_error = http_err
            if status in (408, 409, 429) or (status is not None and 500 <= status < 600):
                if attempt < max_retries:
                    time.sleep(backoff_seconds * attempt)
                    continue
            raise Exception(f"Error in nano-banana generation: {str(http_err)}")
        except requests.exceptions.Timeout as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception("Error in nano-banana generation: request timed out")
        except Exception as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception(f"Error in nano-banana generation: {str(e)}")

    raise Exception(f"Error in nano-banana generation: {str(last_error)}")

def _comfly_nano_banana_edit(api_url: str, api_key: str, model: str, prompt: str, pil_images: list, size: str, temperature: float = 1.0, top_p: float = 0.95, max_tokens: int = 32768, seed: int = 0) -> dict:
    """Call chat completions endpoint for nano-banana image editing with multiple images (supports both Comfly and T8 mirror sites)."""
    import requests, json, io, base64, re, time, os
    # ä½¿ç”¨ä¼ å…¥çš„api_urlï¼Œå¦‚æœå·²ç»æ˜¯å®Œæ•´URLåˆ™ç›´æ¥ä½¿ç”¨ï¼Œå¦åˆ™æ„å»ºå®Œæ•´URL
    if api_url.endswith('/v1/chat/completions'):
        url = api_url
    elif "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
        url = f"{api_url.rstrip('/')}/v1/chat/completions"
    else:
        # Comflyæˆ–å…¶ä»–å…¼å®¹çš„é•œåƒç«™
        url = "https://ai.comfly.chat/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    # æ„å»ºcontentæ•°ç»„ï¼ŒåŒ…å«æ–‡æœ¬å’Œæ‰€æœ‰å›¾åƒ
    content = [{"type": "text", "text": prompt}]
    
    # æ·»åŠ æ‰€æœ‰è¾“å…¥å›¾åƒï¼ˆä½¿ç”¨åŸå›¾å°ºå¯¸ä¸æ— æŸPNGç¼–ç ï¼‰
    for pil_image in pil_images:
        if pil_image is not None:
            try:
                w, h = pil_image.size
                print(f"[ComflyNanoBananaMirror] Use original image size: {w}x{h}")
                buffered = io.BytesIO()
                pil_image.save(buffered, format="PNG")
                image_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
                content.append({
                    "type": "image_url",
                    "image_url": {"url": f"data:image/png;base64,{image_base64}"}
                })
            except Exception as e:
                try:
                    print(f"[ComflyNanoBananaMirror] Image encode error: {str(e)}")
                except Exception:
                    pass
    
    # æ„å»ºnano-bananaçš„å›¾åƒç¼–è¾‘è¯·æ±‚æ ¼å¼
    messages = [{
        "role": "user",
        "content": content
    }]
    
    payload = {
        "model": str(model),
        "messages": messages,
        "temperature": float(temperature),
        "top_p": float(top_p),
        "max_tokens": int(max_tokens),
        "stream": True
    }
    
    if seed > 0:
        payload["seed"] = seed
    
    # è°ƒè¯•ä¿¡æ¯
    try:
        print(f"[ComflyNanoBananaMirror] Building payload: model={model}, max_tokens={max_tokens}, seed={seed}")
        print(f"[ComflyNanoBananaMirror] Payload keys: {list(payload.keys())}")
        print(f"[ComflyNanoBananaMirror] Messages structure: {payload.get('messages', 'MISSING')}")
    except Exception:
        pass

    # å‘é€æµå¼è¯·æ±‚ï¼ˆå¯¹408ç­‰å¯é‡è¯•é”™è¯¯è¿›è¡Œé‡è¯•ï¼‰
    full_response = ""
    session = requests.Session()
    max_retries = 3
    backoff_seconds = 2

    last_error = None
    for attempt in range(1, max_retries + 1):
        try:
            # æ‰“å°ä»£ç†ä¿¡æ¯ï¼ˆè‹¥å­˜åœ¨ï¼‰
            try:
                proxy = os.environ.get("HTTPS_PROXY") or os.environ.get("https_proxy") or os.environ.get("HTTP_PROXY") or os.environ.get("http_proxy")
                if proxy:
                    print(f"Use Proxy: {proxy}")
            except Exception:
                pass

            print(f"[ComflyNanoBananaMirror] POST attempt {attempt}/{max_retries} â†’ {url}")
            response = session.post(
                url,
                headers=headers,
                json=payload,
                stream=True,
                timeout=(20, 120),  # è¿æ¥/è¯»å–è¶…æ—¶åˆ†ç¦»
                allow_redirects=True,
            )
            print(f"[ComflyNanoBananaMirror] HTTP status: {response.status_code}")
            try:
                print(f"[ComflyNanoBananaMirror] Content-Type: {response.headers.get('Content-Type','')}")
            except Exception:
                pass
            # è‹¥é200ï¼Œæ‰“å°è¿”å›ä½“å¸®åŠ©å®šä½422åŸå› 
            if response.status_code != 200:
                try:
                    print(f"[ComflyNanoBananaMirror] Error body: {response.text[:500]}")
                except Exception:
                    pass
            response.raise_for_status()

            line_count = 0
            received_any = False
            ctype = (response.headers.get('Content-Type') or '').lower()
            if 'text/event-stream' in ctype or 'stream' in ctype:
                for line in response.iter_lines(chunk_size=1, decode_unicode=False):
                    if line is None or not line:
                        continue
                    received_any = True
                    try:
                        line_text = line.decode('utf-8', errors='ignore').strip()
                    except Exception:
                        line_text = str(line).strip()
                    line_count += 1
                    if not line_text.startswith('data: '):
                        continue
                    data = line_text[6:]
                    if data == '[DONE]':
                        print(f"[ComflyNanoBananaMirror] Stream finished. Total SSE lines: {line_count}")
                        break
                    try:
                        chunk = json.loads(data)
                    except json.JSONDecodeError:
                        continue

                    if 'choices' in chunk and chunk['choices']:
                        choice0 = chunk['choices'][0]
                        delta = choice0.get('delta', {})
                        piece = delta.get('content') or (choice0.get('message', {}) or {}).get('content')
                        if piece:
                            full_response += piece
                            try:
                                print(f"[ComflyNanoBananaMirror] +{len(piece)} chars (total {len(full_response)})")
                            except Exception:
                                pass
            else:
                # éæµå¼ï¼šç›´æ¥è§£æä¸€æ¬¡æ€§JSON
                try:
                    body = response.content.decode('utf-8', errors='ignore')
                except Exception:
                    body = response.text
                try:
                    obj = json.loads(body)
                    received_any = True
                    if isinstance(obj, dict):
                        choices = obj.get('choices') or []
                        if choices:
                            msg = choices[0].get('message', {})
                            content_text = isinstance(msg, dict) and msg.get('content')
                            if isinstance(content_text, str):
                                full_response += content_text
                except Exception:
                    pass

            if not received_any:
                print("[ComflyNanoBananaMirror] Warning: no bytes received from stream; fallback to non-streaming request")
                # Fallback once with non-streaming
                try:
                    resp2 = session.post(
                        url,
                        headers=headers,
                        json={**payload, "stream": False},
                        timeout=(20, 120),
                        allow_redirects=True,
                    )
                    if resp2.status_code != 200:
                        try:
                            print(f"[ComflyNanoBananaMirror] Fallback error body: {resp2.text[:500]}")
                        except Exception:
                            pass
                    resp2.raise_for_status()
                    obj = resp2.json()
                    choices = obj.get('choices') or []
                    if choices:
                        msg = choices[0].get('message', {})
                        content_text = isinstance(msg, dict) and msg.get('content')
                        if isinstance(content_text, str):
                            full_response += content_text
                except Exception as _:
                    pass

            # ä¼˜å…ˆæå–base64å›¾ç‰‡
            base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
            matches = re.findall(base64_pattern, full_response)
            if matches:
                return {
                    "data": [{
                        "b64_json": matches[0],
                        "url": "",
                        "revised_prompt": prompt
                    }],
                    "response_text": full_response
                }

            # å…¶æ¬¡æå–å›¾ç‰‡URL
            image_md_pattern = r'!\[.*?\]\((.*?)\)'
            url_matches = re.findall(image_md_pattern, full_response)
            if not url_matches:
                url_pattern = r'https?://\S+\.(?:jpg|jpeg|png|gif|webp)'
                url_matches = re.findall(url_pattern, full_response)
            if not url_matches:
                all_urls_pattern = r'https?://\S+'
                url_matches = re.findall(all_urls_pattern, full_response)

            if url_matches:
                image_url = url_matches[0]
                try:
                    print(f"[ComflyNanoBananaMirror] Found image URL: {image_url}")
                except Exception:
                    pass
                try:
                    img_response = requests.get(image_url, timeout=30)
                    img_response.raise_for_status()
                    # åå¤„ç†ï¼šæŒ‰ size ç»Ÿä¸€é€‚é…è¾“å‡ºå°ºå¯¸ï¼ˆä¿éšœå°ºå¯¸æ§åˆ¶å¿…ç”Ÿæ•ˆï¼‰
                    try:
                        from io import BytesIO
                        from PIL import Image as _PILImage
                        raw_img = _PILImage.open(BytesIO(img_response.content)).convert('RGB')
                        target_size = size if isinstance(size, str) else str(size)
                        if 'x' in target_size:
                            tw, th = map(int, target_size.split('x'))
                            # ä½¿ç”¨ç­‰æ¯”æ‰©å±•+ç•™ç™½ï¼Œé¿å…æ‹‰ä¼¸
                            processed = smart_resize_with_padding(raw_img, (tw, th))
                        else:
                            processed = raw_img
                        buf = BytesIO()
                        processed.save(buf, format='JPEG', quality=95)
                        image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
                        try:
                            print(f"ğŸ”§ Final output size: {processed.size[0]}x{processed.size[1]}")
                        except Exception:
                            pass
                    except Exception:
                        # å›é€€ï¼šç›´æ¥é€ä¼ 
                        image_base64 = base64.b64encode(img_response.content).decode('utf-8')

                    return {
                        "data": [{
                            "b64_json": image_base64,
                            "url": image_url,
                            "revised_prompt": prompt
                        }],
                        "response_text": full_response
                    }
                except Exception:
                    return {
                        "data": [{
                            "b64_json": "",
                            "url": image_url,
                            "revised_prompt": prompt
                        }],
                        "response_text": full_response
                    }

            # æ²¡æœ‰å›¾ç‰‡ï¼Œä»…è¿”å›å“åº”æ–‡æœ¬
            return {
                "data": [{
                    "b64_json": "",
                    "url": "",
                    "revised_prompt": prompt
                }],
                "response_text": full_response
            }

        except requests.exceptions.HTTPError as http_err:
            status = getattr(http_err.response, 'status_code', None)
            try:
                body = http_err.response.text if http_err.response is not None else ''
                if body:
                    print(f"[ComflyNanoBananaMirror] HTTP {status} body: {body[:1000]}")
            except Exception:
                pass
            last_error = http_err
            # å¯¹408/429/5xxè¿›è¡Œé‡è¯•
            if status in (408, 409, 429) or (status is not None and 500 <= status < 600):
                if attempt < max_retries:
                    time.sleep(backoff_seconds * attempt)
                    continue
            raise Exception(f"Error in nano-banana image editing: {str(http_err)}")
        except requests.exceptions.Timeout as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception("Error in nano-banana image editing: request timed out")
        except Exception as e:
            last_error = e
            if attempt < max_retries:
                time.sleep(backoff_seconds * attempt)
                continue
            raise Exception(f"Error in nano-banana image editing: {str(e)}")

    # å¦‚æœåˆ°è¿™é‡Œä»ç„¶å¤±è´¥
    raise Exception(f"Error in nano-banana image editing: {str(last_error)}")

def _comfly_falai_edit(api_url: str, api_key: str, model: str, prompt: str, pil_image: Image.Image, size: str) -> dict:
    """Call Comfly's OpenAI-compatible images edits endpoint for FAL AI models."""
    import requests, io
    url = api_url.rstrip('/') + "/v1/images/edits"
    headers = {"Authorization": f"Bearer {api_key}"}
    buf = io.BytesIO()
    pil_image.save(buf, format='PNG')
    buf.seek(0)
    data = {
        "model": _normalize_model_name(model),
        "prompt": prompt,
        "size": _parse_size_str(size)
    }
    files = {"image": ("image.png", buf, "image/png")}
    r = requests.post(url, headers=headers, data=data, files=files, timeout=180)
    r.raise_for_status()
    return r.json()

def get_mirror_site_config(mirror_site_name: str) -> Dict[str, str]:
    """æ ¹æ®é•œåƒç«™åç§°è·å–å¯¹åº”çš„é…ç½®"""
    try:
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        mirror_sites = config.get('mirror_sites', {})
        
        if mirror_site_name in mirror_sites:
            site_config = mirror_sites[mirror_site_name]
            return {
                "url": site_config.get("url", ""),
                "api_key": site_config.get("api_key", ""),
                "api_format": site_config.get("api_format", ""),
                "models": site_config.get("models", []),
                "description": site_config.get("description", "")
            }
        else:
            # è¿”å›é»˜è®¤é…ç½®
            return {
                "url": "https://generativelanguage.googleapis.com",
                "api_key": "",
                "api_format": "gemini",
                "models": [],
                "description": ""
            }
    except Exception as e:
        _log_warning(f"Failed to get mirror site config: {e}")
        return {
            "url": "https://generativelanguage.googleapis.com",
            "api_key": "",
            "api_format": "gemini",
            "models": [],
            "description": ""
        }

def validate_openrouter_config(api_url: str, api_key: str, model: str) -> Dict[str, Any]:
    """éªŒè¯OpenRouteré…ç½®å¹¶è¿”å›ä¼˜åŒ–å»ºè®®"""
    validation_result = {
        "is_valid": True,
        "warnings": [],
        "suggestions": [],
        "optimized_params": {}
    }
    
    # éªŒè¯API URL
    if not api_url or "openrouter.ai" not in api_url:
        validation_result["is_valid"] = False
        validation_result["warnings"].append("OpenRouter API URL æ— æ•ˆ")
        return validation_result
    
    # éªŒè¯API Key
    if not api_key or not api_key.startswith("sk-or-v1-"):
        validation_result["warnings"].append("OpenRouter API Key æ ¼å¼å¯èƒ½ä¸æ­£ç¡®")
    
    # éªŒè¯æ¨¡å‹åç§°
    if "dall-e" in model.lower():
        validation_result["optimized_params"]["size"] = ["1024x1024", "1792x1024", "1024x1792"]
        validation_result["suggestions"].append("DALL-E æ¨¡å‹æ¨èä½¿ç”¨æ ‡å‡†å°ºå¯¸ä»¥è·å¾—æœ€ä½³æ•ˆæœ")
    elif "stable-diffusion" in model.lower():
        validation_result["suggestions"].append("Stable Diffusion æ¨¡å‹å°ºå¯¸ä¼šè‡ªåŠ¨è°ƒæ•´ä¸º8çš„å€æ•°")
    elif "gemini" in model.lower():
        validation_result["suggestions"].append("Gemini æ¨¡å‹æ”¯æŒå¤šç§å°ºå¯¸å’Œè´¨é‡è®¾ç½®")
    
    return validation_result

def process_openrouter_stream(response) -> str:
    """å¤„ç†OpenRouterçš„æµå¼å“åº”"""
    accumulated_content = ""
    chunk_count = 0
    empty_chunks = 0
    content_chunks = 0
    last_content_chunk = 0
    
    print(f"ğŸ”„ å¼€å§‹å¤„ç†OpenRouteræµå¼å“åº”...")
    
    try:
        for line in response.iter_lines(decode_unicode=True, chunk_size=None):
            if line and line.startswith('data: '):
                chunk_count += 1
                data_content = line[6:]  # Remove 'data: ' prefix
                
                print(f"ğŸ“¦ å¤„ç†ç¬¬{chunk_count}ä¸ªæ•°æ®å—...")
                
                if data_content.strip() == '[DONE]':
                    print(f"âœ… æ”¶åˆ°ç»“æŸä¿¡å·[DONE]")
                    break
                
                try:
                    # å°è¯•è§£æJSON
                    chunk_data = json.loads(data_content)
                    print(f"ğŸ” æ•°æ®å—ç»“æ„: {list(chunk_data.keys())}")
                    
                    # æå–å†…å®¹
                    if 'choices' in chunk_data and chunk_data['choices']:
                        choice = chunk_data['choices'][0]
                        print(f"ğŸ” é€‰æ‹©é¡¹ç»“æ„: {list(choice.keys())}")
                        
                        if 'delta' in choice:
                            delta = choice['delta']
                            print(f"ğŸ” Deltaç»“æ„: {list(delta.keys())}")
                            
                            # æ£€æŸ¥imageså­—æ®µï¼ˆOpenRouterå¯èƒ½åœ¨è¿™é‡Œè¿”å›å›¾åƒæ•°æ®ï¼‰
                            if 'images' in delta and delta['images']:
                                print(f"ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouter imageså­—æ®µï¼")
                                images_data = delta['images']
                                print(f"ğŸ” Imageså­—æ®µç±»å‹: {type(images_data)}")
                                print(f"ğŸ” Imageså­—æ®µå†…å®¹: {str(images_data)[:200]}...")
                                
                                # ä½¿ç”¨å‚è€ƒé¡¹ç›®çš„æ–¹æ³•ï¼šç›´æ¥æœç´¢data:image/å­—ç¬¦ä¸²
                                import re
                                chunk_str = str(images_data)
                                if 'data:image/' in chunk_str:
                                    print(f"ğŸ¯ OpenRouteråœ¨imageså­—æ®µä¸­å‘ç°å›¾ç‰‡æ•°æ®!")
                                    # ä½¿ç”¨å‚è€ƒé¡¹ç›®çš„æ­£ç¡®æ­£åˆ™è¡¨è¾¾å¼
                                    base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                    matches = re.findall(base64_pattern, chunk_str)
                                    if matches:
                                        for url in matches:
                                            print(f"ğŸ¯ OpenRouteræå–base64å›¾ç‰‡ï¼Œé•¿åº¦: {len(url)}å­—ç¬¦")
                                            accumulated_content += " " + url
                                    else:
                                        print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾ç‰‡æ•°æ®")
                                        # å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥æå–data:image/å¼€å§‹åˆ°å­—ç¬¦ä¸²ç»“æŸ
                                        start_pos = chunk_str.find('data:image/')
                                        if start_pos != -1:
                                            extracted_data = chunk_str[start_pos:]
                                            print(f"ğŸ¯ å¤‡ç”¨æ–¹æ³•æå–å›¾ç‰‡æ•°æ®ï¼Œé•¿åº¦: {len(extracted_data)}å­—ç¬¦")
                                            accumulated_content += " " + extracted_data
                                else:
                                    print(f"âš ï¸ æœªæ‰¾åˆ°data:image/æ ‡è®°")
                                
                                content_chunks += 1
                                last_content_chunk = chunk_count
                            
                            # æ£€æŸ¥contentå­—æ®µ
                            if 'content' in delta and delta['content']:
                                content = delta['content']
                                accumulated_content += content
                                content_chunks += 1
                                last_content_chunk = chunk_count
                                print(f"ğŸ“ æ·»åŠ å†…å®¹: {len(content)}å­—ç¬¦ (ç´¯è®¡: {len(accumulated_content)}å­—ç¬¦)")
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                                if '![image]' in content:
                                    print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒæ•°æ®æ ‡è®°ï¼")
                            
                            # å¦‚æœæ—¢æ²¡æœ‰imagesä¹Ÿæ²¡æœ‰contentï¼Œæ ‡è®°ä¸ºç©ºå—
                            if not ('images' in delta and delta['images']) and not ('content' in delta and delta['content']):
                                empty_chunks += 1
                                print(f"âš ï¸ ç©ºçš„deltaå— (æ— imageså’Œcontentå­—æ®µ) - è¿™æ˜¯æ­£å¸¸çš„ï¼ŒOpenRouterç”¨ç©ºå—ä¿æŒè¿æ¥")
                        
                        elif 'message' in choice:
                            message = choice['message']
                            print(f"ğŸ” Messageç»“æ„: {list(message.keys())}")
                            
                            if 'content' in message and message['content']:
                                content = message['content']
                                accumulated_content += content
                                content_chunks += 1
                                last_content_chunk = chunk_count
                                print(f"ğŸ“ æ·»åŠ æ¶ˆæ¯å†…å®¹: {len(content)}å­—ç¬¦ (ç´¯è®¡: {len(accumulated_content)}å­—ç¬¦)")
                                
                                # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                                if '![image]' in content:
                                    print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒæ•°æ®æ ‡è®°ï¼")
                            else:
                                empty_chunks += 1
                                print(f"âš ï¸ ç©ºçš„æ¶ˆæ¯å— (æ— contentå­—æ®µ)")
                        else:
                            empty_chunks += 1
                            print(f"âš ï¸ æœªçŸ¥çš„é€‰æ‹©é¡¹ç»“æ„")
                    else:
                        empty_chunks += 1
                        print(f"âš ï¸ æ•°æ®å—ä¸­æ²¡æœ‰choiceså­—æ®µ")
                
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}, è·³è¿‡æ­¤å—")
                    continue
        
        print(f"âœ… æµå¼å“åº”å¤„ç†å®Œæˆ:")
        print(f"   ğŸ“Š æ€»å—æ•°: {chunk_count}")
        print(f"   ğŸ“ å†…å®¹å—æ•°: {content_chunks}")
        print(f"   âš ï¸ ç©ºå—æ•°: {empty_chunks}")
        print(f"   ğŸ“ æ€»å†…å®¹é•¿åº¦: {len(accumulated_content)}")
        
        if accumulated_content:
            print(f"   ğŸ” å†…å®¹é¢„è§ˆ: {accumulated_content[:200]}{'...' if len(accumulated_content) > 200 else ''}")
            if last_content_chunk > 0:
                print(f"   ğŸ“ æœ€åä¸€ä¸ªå†…å®¹å—ä½ç½®: ç¬¬{last_content_chunk}å—")
        else:
            print(f"   âš ï¸ è­¦å‘Š: æ²¡æœ‰æå–åˆ°ä»»ä½•å†…å®¹ï¼")
            print(f"   ğŸ’¡ å»ºè®®: æ£€æŸ¥OpenRouter APIå“åº”æ ¼å¼æˆ–æ¨¡å‹é…ç½®")
        
        return accumulated_content
        
    except Exception as e:
        print(f"âŒ æµå¼å“åº”å¤„ç†å¤±è´¥: {e}")
        return accumulated_content

def validate_api_url(url):
    """éªŒè¯API URLæ ¼å¼å¹¶è‡ªåŠ¨è¡¥å…¨"""
    if not url or not url.strip():
        return False
    
    url = url.strip()
    
    # å¦‚æœå·²ç»æ˜¯å®Œæ•´URLæ ¼å¼ï¼Œç›´æ¥è¿”å›True
    if url.startswith(('http://', 'https://')):
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        return url_pattern.match(url) is not None
    
    # å¦‚æœä¸æ˜¯å®Œæ•´URLï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆåŸŸå/IP
    domain_pattern = re.compile(
        r'^(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
        r'localhost|'  # localhost...
        r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
        r'(?::\d+)?'  # optional port
        r'(?:/?|[/?]\S+)?$', re.IGNORECASE)
    
    return domain_pattern.match(url) is not None

def build_api_url(base_url, model, api_format="gemini"):
    """æ„å»ºå®Œæ•´çš„API URL"""
    base_url = base_url.strip().rstrip('/')
    
    # è‡ªåŠ¨è¡¥å…¨åè®®å‰ç¼€
    if not base_url.startswith(('http://', 'https://')):
        base_url = f"https://{base_url}"
    
    # OpenRouteré•œåƒç«™ç‰¹æ®Šå¤„ç†
    if "openrouter.ai" in base_url:
        # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹ï¼ŒURLæ„å»ºåœ¨OpenRouterå¤„ç†é€»è¾‘ä¸­
        return base_url
    
    # T8é•œåƒç«™ç‰¹æ®Šå¤„ç†
    if "t8star.cn" in base_url or "ai.t8star.cn" in base_url:
        return f"{base_url}/v1/chat/completions"
    
    # Comflyé•œåƒç«™ç‰¹æ®Šå¤„ç† - å¯¹nano-bananaä½¿ç”¨chat/completionsï¼Œå…¶ä»–æœåŠ¡ä½¿ç”¨æ ‡å‡†Gemini API
    if _is_comfly_base(base_url):
        if model in ["nano-banana", "nano-banana-hd"]:
            return "https://ai.comfly.chat/v1/chat/completions"
        else:
            # å…¶ä»–ComflyæœåŠ¡ä½¿ç”¨æ ‡å‡†Gemini APIæ ¼å¼
            return f"{base_url}/models/{_normalize_model_name(model)}:generateContent"
    
    # API4GPTé•œåƒç«™ç‰¹æ®Šå¤„ç†
    if "www.api4gpt.com" in base_url:
        # API4GPTçš„URLæ„å»ºåœ¨call_api4gpt_apiå‡½æ•°ä¸­å¤„ç†
        return base_url
    
    # å¦‚æœç”¨æˆ·æä¾›çš„æ˜¯å®Œæ•´URLï¼Œç›´æ¥ä½¿ç”¨
    if '/models/' in base_url and ':generateContent' in base_url:
        return base_url
    
    # å¦‚æœæ˜¯åŸºç¡€URLï¼Œæ„å»ºå®Œæ•´è·¯å¾„
    if base_url.endswith('/v1beta') or base_url.endswith('/v1'):
        return f"{base_url}/models/{model}:generateContent"
    
    # é»˜è®¤æ·»åŠ v1betaè·¯å¾„
    return f"{base_url}/v1beta/models/{model}:generateContent"

def build_t8_api_request(model, prompt, image_base64=None, temperature=0.9, max_tokens=2048):
    """æ„å»ºT8é•œåƒç«™çš„APIè¯·æ±‚æ ¼å¼
    
    T8é•œåƒç«™ä½¿ç”¨OpenAIå…¼å®¹çš„APIæ ¼å¼ï¼Œä½†éœ€è¦ç‰¹æ®Šå¤„ç†
    """
    # æ„å»ºæ¶ˆæ¯å†…å®¹
    content = []
    
    # æ·»åŠ æ–‡æœ¬å†…å®¹
    content.append({
        "type": "text",
        "text": prompt
    })
    
    # å¦‚æœæœ‰å›¾åƒï¼Œæ·»åŠ å›¾åƒå†…å®¹
    if image_base64:
        content.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_base64}"
            }
        })
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "model": _normalize_model_name(model),
        "messages": [
            {
                "role": "user",
                "content": content
            }
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False
    }
    
    return request_data

def call_t8_api(url, api_key, request_data, timeout=300):
    """è°ƒç”¨T8é•œåƒç«™API"""
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key.strip()}"
    }
    
    try:
        response = requests.post(url, headers=headers, json=request_data, timeout=timeout)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        raise ValueError(f"T8é•œåƒç«™APIè°ƒç”¨å¤±è´¥: {str(e)}")
    except json.JSONDecodeError as e:
        raise ValueError(f"T8é•œåƒç«™APIå“åº”è§£æå¤±è´¥: {str(e)}")

def build_api4gpt_request(service_type, model, prompt, image_base64=None, size="1024x1024", quality="hd", style="natural", temperature=0.9, max_tokens=2048):
    """æ„å»ºAPI4GPTçš„APIè¯·æ±‚æ ¼å¼
    
    API4GPTæ”¯æŒå¤šç§å›¾åƒæœåŠ¡ï¼ŒåŒ…æ‹¬nano-bananaã€DALL-E 3ã€Stable-Diffusionã€Fluxç­‰
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
    """
    if service_type == "nano-banana":
        # nano-bananaæœåŠ¡ä½¿ç”¨å®˜æ–¹æ–‡æ¡£æ ¼å¼
        request_data = {
            "prompt": prompt,
            "n": 1,
            "model": "gemini-2.5-flash-image"
        }
        
        # å¦‚æœæœ‰å›¾åƒï¼Œæ·»åŠ å›¾åƒå†…å®¹ï¼ˆç”¨äºå›¾åƒç¼–è¾‘ï¼‰
        if image_base64:
            # å¯¹äºå›¾åƒç¼–è¾‘ï¼Œä½¿ç”¨multipart/form-dataæ ¼å¼
            # è¿™é‡Œè¿”å›ä¸€ä¸ªæ ‡è®°ï¼Œè¡¨ç¤ºéœ€è¦ä½¿ç”¨multipartæ ¼å¼
            request_data["_multipart"] = True
            request_data["image"] = image_base64
            
    elif service_type == "dall-e-3":
        # DALL-E 3æœåŠ¡ä½¿ç”¨OpenAIæ ¼å¼
        request_data = {
            "model": _normalize_model_name(model),
            "prompt": prompt,
            "n": 1,
            "size": size,
            "quality": quality,
            "style": style,
            "response_format": "b64_json"
        }
        
    elif service_type == "stable-diffusion":
        # Stable-DiffusionæœåŠ¡
        request_data = {
            "prompt": prompt,
            "negative_prompt": "",
            "width": int(size.split('x')[0]),
            "height": int(size.split('x')[1]),
            "num_inference_steps": 20,
            "guidance_scale": 7.5,
            "num_images_per_prompt": 1
        }
        
    elif service_type == "flux":
        # FluxæœåŠ¡
        request_data = {
            "prompt": prompt,
            "width": int(size.split('x')[0]),
            "height": int(size.split('x')[1]),
            "num_images": 1,
            "scheduler": "euler",
            "num_inference_steps": 20,
            "guidance_scale": 7.5
        }
        
    else:
        # é»˜è®¤ä½¿ç”¨nano-bananaæ ¼å¼
        request_data = {
            "prompt": prompt,
            "n": 1,
            "model": "gemini-2.5-flash-image"
        }
    
    return request_data

def call_api4gpt_api(url, api_key, service_type, request_data, timeout=300):
    """è°ƒç”¨API4GPT API
    
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
    """
    # æ ¹æ®æœåŠ¡ç±»å‹æ„å»ºä¸åŒçš„APIç«¯ç‚¹
    if service_type == "nano-banana":
        # nano-bananaä½¿ç”¨ /v1/images/generations ç«¯ç‚¹
        api_endpoint = f"{url}/v1/images/generations"
    elif service_type == "dall-e-3":
        api_endpoint = f"{url}/v1/images/generations"
    elif service_type == "stable-diffusion":
        api_endpoint = f"{url}/v1/images/generations"
    elif service_type == "flux":
        api_endpoint = f"{url}/v1/images/generations"
    else:
        api_endpoint = f"{url}/v1/images/generations"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦ä½¿ç”¨multipartæ ¼å¼
    if request_data.get("_multipart") and service_type == "nano-banana":
        # å¯¹äºå›¾åƒç¼–è¾‘ï¼Œä½¿ç”¨multipart/form-dataæ ¼å¼
        print("ğŸ”— ä½¿ç”¨API4GPT multipartæ ¼å¼è¿›è¡Œå›¾åƒç¼–è¾‘")
        
        # å‡†å¤‡multipartæ•°æ®
        files = {}
        data = {}
        
        # æ·»åŠ å›¾åƒæ–‡ä»¶
        if "image" in request_data:
            # å°†base64è½¬æ¢ä¸ºæ–‡ä»¶å¯¹è±¡
            image_data = base64.b64decode(request_data["image"])
            files["image"] = ("image.jpg", image_data, "image/jpeg")
        
        # æ·»åŠ å…¶ä»–å‚æ•°
        data["prompt"] = request_data["prompt"]
        data["n"] = request_data["n"]
        data["model"] = request_data["model"]
        
        # ä½¿ç”¨multipartç«¯ç‚¹
        edit_endpoint = f"{url}/v1/images/edits"
        
        headers = {
            "Authorization": f"Bearer {api_key.strip()}"
        }
        
        try:
            response = requests.post(
                edit_endpoint, 
                headers=headers, 
                files=files, 
                data=data, 
                timeout=timeout
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise ValueError(f"API4GPT multipart APIè°ƒç”¨å¤±è´¥: {str(e)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"API4GPT multipart APIå“åº”è§£æå¤±è´¥: {str(e)}")
    else:
        # æ ‡å‡†JSONæ ¼å¼
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key.strip()}"
        }
        
        try:
            response = requests.post(api_endpoint, headers=headers, json=request_data, timeout=timeout)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {str(e)}")
        except json.JSONDecodeError as e:
            raise ValueError(f"API4GPT APIå“åº”è§£æå¤±è´¥: {str(e)}")

def parse_api4gpt_response(response_data, service_type):
    """è§£æAPI4GPTçš„å“åº”æ•°æ®
    
    æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼šhttps://doc.api4gpt.com/api-341609441
    """
    if service_type == "nano-banana":
        # è§£ænano-bananaå“åº”ï¼ˆå®˜æ–¹æ–‡æ¡£æ ¼å¼ï¼‰
        response_text = "nano-bananaå›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        # æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå“åº”æ ¼å¼ä¸ºï¼š
        # {
        #   "created": 1745711868,
        #   "data": [
        #     {
        #       "revised_prompt": "ä¸€åªç†ŠçŒ«åœ¨éª‘è‡ªè¡Œè½¦...",
        #       "url": "https://filesystem.site/cdn/..."
        #     }
        #   ]
        # }
        
        if "data" in response_data and response_data["data"]:
            image_data = response_data["data"][0]
            
            # æå–ä¿®è®¢åçš„æç¤ºè¯
            if "revised_prompt" in image_data:
                response_text = f"å›¾åƒç”Ÿæˆå®Œæˆã€‚ä¿®è®¢åçš„æç¤ºè¯: {image_data['revised_prompt']}"
            
            # æå–å›¾åƒURL
            if "url" in image_data:
                image_url = image_data["url"]
                try:
                    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½API4GPTç”Ÿæˆçš„å›¾åƒ: {image_url}")
                    response = requests.get(image_url, timeout=30)
                    if response.status_code == 200:
                        image_bytes = response.content
                        generated_image = Image.open(io.BytesIO(image_bytes))
                        print("âœ… æˆåŠŸä¸‹è½½API4GPTç”Ÿæˆçš„å›¾åƒ")
                    else:
                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
        
        return response_text, generated_image
        
    elif service_type == "dall-e-3":
        # è§£æDALL-E 3å“åº”
        response_text = "DALL-E 3å›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        if "data" in response_data and response_data["data"]:
            image_data = response_data["data"][0]
            if "url" in image_data:
                image_url = image_data["url"]
                try:
                    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½DALL-E 3ç”Ÿæˆçš„å›¾åƒ: {image_url}")
                    response = requests.get(image_url, timeout=30)
                    if response.status_code == 200:
                        image_bytes = response.content
                        generated_image = Image.open(io.BytesIO(image_bytes))
                        print("âœ… æˆåŠŸä¸‹è½½DALL-E 3ç”Ÿæˆçš„å›¾åƒ")
                    else:
                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
        
        return response_text, generated_image
        
    elif service_type == "stable-diffusion":
        # è§£æStable-Diffusionå“åº”
        response_text = "Stable-Diffusionå›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        if "images" in response_data and response_data["images"]:
            image_data = response_data["images"][0]
            if image_data:
                try:
                    image_bytes = base64.b64decode(image_data)
                    generated_image = Image.open(io.BytesIO(image_bytes))
                    print("âœ… æˆåŠŸè§£æStable-Diffusionå›¾åƒæ•°æ®")
                except Exception as e:
                    print(f"âš ï¸ Stable-Diffusionå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
        
        return response_text, generated_image
        
    elif service_type == "flux":
        # è§£æFluxå“åº”
        response_text = "Fluxå›¾åƒç”Ÿæˆå®Œæˆ"
        generated_image = None
        
        if "images" in response_data and response_data["images"]:
            image_data = response_data["images"][0]
            if image_data:
                try:
                    image_bytes = base64.b64decode(image_data)
                    generated_image = Image.open(io.BytesIO(image_bytes))
                    print("âœ… æˆåŠŸè§£æFluxå›¾åƒæ•°æ®")
                except Exception as e:
                    print(f"âš ï¸ Fluxå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
        
        return response_text, generated_image
        
    else:
        # é»˜è®¤è¿”å›ç©ºç»“æœ
        return "", None

def smart_retry_delay(attempt, error_code=None):
    """æ™ºèƒ½é‡è¯•å»¶è¿Ÿ - æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­‰å¾…æ—¶é—´"""
    base_delay = 2 ** attempt  # æŒ‡æ•°é€€é¿
    
    if error_code == 429:  # é™æµé”™è¯¯
        rate_limit_delay = 60 + random.uniform(10, 30)  # 60-90ç§’éšæœºç­‰å¾…
        return max(base_delay, rate_limit_delay)
    elif error_code in [500, 502, 503, 504]:  # æœåŠ¡å™¨é”™è¯¯
        return base_delay + random.uniform(1, 5)  # æ·»åŠ éšæœºæŠ–åŠ¨
    else:
        return base_delay

def create_dummy_image(width=512, height=512):
    """Create a placeholder image"""
    dummy_array = np.zeros((height, width, 3), dtype=np.uint8)
    dummy_tensor = torch.from_numpy(dummy_array).float() / 255.0
    return dummy_tensor.unsqueeze(0)

def resize_image_for_api(image, max_size=2048):
    """è°ƒæ•´å›¾åƒå¤§å°ä»¥æ»¡è¶³APIé™åˆ¶"""
    if max(image.size) > max_size:
        ratio = max_size / max(image.size)
        new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        _log_info(f"Image resized to {new_size} for API compatibility")
    return image

def remove_white_areas(image: Image.Image, white_threshold: int = 250) -> Image.Image:
    """
    æ£€æµ‹å¹¶å»é™¤å›¾åƒä¸­çš„ç™½è‰²åŒºåŸŸ

    Args:
        image: è¾“å…¥å›¾åƒ
        white_threshold: ç™½è‰²é˜ˆå€¼ï¼Œåƒç´ å€¼å¤§äºæ­¤å€¼è¢«è®¤ä¸ºæ˜¯ç™½è‰² (0-255)

    Returns:
        å»é™¤ç™½è‰²åŒºåŸŸåçš„å›¾åƒ
    """
    try:
        import numpy as np

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(image)
        height, width = img_array.shape[:2]

        print(f"ğŸ” å¼€å§‹æ£€æµ‹ç™½è‰²åŒºåŸŸï¼Œé˜ˆå€¼: {white_threshold}")

        # å¤šç§ç™½è‰²æ£€æµ‹ç­–ç•¥
        white_masks = []

        # ç­–ç•¥1: ä¸¥æ ¼ç™½è‰²æ£€æµ‹ (RGBä¸‰ä¸ªé€šé“éƒ½å¤§äºé˜ˆå€¼)
        if len(img_array.shape) == 3:  # RGBå›¾åƒ
            strict_white_mask = np.all(img_array >= white_threshold, axis=2)
            white_masks.append(strict_white_mask)

            # ç­–ç•¥2: è¿‘ä¼¼ç™½è‰²æ£€æµ‹ (RGBå·®å¼‚å°ä¸”å¹³å‡å€¼é«˜)
            rgb_mean = np.mean(img_array, axis=2)
            rgb_std = np.std(img_array, axis=2)
            approx_white_mask = (rgb_mean >= white_threshold - 10) & (rgb_std <= 15)
            white_masks.append(approx_white_mask)

            # ç­–ç•¥3: çº¯ç™½è‰²æ£€æµ‹ (RGB = 255)
            pure_white_mask = np.all(img_array == 255, axis=2)
            white_masks.append(pure_white_mask)

        else:  # ç°åº¦å›¾åƒ
            white_mask = img_array >= white_threshold
            white_masks.append(white_mask)

        # åˆå¹¶æ‰€æœ‰ç™½è‰²æ£€æµ‹ç»“æœ
        combined_white_mask = np.logical_or.reduce(white_masks)

        # è®¡ç®—ç™½è‰²åƒç´ æ¯”ä¾‹
        white_ratio = np.sum(combined_white_mask) / (height * width)
        print(f"ğŸ” ç™½è‰²åƒç´ æ¯”ä¾‹: {white_ratio:.2%}")

        # å¦‚æœç™½è‰²åƒç´ æ¯”ä¾‹å¤ªä½ï¼Œä¸éœ€è¦å¤„ç†
        if white_ratio < 0.02:  # å°äº2%
            print(f"â„¹ï¸ ç™½è‰²åƒç´ æ¯”ä¾‹è¾ƒä½({white_ratio:.2%})ï¼Œè·³è¿‡å¤„ç†")
            return image

        # æ‰¾åˆ°éç™½è‰²åŒºåŸŸçš„è¾¹ç•Œæ¡†
        non_white_mask = ~combined_white_mask

        # æ‰¾åˆ°éç™½è‰²åƒç´ çš„è¡Œå’Œåˆ—
        non_white_rows = np.any(non_white_mask, axis=1)
        non_white_cols = np.any(non_white_mask, axis=0)

        # å¦‚æœæ²¡æœ‰éç™½è‰²åƒç´ ï¼Œè¿”å›åŸå›¾
        if not np.any(non_white_rows) or not np.any(non_white_cols):
            print(f"âš ï¸ å›¾åƒå‡ ä¹å…¨æ˜¯ç™½è‰²ï¼Œä¿æŒåŸå›¾")
            return image

        # æ‰¾åˆ°è¾¹ç•Œ
        top = np.argmax(non_white_rows)
        bottom = len(non_white_rows) - 1 - np.argmax(non_white_rows[::-1])
        left = np.argmax(non_white_cols)
        right = len(non_white_cols) - 1 - np.argmax(non_white_cols[::-1])

        # æ£€æµ‹è¾¹ç¼˜ç™½è‰²åŒºåŸŸçš„åšåº¦
        edge_thickness = {
            'top': top,
            'bottom': height - 1 - bottom,
            'left': left,
            'right': width - 1 - right
        }

        print(f"ğŸ” è¾¹ç¼˜ç™½è‰²åšåº¦: ä¸Š{edge_thickness['top']}, ä¸‹{edge_thickness['bottom']}, å·¦{edge_thickness['left']}, å³{edge_thickness['right']}")

        # åªæœ‰å½“è¾¹ç¼˜ç™½è‰²åŒºåŸŸè¶³å¤Ÿåšä¸”æ˜¯çœŸæ­£çš„è¾¹æ¡†æ—¶æ‰è¿›è¡Œè£å‰ª
        min_edge_thickness = max(30, width // 8, height // 8)  # è‡³å°‘30åƒç´ æˆ–å›¾åƒå°ºå¯¸çš„12.5%

        # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„è¾¹æ¡†ï¼ˆå››è¾¹éƒ½æœ‰ç™½è‰²æˆ–è€…å¯¹è¾¹æœ‰ç™½è‰²ï¼‰
        thick_edges = [k for k, v in edge_thickness.items() if v >= min_edge_thickness]

        # åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µæ‰è£å‰ªï¼š
        # 1. å››è¾¹éƒ½æœ‰åšç™½è¾¹
        # 2. å¯¹è¾¹éƒ½æœ‰åšç™½è¾¹ï¼ˆä¸Šä¸‹æˆ–å·¦å³ï¼‰
        # 3. ä¸‰è¾¹æœ‰åšç™½è¾¹
        is_border = (
            len(thick_edges) >= 3 or  # ä¸‰è¾¹æˆ–å››è¾¹æœ‰åšç™½è¾¹
            ('top' in thick_edges and 'bottom' in thick_edges) or  # ä¸Šä¸‹éƒ½æœ‰
            ('left' in thick_edges and 'right' in thick_edges)     # å·¦å³éƒ½æœ‰
        )

        if not is_border:
            print(f"â„¹ï¸ ä¸æ˜¯çœŸæ­£çš„ç™½è‰²è¾¹æ¡†ï¼Œè·³è¿‡è£å‰ªã€‚åšè¾¹: {thick_edges}")
            return image

        print(f"âœ… æ£€æµ‹åˆ°ç™½è‰²è¾¹æ¡†ï¼Œåšè¾¹: {thick_edges}")

        # æ·»åŠ ä¸€äº›è¾¹è·ï¼Œé¿å…è£å‰ªè¿‡ç´§
        margin = min(5, width // 50, height // 50)  # æœ€å¤š5åƒç´ æˆ–å›¾åƒå°ºå¯¸çš„2%
        top = max(0, top - margin)
        bottom = min(height - 1, bottom + margin)
        left = max(0, left - margin)
        right = min(width - 1, right + margin)

        # æ£€æŸ¥è£å‰ªåŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
        crop_width = right - left + 1
        crop_height = bottom - top + 1

        if crop_width <= 0 or crop_height <= 0:
            print(f"âš ï¸ è£å‰ªåŒºåŸŸæ— æ•ˆï¼Œä¿æŒåŸå›¾")
            return image

        # è®¡ç®—è£å‰ªæ¯”ä¾‹
        crop_ratio = (crop_width * crop_height) / (width * height)

        # å¦‚æœè£å‰ªåçš„åŒºåŸŸå¤ªå°ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤
        if crop_ratio < 0.3:  # å°äº30%
            print(f"âš ï¸ è£å‰ªååŒºåŸŸè¿‡å°({crop_ratio:.2%})ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤ï¼Œä¿æŒåŸå›¾")
            return image

        print(f"âœ… æ£€æµ‹åˆ°ç™½è‰²è¾¹æ¡†ï¼Œè£å‰ªåŒºåŸŸ: ({left}, {top}) -> ({right}, {bottom})")
        print(f"âœ… è£å‰ªå°ºå¯¸: {crop_width}x{crop_height} (ä¿ç•™{crop_ratio:.1%})")

        # è£å‰ªå›¾åƒ
        cropped_image = image.crop((left, top, right + 1, bottom + 1))

        return cropped_image

    except Exception as e:
        print(f"âŒ ç™½è‰²åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return image

def smart_resize_with_padding(image: Image.Image, target_size: Tuple[int, int],
                             fill_color: Tuple[int, int, int] = (255, 255, 255)) -> Image.Image:
    """
    ç»Ÿä¸€è°ƒç”¨ä¸»å®ç°ï¼šæŒ‰ç›®æ ‡å°ºå¯¸ç›´æ¥æ‰©å›¾ï¼Œé¿å…å…ˆæ”¾å¤§å†è£å‰ªã€‚
    å¢å¼ºåŠŸèƒ½ï¼šé¦–å…ˆæ£€æµ‹å¹¶å»é™¤ç™½è‰²åŒºåŸŸï¼Œç„¶åè¿›è¡Œæ™ºèƒ½å¤„ç†ã€‚
    """
    try:
        img_width, img_height = image.size
        target_width, target_height = target_size

        print(f"ğŸ¯ å¼€å§‹å›¾åƒå¤„ç†: {img_width}x{img_height} -> {target_width}x{target_height}")

        # ğŸš€ ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å¹¶å»é™¤ç™½è‰²åŒºåŸŸ
        processed_image = remove_white_areas(image)
        if processed_image.size != image.size:
            print(f"âœ… ç™½è‰²åŒºåŸŸå·²å»é™¤: {image.size} -> {processed_image.size}")
            image = processed_image
            img_width, img_height = image.size

            # å¦‚æœè¿˜æœ‰ç™½è‰²åŒºåŸŸï¼Œå°è¯•æ›´æ¿€è¿›çš„æ£€æµ‹
            processed_image2 = remove_white_areas(image, white_threshold=230)
            if processed_image2.size != image.size:
                print(f"âœ… æ¿€è¿›æ¨¡å¼å†æ¬¡å»é™¤ç™½è‰²åŒºåŸŸ: {image.size} -> {processed_image2.size}")
                image = processed_image2
                img_width, img_height = image.size
        else:
            print(f"â„¹ï¸ æœªæ£€æµ‹åˆ°éœ€è¦å»é™¤çš„ç™½è‰²åŒºåŸŸ")

        # æ¯”ä¾‹ç›¸åŒæ—¶ï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸
        if abs(img_width/img_height - target_width/target_height) < 0.01:
            print(f"ğŸ¯ æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸")
            resized_img = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
            return resized_img

        # ä½¿ç”¨è£å‰ªæ¨¡å¼ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œç„¶åæ™ºèƒ½è£å‰ª
        print(f"ğŸ¯ è£å‰ªæ¨¡å¼ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œç„¶åæ™ºèƒ½è£å‰ª")

        # è®¡ç®—æœ€ä½³ç¼©æ”¾æ¯”ä¾‹
        scale_x = target_width / img_width
        scale_y = target_height / img_height
        scale = max(scale_x, scale_y)  # ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–

        # è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸
        enlarged_width = int(img_width * scale)
        enlarged_height = int(img_height * scale)

        print(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§: {img_width}x{img_height} -> {enlarged_width}x{enlarged_height}")
        print(f"ğŸ”§ ç¼©æ”¾æ¯”ä¾‹: {scale:.3f}")

        # ä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·è¿›è¡Œæ”¾å¤§ï¼ˆç¦ç”¨AIæ”¾å¤§é¿å…é—®é¢˜ï¼‰
        enlarged_image = image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)

        # æ™ºèƒ½è£å‰ª - ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
        if enlarged_width >= target_width and enlarged_height >= target_height:
            print(f"ğŸ”§ æ™ºèƒ½è£å‰ªï¼šä»é«˜æ¸…æ”¾å¤§å›¾åƒä¸­è£å‰ªç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å±…ä¸­")

            # ç²¾ç¡®è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“å®Œå…¨å±…ä¸­
            crop_x = (enlarged_width - target_width) // 2
            crop_y = (enlarged_height - target_height) // 2

            print(f"ğŸ”§ ç²¾ç¡®å±…ä¸­è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")

            # ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
            final_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))

            print(f"âœ… é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªå®Œæˆ")
            return final_image
        else:
            print(f"âš ï¸ é«˜æ¸…æ”¾å¤§åå°ºå¯¸ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……")
            # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒ
            final_image = Image.new('RGB', (target_width, target_height), fill_color)

            # å°†é«˜æ¸…æ”¾å¤§çš„å›¾åƒå±…ä¸­æ”¾ç½®
            paste_x = (target_width - enlarged_width) // 2
            paste_y = (target_height - enlarged_height) // 2
            final_image.paste(enlarged_image, (paste_x, paste_y))

            print(f"âœ… æ™ºèƒ½å¡«å……å®Œæˆ")
            return final_image

    except Exception as e:
        print(f"âŒ å›¾åƒå¤„ç†å¤±è´¥ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ: {e}")
        import traceback
        print(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")

        # å›é€€åˆ°æœ€ç®€å•çš„å¤„ç†æ–¹å¼
        try:
            from .gemini_banana import smart_resize_with_padding as core_resize
        except ImportError:
            from gemini_banana import smart_resize_with_padding as core_resize
        return core_resize(image, target_size, fill_color=fill_color, fill_strategy="paste")

class KenChenLLMGeminiBananaMirrorImageGenNode:
    """Gemini Banana é•œåƒç«™å›¾ç‰‡ç”ŸæˆèŠ‚ç‚¹
    
    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒé€‰æ‹©é¢„é…ç½®çš„é•œåƒç«™ï¼ˆofficial, comfly, customï¼‰
    - è‡ªåŠ¨å¡«å……å¯¹åº”é•œåƒç«™çš„ API URL å’Œ API Key
    - é€‰æ‹© custom æ—¶å¯æ‰‹åŠ¨è¾“å…¥è‡ªå®šä¹‰é•œåƒç«™ä¿¡æ¯
    - æ™ºèƒ½URLæ ¼å¼éªŒè¯å’Œè‡ªåŠ¨è¡¥å…¨
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # å¯¹é½ Gemini Banana Text to Image Banana çš„æ§ä»¶
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        image_settings = config.get('image_settings', {})
        
        # è·å–é•œåƒç«™é…ç½®
        mirror_sites = config.get('mirror_sites', {})
        
        # ä¸å†é‡å¤æ·»åŠ T8é•œåƒç«™é…ç½®ï¼Œå› ä¸ºé…ç½®æ–‡ä»¶ä¸­å·²ç»æœ‰äº†
        
        mirror_options = list(mirror_sites.keys())
        if not mirror_options:
            mirror_options = ["official", "comfly", "custom"]
        
        # è·å–é»˜è®¤é•œåƒç«™é…ç½®
        default_site = "comfly" if "comfly" in mirror_options else mirror_options[0] if mirror_options else "official"
        default_config = get_mirror_site_config(default_site)
        
        # ğŸš€ è¶…è¶Šå‚è€ƒé¡¹ç›®çš„å›¾åƒæ§åˆ¶é¢„è®¾
        size_presets = image_settings.get('size_presets', [
            "Original size", "512x512", "768x768", "1024x1024", "1024x1792", "1792x1024",
            "1920x1080", "2560x1440", "3840x2160"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é«˜åˆ†è¾¨ç‡é€‰é¡¹
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„è¶…é«˜æ¸…é€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "mirror_site": (mirror_options, {"default": default_site}),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "é•œåƒç«™APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç•™ç©ºæ—¶è‡ªåŠ¨è·å–ï¼‰"
                }),
                "prompt": ("STRING", {"default": "A beautiful mountain landscape at sunset", "multiline": True}),
                # æ”¯æŒå¤šç§AIæ¨¡å‹å’Œå›¾åƒç”ŸæˆæœåŠ¡: nano-bananaæ”¯æŒComflyå’ŒT8é•œåƒç«™
                "model": (["nano-banana [Comfly-T8]", "nano-banana-hd [Comfly-T8]", "gemini-2.5-flash-image-preview", "gemini-2.0-flash-preview-image-generation", "fal-ai/nano-banana [Comfly-T8]", "google/gemini-2.5-flash-image-preview [OpenRouter]"], {"default": "nano-banana [Comfly-T8]"}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "size": (size_presets, {"default": image_settings.get('default_size', "1024x1024")}),
                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),
                
                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),
                
                # ğŸš€ è´¨é‡å¢å¼ºæ§åˆ¶ç»„
                "quality_enhancement": ("BOOLEAN", {"default": True, "label": "Enable Quality Enhancement"}),
                "enhance_quality": ("BOOLEAN", {"default": True, "label": "Enhanced Image Quality"}),
                "smart_resize": ("BOOLEAN", {"default": True, "label": "Smart Resize with Padding"}),
                "fill_color": ("STRING", {
                    "default": "255,255,255",
                    "placeholder": "å¡«å……é¢œè‰² RGB (å¦‚: 255,255,255)"
                }),
                
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                "custom_size": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "placeholder": "è‡ªå®šä¹‰å°ºå¯¸ (å¦‚: 1920x1080)"
                }),
                "api4gpt_service": (["nano-banana"], {
                    "default": "nano-banana",
                    "tooltip": "API4GPTæœåŠ¡ç±»å‹é€‰æ‹©ï¼ˆä»…åœ¨API4GPTé•œåƒç«™æ—¶æœ‰æ•ˆï¼‰"
                }),
                
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {"unique_id": "UNIQUE_ID"}
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("image", "response_text")
    FUNCTION = "generate_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    
    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000
            
            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")
            
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt, 
                            "response": display_response, 
                            "response_id": str(random.randint(100000, 999999)), 
                            "timestamp": time.time()
                        }
                    ], ensure_ascii=False)
                },
            }
            PromptServer.instance.send_sync("display_component", render_spec)
        except Exception as e:
            print(f"[LLM Agent Assistant] Chat push failed: {e}")
            pass
    
    def generate_image(self, mirror_site: str, api_key: str, prompt: str, model: str, 
                      proxy: str, size: str, quality: str, style: str, detail_level: str, camera_control: str, 
                      lighting_control: str, template_selection: str, quality_enhancement: bool, enhance_quality: bool, 
                      smart_resize: bool, fill_color: str, temperature: float, top_p: float, top_k: int, 
                      max_output_tokens: int, seed: int, custom_size: str = "", api4gpt_service: str = "nano-banana", 
                      custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨é•œåƒç«™APIç”Ÿæˆå›¾ç‰‡"""
        
        # ğŸš€ ç«‹å³è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå»é™¤UIæ ‡è¯†
        model = _normalize_model_name(model)
        
        # æ ¹æ®é•œåƒç«™ä»é…ç½®è·å–URLå’ŒAPI Key
        site_config = get_mirror_site_config(mirror_site) if mirror_site else {"url": "", "api_key": ""}
        api_url = site_config.get("url", "").strip()
        if site_config.get("api_key") and not api_key.strip():
            api_key = site_config["api_key"]
            print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™API Key: {api_key[:8]}...")
        if not api_url:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘è¯¥é•œåƒç«™çš„API URL")
        print(f"ğŸ”— è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™URL: {api_url}")
        
        # éªŒè¯API URL
        if not validate_api_url(api_url):
            raise ValueError("API URLæ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†å›¾åƒæ§åˆ¶å‚æ•°
        try:
            from .gemini_banana import process_image_controls, enhance_prompt_with_controls
        except ImportError:
            from gemini_banana import process_image_controls, enhance_prompt_with_controls
        controls = process_image_controls(size, quality, style, custom_size)
        # å¯¹äºnano-bananaæ¨¡å‹ï¼Œè·³è¿‡å°ºå¯¸æç¤ºï¼Œè®©æ¨¡å‹è‡ªç”±ç”Ÿæˆ
        skip_size_hints = model in ["nano-banana", "nano-banana-hd"]
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(), controls, detail_level, camera_control, lighting_control,
            template_selection, quality_enhancement, enhance_quality, smart_resize, fill_color,
            skip_size_hints=skip_size_hints
        )
        
        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: å°ºå¯¸={controls['size']}, è´¨é‡={controls['quality']}, é£æ ¼={controls['style']}")
        if controls['is_custom_size']:
            print(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰å°ºå¯¸: {controls['size']}")
        
        # ä»£ç†å¤„ç†ï¼šæœ‰æ•ˆåˆ™è®¾ç½®ï¼Œæ— æ•ˆ/æœªå¡«åˆ™æ¸…é™¤ï¼Œé¿å…æ®‹ç•™ç¯å¢ƒå˜é‡å½±å“è¯·æ±‚
        if proxy and proxy.strip() and "None" not in proxy:
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
            print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
        else:
            existing = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
            if existing:
                print(f"ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼Œæ²¿ç”¨ç³»ç»Ÿä»£ç†: {existing}")
            else:
                print("ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼ˆç³»ç»Ÿæ— ä»£ç†ï¼‰")
        
        # æ„å»ºå®Œæ•´çš„API URL
        full_url = build_api_url(api_url, model)
        print(f"ğŸŒ ä½¿ç”¨APIåœ°å€: {full_url}")
        
        # æ£€æŸ¥é•œåƒç«™ç±»å‹ - æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        is_nano_banana_official = mirror_site == "nano-bananaå®˜æ–¹"
        is_t8_mirror = "t8star.cn" in api_url or "ai.t8star.cn" in api_url
        is_api4gpt_mirror = "www.api4gpt.com" in api_url
        is_comfly_mirror = _is_comfly_base(api_url)
        is_openrouter_mirror = "openrouter.ai" in api_url
        is_openai_mirror = "api.openai.com" in api_url or site_config.get("api_format") == "openai"
        
        # æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºå¤„ç†é•œåƒç«™ï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        
        # 1. nano-bananaå®˜æ–¹é•œåƒç«™å¤„ç†
        if is_nano_banana_official:
            print("ğŸ”— æ£€æµ‹åˆ°nano-bananaå®˜æ–¹é•œåƒç«™ï¼Œä½¿ç”¨Googleå®˜æ–¹API")

            # æ„å»ºå†…å®¹éƒ¨åˆ†
            content_parts = [{"text": enhanced_prompt}]

            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
                "responseModalities": ["TEXT", "IMAGE"]
            }

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            try:
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨ï¼ˆå®˜æ–¹APIä¼˜å…ˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST APIï¼‰
                response_json = generate_with_priority_api(
                    api_key=api_key,
                    model=_normalize_model_name(model),
                    content_parts=content_parts,
                    generation_config=generation_config,
                    max_retries=5,
                    proxy=proxy
                )

                if response_json:
                    # æå–ç”Ÿæˆçš„å›¾åƒ
                    generated_image = process_generated_image_from_response(response_json)
                    print(f"ğŸ” å›¾åƒæå–ç»“æœ: {generated_image is not None}")
                    if generated_image:
                        print(f"ğŸ” åŸå§‹å›¾åƒå°ºå¯¸: {generated_image.size}")

                    # æå–å“åº”æ–‡æœ¬
                    response_text = extract_text_from_response(response_json)

                    if generated_image:
                        # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                        print("ğŸš€ å¼€å§‹åº”ç”¨å›¾åƒå¢å¼ºæŠ€æœ¯...")
                        try:
                            enhanced_image = _apply_full_enhancements(
                                generated_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            if enhanced_image:
                                generated_image = enhanced_image
                                print(f"âœ… å›¾åƒå¢å¼ºå®Œæˆ")
                                try:
                                    print(f"ğŸ”§ Final output size: {generated_image.size[0]}x{generated_image.size[1]}")
                                except Exception:
                                    pass
                            else:
                                print("âš ï¸ å›¾åƒå¢å¼ºè¿”å›Noneï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                        except Exception as e:
                            print(f"âŒ å›¾åƒå¢å¼ºå¤±è´¥: {e}")
                            import traceback
                            print(f"ğŸ” å¢å¼ºé”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")

                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆnano-bananaå®˜æ–¹ï¼‰")
                        return (pil_to_tensor(generated_image), response_text)
                    else:
                        print("âš ï¸ nano-bananaå®˜æ–¹APIå“åº”ä¸­æœªæ‰¾åˆ°å›¾åƒæ•°æ®")
                        # è¿”å›é»˜è®¤å›¾åƒ
                        default_image = Image.new('RGB', (1024, 1024), color='black')
                        return (pil_to_tensor(default_image), response_text)
                else:
                    raise Exception("nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥")

            except Exception as e:
                print(f"âŒ nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
                raise e
            
        # 2. Comflyé•œåƒç«™å¤„ç†
        elif is_comfly_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°Comflyé•œåƒç«™ï¼Œä½¿ç”¨Comfly APIæ ¼å¼")

            if model in ["nano-banana", "nano-banana-hd"]:
                # Comfly nano-banana ç›´è¿ï¼ˆç”Ÿæˆï¼‰
                try:
                    result = _comfly_nano_banana_generate(api_url, api_key, model, enhanced_prompt, controls['size'], temperature, top_p, max_output_tokens, seed)
                    # print(f"[DEBUG] result type: {type(result)}")
                    # print(f"[DEBUG] result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
                    # print(f"[DEBUG] result content: {str(result)[:500]}...")
                    generated_image = None
                    response_text = ""
                    
                    if isinstance(result, dict) and 'data' in result:
                        if not result['data']:
                            # å¦‚æœdataä¸ºç©ºï¼Œè¯´æ˜æ²¡æœ‰ç”Ÿæˆå›¾åƒ
                            response_text = result.get('response_text', '')
                            print(f"âš ï¸ Comfly nano-banana æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®")
                            print(f"ğŸ“ å“åº”æ–‡æœ¬: {response_text[:200]}...")
                            raise Exception(f"Comfly nano-banana æœåŠ¡æ²¡æœ‰è¿”å›å›¾åƒæ•°æ®ï¼Œå“åº”: {response_text[:100]}...")

                        # å¦‚æœdataä¸ä¸ºç©ºï¼Œç»§ç»­å¤„ç†
                        if result['data']:
                            b64 = result['data'][0].get('b64_json')
                            image_url = result['data'][0].get('url', '')
                            response_text = result.get('response_text', "")

                            # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                            if image_url and image_url not in response_text:
                                response_text += f"\nå›¾åƒURL: {image_url}"

                            # print(f"[DEBUG] å›¾åƒURL: {image_url}")
                            # print(f"[DEBUG] å“åº”æ–‡æœ¬: {response_text}")

                            if b64:
                                from base64 import b64decode
                                import io
                                try:
                                    # ä¿®å¤base64å¡«å……é—®é¢˜
                                    b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                    img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                                except Exception as decode_error:
                                    print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                    # å°è¯•ç›´æ¥è§£ç 
                                    try:
                                        img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                    except Exception as e2:
                                        print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                        img = None
                                generated_image = img
                            else:
                                # å¦‚æœæ²¡æœ‰base64æ•°æ®ï¼Œå°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å›¾åƒ
                                import re
                                base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                                matches = re.findall(base64_pattern, response_text)
                                if matches:
                                    from base64 import b64decode
                                    import io
                                    img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                    generated_image = img
                    
                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨å°ºå¯¸ä¸è´¨é‡å¢å¼ºåå†è¿”å›
                    if generated_image:
                        # å…¨é‡å¢å¼º
                        try:
                            generated_image = _apply_full_enhancements(
                                generated_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {generated_image.size[0]}x{generated_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass

                        image_tensor = pil_to_tensor(generated_image)
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆComfly nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)
                        
                except Exception as e:
                    print(f"âŒ Comfly(nano-banana) ç”Ÿæˆå¤±è´¥: {e}")
                    raise e
            else:
                # énano-bananaæ¨¡å‹ä½¿ç”¨Gemini APIæ ¼å¼
                request_data = {
                "model": model,  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„modelå­—æ®µ
                "contents": [{
                    "parts": [{"text": enhanced_prompt}]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }

                # æ¸…ç† None å€¼
                if request_data["generationConfig"]["seed"] is None:
                    del request_data["generationConfig"]["seed"]

                headers = {
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {api_key.strip()}"
                }

        # 3. T8é•œåƒç«™å¤„ç†
        elif is_t8_mirror:
            # T8é•œåƒç«™ä¹Ÿæ”¯æŒ nano-bananaï¼Œè°ƒç”¨æ–¹å¼ä¸ Comfly ä¸€è‡´
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨chat/completionsç«¯ç‚¹ (nano-banana ç›´è¿)")
            if _normalize_model_name(model) in ["nano-banana", "nano-banana-hd"]:
                try:
                    result = _comfly_nano_banana_generate(api_url, api_key, _normalize_model_name(model), enhanced_prompt, controls['size'], temperature, top_p, max_output_tokens, seed)
                    # ğŸ” è°ƒè¯•ï¼šæ‰“å°T8è¿”å›çš„ç»“æœæ ¼å¼ - å·²å…³é—­
                    # print(f"[DEBUG] T8 result type: {type(result)}")
                    # print(f"[DEBUG] T8 result keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
                    # print(f"[DEBUG] T8 result content: {str(result)[:500]}...")

                    generated_image = None
                    response_text = ""
                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        image_url = result['data'][0].get('url', '')
                        response_text = result.get('response_text', "")

                        # ç¡®ä¿å›¾åƒURLä¿¡æ¯æ˜¾ç¤ºåœ¨å“åº”ä¸­
                        if image_url and image_url not in response_text:
                            response_text += f"\nå›¾åƒURL: {image_url}"

                        # print(f"[DEBUG] T8 å›¾åƒURL: {image_url}")
                        # print(f"[DEBUG] T8 å“åº”æ–‡æœ¬: {response_text}")

                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception:
                                img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                            generated_image = img
                    if generated_image:
                        # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                        try:
                            generated_image = _apply_full_enhancements(
                                generated_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {generated_image.size[0]}x{generated_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆT8 nano-bananaï¼‰")
                        return (pil_to_tensor(generated_image), response_text)
                except Exception as e:
                    print(f"âŒ T8(nano-banana) ç”Ÿæˆå¤±è´¥: {e}")
                    raise e
            # å…¶ä»–æ¨¡å‹ä»èµ°åŸ T8 OpenAI å…¼å®¹æ ¼å¼
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨OpenAIå…¼å®¹çš„APIæ ¼å¼")
            request_data = build_t8_api_request(
                model=_normalize_model_name(model),
                prompt=enhanced_prompt,
                temperature=temperature,
                max_tokens=max_output_tokens
            )
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
            
        # 4. API4GPTé•œåƒç«™å¤„ç†
        elif is_api4gpt_mirror:
            # API4GPTé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°API4GPTé•œåƒç«™")
            
            # ä½¿ç”¨API4GPTæœåŠ¡æ¥å£è¿›è¡Œå›¾åƒç”Ÿæˆ
            print("ğŸ”— ä½¿ç”¨API4GPTæœåŠ¡æ¥å£è¿›è¡Œå›¾åƒç”Ÿæˆ")
            
            if api4gpt_service == "nano-banana":
                # nano-bananaæœåŠ¡ä½¿ç”¨å®˜æ–¹API4GPTæ ¼å¼
                print("ğŸ”— ä½¿ç”¨API4GPT nano-bananaæœåŠ¡è¿›è¡Œå›¾åƒç”Ÿæˆ")
                
                # æ„å»ºAPI4GPTè¯·æ±‚æ•°æ®
                request_data = build_api4gpt_request(
                    service_type=api4gpt_service,
                    model=_normalize_model_name(model),
                    prompt=enhanced_prompt,
                    size=controls['size'],
                    quality=controls['quality'],
                    style=controls['style'],
                    temperature=temperature,
                    max_tokens=max_output_tokens
                )
                
                # è°ƒç”¨API4GPT API
                try:
                    result = call_api4gpt_api(api_url, api_key, api4gpt_service, request_data)
                    print("âœ… API4GPT APIè°ƒç”¨æˆåŠŸ")
                    
                    # è§£æAPI4GPTå“åº”
                    response_text, generated_image = parse_api4gpt_response(result, api4gpt_service)
                    
                    if generated_image:
                        print(f"âœ… æˆåŠŸæå–API4GPTç”Ÿæˆçš„å›¾åƒ")
                    else:
                        print("âš ï¸ API4GPTæœªè¿”å›å›¾åƒï¼Œåˆ›å»ºå ä½ç¬¦")
                        generated_image = Image.new('RGB', (512, 512), color='lightgray')
                        if not response_text:
                            response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›å›¾åƒæ•°æ®"
                    
                    # å¤„ç†å›¾åƒå°ºå¯¸è°ƒæ•´
                    if generated_image:
                        try:
                            target_size = controls['size']
                            if 'x' in target_size:
                                target_width, target_height = map(int, target_size.split('x'))
                                
                                if generated_image.size != (target_width, target_height):
                                    print(f"ğŸ”§ ä½¿ç”¨æ‰©å›¾æŠ€æœ¯è°ƒæ•´å›¾åƒå°ºå¯¸: {generated_image.size} -> {target_size}")
                                    
                                    # ğŸš€ ä¼˜åŒ–ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œæ™ºèƒ½è£å‰ªç¡®ä¿ä¸»ä½“å±…ä¸­
                                    if generated_image.size[0]/generated_image.size[1] != target_width/target_height:
                                        print(f"ğŸ“ æ£€æµ‹åˆ°æ¯”ä¾‹å˜åŒ–ï¼Œä½¿ç”¨é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªæ–¹æ³•")
                                        
                                        # ğŸ¯ é«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œä¸æ‹‰ä¼¸å˜å½¢ï¼‰
                                        # è®¡ç®—æœ€ä½³ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ç›®æ ‡åŒºåŸŸ
                                        scale_x = target_width / generated_image.size[0]      # å®½åº¦æ¯”ä¾‹
                                        scale_y = target_height / generated_image.size[1]    # é«˜åº¦æ¯”ä¾‹
                                        scale = max(scale_x, scale_y)  # ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
                                        
                                        # è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œç¡®ä¿è¦†ç›–ç›®æ ‡åŒºåŸŸï¼‰
                                        enlarged_width = int(generated_image.size[0] * scale)
                                        enlarged_height = int(generated_image.size[1] * scale)
                                        
                                        print(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§: {generated_image.size[0]}x{generated_image.size[1]} -> {enlarged_width}x{enlarged_height}")
                                        print(f"ğŸ”§ ç¼©æ”¾æ¯”ä¾‹: {scale:.3f} (ä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ï¼Œç„¶åæ™ºèƒ½è£å‰ª)")
                                        print(f"ğŸ”§ å…³é”®ï¼šç›´æ¥æ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œä¿æŒå›¾åƒæ¸…æ™°åº¦å’Œæ¯”ä¾‹")
                                        
                                        # ğŸ¯ ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒæ¯”ä¾‹ï¼‰
                                        # ä¼˜å…ˆä½¿ç”¨AIæ¨¡å‹ï¼Œå›é€€åˆ°é«˜è´¨é‡é‡é‡‡æ ·
                                        try:
                                            print(f"ğŸ”§ å°è¯•ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ”¾å¤§...")
                                            ai_upscaled_image = smart_ai_upscale(generated_image, enlarged_width, enlarged_height)
                                            
                                            if ai_upscaled_image is not None:
                                                # å¦‚æœAIæ”¾å¤§æˆåŠŸï¼Œè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
                                                if ai_upscaled_image.size != (enlarged_width, enlarged_height):
                                                    print(f"ğŸ”§ AIæ”¾å¤§åè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸: {ai_upscaled_image.size} -> {enlarged_width}x{enlarged_height}")
                                                    enlarged_image = ai_upscaled_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                                else:
                                                    enlarged_image = ai_upscaled_image
                                                print(f"âœ… AIæ”¾å¤§æ¨¡å‹æ”¾å¤§å®Œæˆï¼Œå›¾åƒè´¨é‡å¤§å¹…æå‡")
                                            else:
                                                print(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
                                                enlarged_image = generated_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                                
                                        except Exception as e:
                                            print(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·: {e}")
                                            # å›é€€åˆ° LANCZOS ç®—æ³•
                                            enlarged_image = generated_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                        
                                        # ğŸ¯ æ™ºèƒ½è£å‰ª - ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
                                        if enlarged_width >= target_width and enlarged_height >= target_height:
                                            print(f"ğŸ”§ æ™ºèƒ½è£å‰ªï¼šä»é«˜æ¸…æ”¾å¤§å›¾åƒä¸­è£å‰ªç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å®Œå…¨å±…ä¸­")
                                            
                                            # ğŸ¯ æ ¸å¿ƒåˆ›æ–°ï¼šæ™ºèƒ½ä¸»ä½“æ£€æµ‹ + ç²¾ç¡®å±…ä¸­è£å‰ª
                                            print(f"ğŸ” å¼€å§‹æ™ºèƒ½ä¸»ä½“æ£€æµ‹...")
                                            
                                            try:
                                                # ä½¿ç”¨æ™ºèƒ½ä¸»ä½“æ£€æµ‹ç®—æ³•
                                                subject_bbox, subject_center = detect_image_foreground_subject(enlarged_image)
                                                subject_x, subject_y, subject_w, subject_h = subject_bbox
                                                subject_center_x, subject_center_y = subject_center
                                                
                                                print(f"ğŸ¯ æ£€æµ‹åˆ°ä¸»ä½“: ä½ç½®({subject_x}, {subject_y}), å°ºå¯¸({subject_w}x{subject_h})")
                                                print(f"ğŸ¯ ä¸»ä½“ä¸­å¿ƒ: ({subject_center_x}, {subject_center_y})")
                                                
                                                # ğŸ¯ æ™ºèƒ½è£å‰ªç­–ç•¥ï¼šç¡®ä¿ä¸»ä½“åœ¨è£å‰ªåå›¾åƒçš„ä¸­å¿ƒ
                                                # è®¡ç®—ä¸»ä½“ä¸­å¿ƒåº”è¯¥åœ¨æ–°å›¾åƒä¸­çš„ä½ç½®
                                                target_center_x = target_width // 2
                                                target_center_y = target_height // 2
                                                
                                                # è®¡ç®—è£å‰ªèµ·å§‹ä½ç½®ï¼Œä½¿ä¸»ä½“ä¸­å¿ƒå¯¹é½åˆ°ç›®æ ‡ä¸­å¿ƒ
                                                crop_x = subject_center_x - target_center_x
                                                crop_y = subject_center_y - target_center_y
                                                
                                                print(f"ğŸ”§ æ™ºèƒ½è®¡ç®—è£å‰ªä½ç½®: ä¸»ä½“ä¸­å¿ƒ({subject_center_x}, {subject_center_y}) -> ç›®æ ‡ä¸­å¿ƒ({target_center_x}, {target_center_y})")
                                                print(f"ğŸ”§ ç†è®ºè£å‰ªä½ç½®: ({crop_x}, {crop_y})")
                                                
                                                # ğŸ¯ è¾¹ç•Œæ£€æŸ¥å’Œè°ƒæ•´
                                                # ç¡®ä¿è£å‰ªåŒºåŸŸä¸è¶…å‡ºå›¾åƒè¾¹ç•Œ
                                                if crop_x < 0:
                                                    print(f"ğŸ”§ è°ƒæ•´ï¼šè£å‰ªXåæ ‡è¿‡å°ï¼Œè°ƒæ•´ä¸º0")
                                                    crop_x = 0
                                                elif crop_x + target_width > enlarged_width:
                                                    print(f"ğŸ”§ è°ƒæ•´ï¼šè£å‰ªXåæ ‡è¿‡å¤§ï¼Œè°ƒæ•´ä¸º{enlarged_width - target_width}")
                                                    crop_x = enlarged_width - target_width
                                                
                                                if crop_y < 0:
                                                    print(f"ğŸ”§ è°ƒæ•´ï¼šè£å‰ªYåæ ‡è¿‡å°ï¼Œè°ƒæ•´ä¸º0")
                                                    crop_y = 0
                                                elif crop_y + target_height > enlarged_height:
                                                    print(f"ğŸ”§ è°ƒæ•´ï¼šè£å‰ªYåæ ‡è¿‡å¤§ï¼Œè°ƒæ•´ä¸º{enlarged_height - target_height}")
                                                    crop_y = enlarged_height - target_height
                                                
                                                print(f"ğŸ”§ æœ€ç»ˆè£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
                                                print(f"ğŸ”§ å›¾åƒå°ºå¯¸: {enlarged_width}x{enlarged_height}")
                                                print(f"ğŸ”§ ç›®æ ‡å°ºå¯¸: {target_width}x{target_height}")
                                                print(f"ğŸ”§ æ™ºèƒ½ç­–ç•¥ï¼šä¸»ä½“ç²¾ç¡®å±…ä¸­ï¼Œç¡®ä¿å®Œå…¨å¯è§")
                                                
                                                # ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
                                                generated_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                                                
                                            except Exception as e:
                                                print(f"âš ï¸ æ™ºèƒ½ä¸»ä½“æ£€æµ‹å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨è£å‰ªç­–ç•¥")
                                                # å¤‡ç”¨ç­–ç•¥ï¼šä»ä¸­å¿ƒè£å‰ª
                                                crop_x = (enlarged_width - target_width) // 2
                                                crop_y = (enlarged_height - target_height) // 2
                                                
                                                # è¾¹ç•Œæ£€æŸ¥
                                                if crop_x < 0: crop_x = 0
                                                if crop_y < 0: crop_y = 0
                                                if crop_x + target_width > enlarged_width: crop_x = enlarged_width - target_width
                                                if crop_y + target_height > enlarged_height: crop_y = enlarged_height - target_height
                                                
                                                print(f"ğŸ”§ å¤‡ç”¨ç­–ç•¥ï¼šä¸­å¿ƒè£å‰ªåŒºåŸŸ ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
                                                generated_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                                            
                                            print(f"âœ… é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªå®Œæˆ")
                                            print(f"âœ… ç»“æœï¼šæ— ç™½è‰²å¡«å……ï¼Œå®Œå…¨ä¸å˜å½¢ï¼Œä¸»ä½“ç²¾ç¡®å±…ä¸­ï¼Œä¿æŒæœ€é«˜æ¸…æ™°åº¦")
                                            print(f"âœ… å›¾åƒè´¨é‡ï¼šé«˜æ¸…æ— æŸï¼Œæ¯”ä¾‹å®Œç¾ï¼Œä¸»ä½“å¯è§")
                                            
                                        else:
                                            print(f"âš ï¸ é«˜æ¸…æ”¾å¤§åå°ºå¯¸ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……ï¼ˆé¿å…æ‹‰ä¼¸å˜å½¢ï¼‰")
                                            # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œä½¿ç”¨å¡«å……è‰²
                                            new_image = Image.new('RGB', (target_width, target_height), (255, 255, 255))
                                            
                                            # å°†é«˜æ¸…æ”¾å¤§çš„å›¾åƒå±…ä¸­æ”¾ç½®
                                            paste_x = (target_width - enlarged_width) // 2
                                            paste_y = (target_height - enlarged_height) // 2
                                            new_image.paste(enlarged_image, (paste_x, paste_y))
                                            
                                            generated_image = new_image
                                            print(f"âœ… æ™ºèƒ½å¡«å……å®Œæˆï¼šé«˜æ¸…æ”¾å¤§å›¾åƒå±…ä¸­æ”¾ç½®ï¼Œè¾¹ç¼˜ç”¨å¡«å……è‰²")
                                        
                                    else:
                                        # æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸ï¼ˆæ— å˜å½¢ï¼‰
                                        print(f"ğŸ“ æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸ï¼ˆæ— å˜å½¢ï¼‰")
                                        generated_image = generated_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
                                    
                                    print(f"âœ… å›¾åƒå°ºå¯¸è°ƒæ•´å®Œæˆ: {generated_image.size}")
                        except Exception as e:
                            print(f"âš ï¸ å›¾åƒå°ºå¯¸è°ƒæ•´å¤±è´¥: {e}, ä¿æŒåŸå§‹å°ºå¯¸")
                    
                    # åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼ˆå…³é—­è‡ªé€‚åº”ï¼‰
                    if enhance_quality and quality in ['hd', 'ultra_hd']:
                        print(f"âœ¨ åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼Œè´¨é‡ç­‰çº§: {quality}")
                        generated_image = enhance_image_quality(generated_image, quality, "disabled")
                        print(f"âœ… å¢å¼ºå®Œæˆ")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(generated_image)
                    print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆAPI4GPTï¼‰")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)
                    
                except Exception as e:
                    print(f"âŒ API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
                    raise ValueError(f"API4GPT APIè°ƒç”¨å¤±è´¥: {e}")
                    
            else:
                # å…¶ä»–API4GPTæœåŠ¡ç»´æŒåŸæœ‰é€»è¾‘
                    print(f"ğŸ”— ä½¿ç”¨API4GPT {api4gpt_service} æœåŠ¡")
                    request_data = build_api4gpt_request(
                        service_type=api4gpt_service,
                        model=_normalize_model_name(model),
                        prompt=enhanced_prompt,
                        size=controls['size'],
                        quality=controls['quality'],
                        style=controls['style'],
                        temperature=temperature,
                        max_tokens=max_output_tokens
                    )
                    headers = {
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {api_key.strip()}"
                    }
        elif is_openrouter_mirror:
            # OpenRouteré•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenRouteré•œåƒç«™ï¼Œä½¿ç”¨OpenRouter APIæ ¼å¼")
            
            # éªŒè¯OpenRouteré…ç½®
            validation = validate_openrouter_config(api_url, api_key, _normalize_model_name(model))
            if not validation["is_valid"]:
                raise ValueError(f"OpenRouteré…ç½®éªŒè¯å¤±è´¥: {'; '.join(validation['warnings'])}")
            
            if validation["warnings"]:
                for warning in validation["warnings"]:
                    print(f"âš ï¸ OpenRouteré…ç½®è­¦å‘Š: {warning}")
            
            if validation["suggestions"]:
                for suggestion in validation["suggestions"]:
                    print(f"ğŸ’¡ OpenRouterä¼˜åŒ–å»ºè®®: {suggestion}")
            
            # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç”Ÿæˆå’Œç¼–è¾‘
            # æ„å»ºOpenAIå…¼å®¹çš„è¯·æ±‚æ ¼å¼
            content = []
            
            # å¦‚æœæœ‰è¾“å…¥å›¾åƒï¼Œæ·»åŠ å›¾åƒå†…å®¹
            if hasattr(self, 'input_image') and self.input_image is not None:
                # å°†å›¾åƒè½¬æ¢ä¸ºbase64
                pil_image = tensor_to_pil(self.input_image)
                image_base64 = image_to_base64(pil_image)
                image_url = f"data:image/png;base64,{image_base64}"
                
                content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            
            # æ·»åŠ æ–‡æœ¬æŒ‡ä»¤
            enhanced_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual image, not just text description.

Task: {enhanced_prompt}

REQUIREMENTS:
1. GENERATE a new image based on my request
2. DO NOT just describe what the image should look like
3. RETURN the actual image file/data
4. The output MUST be a visual image, not text

Execute the image generation/editing task now and return the generated image."""
            
            content.append({
                "type": "text",
                "text": enhanced_instruction
            })
            
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": content
                }],
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_output_tokens,
                "stream": True  # Required for gemini-2.5-flash-image-preview
            }
            
            # æ¸…ç† None å€¼å’Œç©ºå­—ç¬¦ä¸²
            request_data = {k: v for k, v in request_data.items() if v is not None and v != ""}
            
            # ä½¿ç”¨OpenRouterçš„chat/completionsç«¯ç‚¹
            # å¯¹äºOpenRouterï¼Œapi_urlå·²ç»åŒ…å«äº†/v1ï¼Œæ‰€ä»¥ç›´æ¥æ·»åŠ /chat/completions
            if api_url.endswith('/v1'):
                full_url = f"{api_url}/chat/completions"
            else:
                full_url = f"{api_url}/v1/chat/completions"
            print(f"ğŸ”— ä½¿ç”¨OpenRouter chat/completionsç«¯ç‚¹: {full_url}")
            
            # è®¾ç½®OpenRouterè¯·æ±‚å¤´ - å®Œç¾æ”¯æŒåº”ç”¨å½’å› 
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}",
                "HTTP-Referer": "https://github.com/ComfyUI-LLM-Prompt",
                "X-Title": "ComfyUI LLM Prompt Plugin"
            }
            
            print(f"ğŸ¯ OpenRouterè¯·æ±‚æ•°æ®: {json.dumps(request_data, ensure_ascii=False, indent=2)}")
            print(f"ğŸ”§ æ¨¡å‹ç±»å‹: {model}, ä½¿ç”¨chat/completionsç«¯ç‚¹")
        elif is_openai_mirror:
            # OpenAIé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": enhanced_prompt
                }],
                "temperature": temperature,
                "max_tokens": max_output_tokens,
                "stream": True
            }
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        else:
            # æ ‡å‡†Gemini APIæ ¼å¼
            request_data = {
                "contents": [{
                    "parts": [{
                        "text": enhanced_prompt
                    }]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        
        # æ™ºèƒ½é‡è¯•æœºåˆ¶ - å®Œå…¨ç§»æ¤å‚è€ƒé¡¹ç›®
        max_retries = 5
        timeout = 120
        
        # OpenRouterä¸“ç”¨é‡è¯•ç­–ç•¥
        if is_openrouter_mirror:
            print("ğŸ”„ ä½¿ç”¨OpenRouterä¸“ç”¨é‡è¯•ç­–ç•¥")
            # OpenRouterå¯èƒ½éœ€è¦æ›´é•¿çš„è¶…æ—¶æ—¶é—´
            timeout = 180
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ¨ æ­£åœ¨ç”Ÿæˆå›¾ç‰‡... (å°è¯• {attempt + 1}/{max_retries})")
                print(f"ğŸ“ æç¤ºè¯: {enhanced_prompt[:100]}...")
                print(f"ğŸ”— é•œåƒç«™: {api_url}")
                
                # å‘é€è¯·æ±‚
                response = requests.post(full_url, headers=headers, json=request_data, timeout=timeout, stream=True)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
                print(f"ğŸ“¡ å“åº”å¤´: {dict(response.headers)}")
                
                # OpenRouterä¸“ç”¨é”™è¯¯å¤„ç†
                if is_openrouter_mirror and response.status_code != 200:
                    if response.status_code == 401:
                        raise ValueError("OpenRouter APIå¯†é’¥æ— æ•ˆæˆ–å·²è¿‡æœŸ")
                    elif response.status_code == 403:
                        raise ValueError("OpenRouter APIè®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥è´¦æˆ·æƒé™")
                    elif response.status_code == 429:
                        raise ValueError("OpenRouter APIè¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åé‡è¯•")
                    elif response.status_code == 500:
                        raise ValueError("OpenRouteræœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•")
                    else:
                        raise ValueError(f"OpenRouter APIé”™è¯¯: HTTP {response.status_code}")
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    # æå–æ–‡æœ¬å“åº”å’Œå›¾ç‰‡
                    response_text = ""
                    generated_image = None
                    
                    if is_api4gpt_mirror:
                        # API4GPTé•œåƒç«™å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†API4GPTé•œåƒç«™å“åº”")
                        
                        try:
                            # å°è¯•è§£æJSONå“åº”
                            result = response.json()
                            print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")
                            
                            if api4gpt_service == "nano-banana":
                                # nano-bananaä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
                                response_text, generated_image = parse_openai_compatible_response(result)
                            else:
                                # å…¶ä»–æœåŠ¡ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
                                response_text, generated_image = parse_api4gpt_response(result, api4gpt_service)
                            
                            if generated_image:
                                print(f"âœ… æˆåŠŸæå–API4GPTç”Ÿæˆçš„å›¾åƒ")
                            else:
                                print("âš ï¸ API4GPTæœªè¿”å›å›¾åƒï¼Œåˆ›å»ºå ä½ç¬¦")
                                generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                if not response_text:
                                    response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›å›¾åƒæ•°æ®"
                        except Exception as json_error:
                            print(f"âš ï¸ API4GPT JSONè§£æå¤±è´¥: {json_error}")
                            print(f"ğŸ“‹ åŸå§‹å“åº”å†…å®¹: {response.text[:500]}...")
                            generated_image = Image.new('RGB', (512, 512), color='lightgray')
                            response_text = f"API4GPTå“åº”è§£æå¤±è´¥: {json_error}"
                    elif is_openrouter_mirror:
                        # OpenRouteré•œåƒç«™å“åº”å¤„ç† - ä½¿ç”¨æµå¼å“åº”
                        print("ğŸ”— å¤„ç†OpenRouteré•œåƒç«™æµå¼å“åº”")
                        
                        # å¤„ç†æµå¼å“åº”
                        response_text = process_openrouter_stream(response)
                        
                        # æ£€æŸ¥å“åº”æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                        if "data:image/" in response_text:
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouterè¿”å›çš„å›¾åƒæ•°æ®")
                            try:
                                # ä½¿ç”¨æ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–base64å›¾åƒæ•°æ®
                                import re
                                base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                image_matches = re.findall(base64_pattern, response_text)
                                if image_matches:
                                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾åƒæ•°æ®
                                    image_url = image_matches[0]
                                    print(f"ğŸ¯ æˆåŠŸåŒ¹é…OpenRouterå›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_url)}å­—ç¬¦")
                                    
                                    # æå–base64éƒ¨åˆ†
                                    if ';base64,' in image_url:
                                        import io
                                        base64_data = image_url.split(';base64,', 1)[1]
                                        image_bytes = base64.b64decode(base64_data)
                                        generated_image = Image.open(io.BytesIO(image_bytes))
                                        print(f"âœ… æˆåŠŸæå–OpenRouterç”Ÿæˆçš„å›¾åƒ: {generated_image.size}")
                                        
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(base64_pattern, '[å›¾åƒå·²ç”Ÿæˆ]', response_text)
                                    else:
                                        print(f"âš ï¸ å›¾åƒæ•°æ®æ ¼å¼ä¸æ­£ç¡®: {image_url[:100]}...")
                                        generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                        response_text = f"OpenRouterå›¾åƒç”Ÿæˆå®Œæˆï¼Œä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                                else:
                                    print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾åƒæ•°æ®")
                                    generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                    response_text = f"OpenRouterå›¾åƒç”Ÿæˆå®Œæˆï¼Œä½†æœªæ‰¾åˆ°å›¾åƒæ•°æ®"
                            except Exception as e:
                                print(f"âš ï¸ OpenRouterå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                response_text = f"OpenRouterå›¾åƒç”Ÿæˆå®Œæˆï¼Œä½†è§£æå¤±è´¥: {e}"
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œåˆ›å»ºå ä½ç¬¦
                        if not generated_image:
                            print("âš ï¸ OpenRouteræœªè¿”å›å›¾åƒæ•°æ®ï¼Œåˆ›å»ºå ä½ç¬¦")
                            generated_image = Image.new('RGB', (512, 512), color='lightgray')
                            if not response_text:
                                response_text = "OpenRouterå›¾åƒç”Ÿæˆå®Œæˆï¼Œä½†æœªè¿”å›å›¾åƒæ•°æ®"
                    elif is_t8_mirror:
                        # T8é•œåƒç«™OpenAIæ ¼å¼å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†T8é•œåƒç«™OpenAIæ ¼å¼å“åº”")
                        
                        try:
                            # å°è¯•è§£æJSONå“åº”
                            result = response.json()
                            print(f"ğŸ“‹ T8é•œåƒç«™å“åº”ç»“æ„: {list(result.keys())}")
                            
                            if "choices" in result and result["choices"]:
                                choice = result["choices"][0]
                                if "message" in choice and "content" in choice["message"]:
                                    content = choice["message"]["content"]
                                    if isinstance(content, str):
                                        response_text = content
                                        # æ£€æŸ¥æ˜¯å¦åŒ…å«base64å›¾åƒæ•°æ®
                                        if "![image](data:image/" in content:
                                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°T8é•œåƒç«™è¿”å›çš„å›¾åƒæ•°æ®")
                                            try:
                                                # æå–base64å›¾åƒæ•°æ®
                                                import re, io, io
                                                image_match = re.search(r'!\[image\]\(data:image/\w+;base64,([^)]+)\)', content)
                                                if image_match:
                                                    image_data = image_match.group(1)
                                                    image_bytes = base64.b64decode(image_data)
                                                    generated_image = Image.open(io.BytesIO(image_bytes))
                                                    print("âœ… æˆåŠŸæå–T8é•œåƒç«™ç”Ÿæˆçš„å›¾åƒ")
                                                    # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                                    response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç”Ÿæˆ]', content)
                                            except Exception as e:
                                                print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                    elif isinstance(content, list):
                                        # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                                        for item in content:
                                            if item.get("type") == "text":
                                                response_text += item.get("text", "")
                                            elif item.get("type") == "image_url":
                                                # å¤„ç†å›¾åƒURLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                                                print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒURLå“åº”")
                            
                            # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œå†æ£€æŸ¥å…¶ä»–å¯èƒ½çš„ä½ç½®
                            if not generated_image:
                                print("âš ï¸ T8é•œåƒç«™æš‚ä¸æ”¯æŒå›¾åƒç”Ÿæˆï¼Œåˆ›å»ºå ä½ç¬¦")
                                generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                if not response_text:
                                    response_text = "T8é•œåƒç«™æ–‡æœ¬ç”Ÿæˆå®Œæˆï¼Œä½†æš‚ä¸æ”¯æŒå›¾åƒç”ŸæˆåŠŸèƒ½"
                        except Exception as json_error:
                            print(f"âš ï¸ T8é•œåƒç«™ JSONè§£æå¤±è´¥: {json_error}")
                            print(f"ğŸ“‹ åŸå§‹å“åº”å†…å®¹: {response.text[:500]}...")
                            generated_image = Image.new('RGB', (512, 512), color='lightgray')
                            response_text = f"T8é•œåƒç«™å“åº”è§£æå¤±è´¥: {json_error}"
                            
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
                        request_data = {
                            "model": _normalize_model_name(model),
                            "messages": [{
                                "role": "user",
                                "content": enhanced_prompt
                            }],
                            "temperature": temperature,
                            "max_tokens": max_output_tokens,
                            "stream": True
                        }
                        
                        # è®¾ç½®è¯·æ±‚å¤´
                        headers = {
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {api_key.strip()}"
                        }
                    else:
                        # æ ‡å‡†Gemini APIå“åº”å¤„ç†
                        try:
                            result = response.json()
                        except Exception as e:
                            print(f"âš ï¸ æ ‡å‡†Gemini JSONè§£æå¤±è´¥: {e}")
                            result = {}
                        if "candidates" in result and result["candidates"]:
                            candidate = result["candidates"][0]
                            if "content" in candidate and "parts" in candidate["content"]:
                                for part in candidate["content"]["parts"]:
                                    # æå–æ–‡æœ¬
                                    if "text" in part:
                                        response_text += part["text"]
                                    
                                    # æå–å›¾ç‰‡
                                    if "inline_data" in part or "inlineData" in part:
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                import io  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„ioæ¨¡å—å¯¼å…¥
                                                image_bytes = base64.b64decode(inline_data["data"])
                                                generated_image = Image.open(io.BytesIO(image_bytes))
                                                print(f"âœ… æˆåŠŸæå–æ ‡å‡†Geminiç”Ÿæˆçš„å›¾åƒ: {generated_image.size}")
                                            except Exception as e:
                                                print(f"âš ï¸ æ ‡å‡†Geminiå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                                generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                                if not response_text:
                                                    response_text = f"æ ‡å‡†Geminiå›¾åƒç”Ÿæˆå®Œæˆï¼Œä½†è§£æå¤±è´¥: {e}"
                            else:
                                # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œåˆ›å»ºå ä½ç¬¦
                                if not generated_image:
                                    print("âš ï¸ æ ‡å‡†Geminiæœªè¿”å›å›¾åƒæ•°æ®ï¼Œåˆ›å»ºå ä½ç¬¦")
                                    generated_image = Image.new('RGB', (512, 512), color='lightgray')
                                    if not response_text:
                                        response_text = "æ ‡å‡†Geminiå›¾åƒç”Ÿæˆå®Œæˆï¼Œä½†æœªè¿”å›å›¾åƒæ•°æ®"
                    
                    # å¦‚æœæ²¡æœ‰ç”Ÿæˆå›¾ç‰‡ï¼Œåˆ›å»ºå ä½ç¬¦
                    if generated_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç”Ÿæˆçš„å›¾ç‰‡ï¼Œåˆ›å»ºå ä½ç¬¦")
                        generated_image = Image.new('RGB', (512, 512), color='lightgray')
                        if not response_text:
                            response_text = "å›¾ç‰‡ç”Ÿæˆè¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°å›¾ç‰‡æ•°æ®"
                    
                    # ğŸš€ é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªè°ƒæ•´å›¾åƒå°ºå¯¸
                    try:
                        size_value = controls['size']
                        if 'x' in size_value:
                            target_width, target_height = map(int, size_value.split('x'))
                        else:
                            target_width, target_height = generated_image.size
                        current_width, current_height = generated_image.size
                        
                        if (current_width, current_height) != (target_width, target_height):
                            print(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§è°ƒæ•´å›¾åƒå°ºå¯¸: {current_width}x{current_height} -> {target_width}x{target_height}")
                            
                            # ğŸš€ ä¼˜åŒ–ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œæ™ºèƒ½è£å‰ªç¡®ä¿ä¸»ä½“å±…ä¸­
                            if current_width/current_height != target_width/target_height:
                                print(f"ğŸ“ æ£€æµ‹åˆ°æ¯”ä¾‹å˜åŒ–ï¼Œä½¿ç”¨é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªæ–¹æ³•")
                                
                                # ğŸ¯ é«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œä¸æ‹‰ä¼¸å˜å½¢ï¼‰
                                # è®¡ç®—æœ€ä½³ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ç›®æ ‡åŒºåŸŸ
                                scale_x = target_width / current_width      # å®½åº¦æ¯”ä¾‹
                                scale_y = target_height / current_height    # é«˜åº¦æ¯”ä¾‹
                                scale = max(scale_x, scale_y)  # ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
                                
                                # è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œç¡®ä¿è¦†ç›–ç›®æ ‡åŒºåŸŸï¼‰
                                enlarged_width = int(current_width * scale)
                                enlarged_height = int(current_height * scale)
                                
                                print(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§: {current_width}x{current_height} -> {enlarged_width}x{enlarged_height}")
                                print(f"ğŸ”§ ç¼©æ”¾æ¯”ä¾‹: {scale:.3f} (ä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ï¼Œç„¶åæ™ºèƒ½è£å‰ª)")
                                print(f"ğŸ”§ å…³é”®ï¼šç›´æ¥æ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œä¿æŒå›¾åƒæ¸…æ™°åº¦å’Œæ¯”ä¾‹")
                                
                                # ğŸ¯ ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒæ¯”ä¾‹ï¼‰
                                # ä¼˜å…ˆä½¿ç”¨AIæ¨¡å‹ï¼Œå›é€€åˆ°é«˜è´¨é‡é‡é‡‡æ ·
                                try:
                                    print(f"ğŸ”§ å°è¯•ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ”¾å¤§...")
                                    ai_upscaled_image = smart_ai_upscale(generated_image, enlarged_width, enlarged_height)
                                    
                                    if ai_upscaled_image is not None:
                                        # å¦‚æœAIæ”¾å¤§æˆåŠŸï¼Œè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
                                        if ai_upscaled_image.size != (enlarged_width, enlarged_height):
                                            print(f"ğŸ”§ AIæ”¾å¤§åè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸: {ai_upscaled_image.size} -> {enlarged_width}x{enlarged_height}")
                                            enlarged_image = ai_upscaled_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                        else:
                                            enlarged_image = ai_upscaled_image
                                        print(f"âœ… AIæ”¾å¤§æ¨¡å‹æ”¾å¤§å®Œæˆï¼Œå›¾åƒè´¨é‡å¤§å¹…æå‡")
                                    else:
                                        print(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
                                        enlarged_image = generated_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                        
                                except Exception as e:
                                    print(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·: {e}")
                                    # å›é€€åˆ° LANCZOS ç®—æ³•
                                    enlarged_image = generated_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                
                                # ğŸ¯ æ™ºèƒ½è£å‰ª - ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
                                if enlarged_width >= target_width and enlarged_height >= target_height:
                                    print(f"ğŸ”§ æ™ºèƒ½è£å‰ªï¼šä»é«˜æ¸…æ”¾å¤§å›¾åƒä¸­è£å‰ªç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å±…ä¸­")
                                    
                                    # ğŸ¯ ç²¾ç¡®è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“å®Œå…¨å±…ä¸­
                                    crop_x = (enlarged_width - target_width) // 2
                                    crop_y = (enlarged_height - target_height) // 2
                                    
                                    # ğŸ¯ å¾®è°ƒåç§»ï¼Œç¡®ä¿å®Œå…¨å±…ä¸­ï¼ˆé¿å…å¥‡æ•°åƒç´ åå·®ï¼‰
                                    if (enlarged_width - target_width) % 2 == 1:
                                        crop_x += 1
                                    if (enlarged_height - target_height) % 2 == 1:
                                        crop_y += 1
                                    
                                    print(f"ğŸ”§ ç²¾ç¡®å±…ä¸­è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
                                    print(f"ğŸ”§ ç¡®ä¿ä¸»ä½“åœ¨è£å‰ªåå›¾åƒçš„æ­£ä¸­å¿ƒä½ç½®")
                                    
                                    # ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
                                    generated_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                                    
                                    print(f"âœ… é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªå®Œæˆ")
                                    print(f"âœ… ç»“æœï¼šæ— ç™½è‰²å¡«å……ï¼Œå®Œå…¨ä¸å˜å½¢ï¼Œä¸»ä½“ç²¾ç¡®å±…ä¸­ï¼Œä¿æŒæœ€é«˜æ¸…æ™°åº¦")
                                    print(f"âœ… å›¾åƒè´¨é‡ï¼šé«˜æ¸…æ— æŸï¼Œæ¯”ä¾‹å®Œç¾ï¼Œä¸»ä½“å¯è§")
                                    
                                else:
                                    print(f"âš ï¸ é«˜æ¸…æ”¾å¤§åå°ºå¯¸ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……ï¼ˆé¿å…æ‹‰ä¼¸å˜å½¢ï¼‰")
                                    # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œä½¿ç”¨å¡«å……è‰²
                                    new_image = Image.new('RGB', (target_width, target_height), (255, 255, 255))
                                    
                                    # å°†é«˜æ¸…æ”¾å¤§çš„å›¾åƒå±…ä¸­æ”¾ç½®
                                    paste_x = (target_width - enlarged_width) // 2
                                    paste_y = (target_height - enlarged_height) // 2
                                    new_image.paste(enlarged_image, (paste_x, paste_y))
                                    
                                    generated_image = new_image
                                    print(f"âœ… æ™ºèƒ½å¡«å……å®Œæˆï¼šé«˜æ¸…æ”¾å¤§å›¾åƒå±…ä¸­æ”¾ç½®ï¼Œè¾¹ç¼˜ç”¨å¡«å……è‰²")
                                
                            else:
                                # æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸ï¼ˆæ— å˜å½¢ï¼‰
                                print(f"ğŸ“ æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸ï¼ˆæ— å˜å½¢ï¼‰")
                                generated_image = generated_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
                        
                        print(f"âœ… å›¾åƒå°ºå¯¸è°ƒæ•´å®Œæˆ: {generated_image.size}")
                    except Exception as e:
                        print(f"âš ï¸ å›¾åƒå°ºå¯¸è°ƒæ•´å¤±è´¥: {e}, ä¿æŒåŸå§‹å°ºå¯¸")
                    
                    # åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼ˆå…³é—­è‡ªé€‚åº”ï¼‰
                    if enhance_quality and quality in ['hd', 'ultra_hd']:
                        print(f"âœ¨ åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼Œè´¨é‡ç­‰çº§: {quality}")
                        generated_image = enhance_image_quality(generated_image, quality, "disabled")
                        print(f"âœ… å¢å¼ºå®Œæˆ")
                    
                    # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
                    if not response_text:
                        response_text = "å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„æç¤ºè¯ç”Ÿæˆçš„å›¾åƒã€‚"
                        print("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(generated_image)

                    # ğŸ”§ ä¿®å¤ï¼šæ ¹æ®å®é™…é•œåƒç«™æ˜¾ç¤ºæ­£ç¡®çš„å®Œæˆæ¶ˆæ¯
                    if is_comfly_mirror:
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆComflyï¼‰")
                    elif is_t8_mirror:
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆT8ï¼‰")
                    elif is_api4gpt_mirror:
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆAPI4GPTï¼‰")
                    elif is_openrouter_mirror:
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆOpenRouterï¼‰")
                    elif is_openai_mirror:
                        print("âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆOpenAIï¼‰")
                    else:
                        print(f"âœ… å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼ˆ{mirror_site}ï¼‰")

                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    # æ™ºèƒ½ç­‰å¾…
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å›¾ç‰‡ç”Ÿæˆå¤±è´¥: {error_msg}")



    def get_mirror_config(self):
        """è·å–å½“å‰é•œåƒç«™é…ç½®"""
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        mirror_sites = config.get('mirror_sites', {})
        
        # æŸ¥æ‰¾API4GPTé•œåƒç«™é…ç½®
        for site_name, site_config in mirror_sites.items():
            if "api4gpt.com" in site_config.get("url", ""):
                return site_config
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤é…ç½®
        return {
            "url": "https://www.api4gpt.com",
            "api_key": "",
            "api_format": "api4gpt"
        }


class KenChenLLMGeminiBananaMirrorImageEditNode:
    """Gemini Banana é•œåƒç«™å›¾ç‰‡ç¼–è¾‘èŠ‚ç‚¹
    
    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒé€‰æ‹©é¢„é…ç½®çš„é•œåƒç«™ï¼ˆofficial, comfly, customï¼‰
    - è‡ªåŠ¨å¡«å……å¯¹åº”é•œåƒç«™çš„ API URL å’Œ API Key
    - é€‰æ‹© custom æ—¶å¯æ‰‹åŠ¨è¾“å…¥è‡ªå®šä¹‰é•œåƒç«™ä¿¡æ¯
    - æ™ºèƒ½URLæ ¼å¼éªŒè¯å’Œè‡ªåŠ¨è¡¥å…¨
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        image_settings = config.get('image_settings', {})
        
        # è·å–é•œåƒç«™é…ç½®
        mirror_sites = config.get('mirror_sites', {})
        mirror_options = list(mirror_sites.keys())
        if not mirror_options:
            mirror_options = ["official", "comfly", "custom"]
        
        # è·å–é»˜è®¤é•œåƒç«™é…ç½®
        default_site = "comfly" if "comfly" in mirror_options else mirror_options[0] if mirror_options else "official"
        default_config = get_mirror_site_config(default_site)
        
        # ğŸš€ è¶…è¶Šå‚è€ƒé¡¹ç›®çš„å›¾åƒæ§åˆ¶é¢„è®¾
        size_presets = image_settings.get('size_presets', [
            "Original size", "512x512", "768x768", "1024x1024", "1024x1792", "1792x1024",
            "1920x1080", "2560x1440", "3840x2160"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é«˜åˆ†è¾¨ç‡é€‰é¡¹
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„è¶…é«˜æ¸…é€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "mirror_site": (mirror_options, {"default": default_site}),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "é•œåƒç«™APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç•™ç©ºæ—¶è‡ªåŠ¨è·å–ï¼‰"
                }),
                "image": ("IMAGE",),
                "prompt": ("STRING", {"default": "Can you add a llama next to me?", "multiline": True}),
                # æ”¯æŒå¤šç§AIæ¨¡å‹å’Œå›¾åƒç¼–è¾‘æœåŠ¡: nano-bananaæ”¯æŒComflyå’ŒT8é•œåƒç«™, OpenRouteræ¨¡å‹: google/gemini-2.5-flash-image-preview (ä»˜è´¹)
                "model": (["nano-banana [Comfly-T8]", "nano-banana-hd [Comfly-T8]", "gemini-2.5-flash-image-preview", "gemini-2.0-flash-preview-image-generation", "fal-ai/nano-banana/edit [Comfly-T8]", "google/gemini-2.5-flash-image-preview [OpenRouter]"], {"default": "nano-banana [Comfly-T8]"}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "size": (size_presets, {"default": image_settings.get('default_size', "1024x1024")}),
                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),
                
                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),
                
                # ğŸš€ è´¨é‡å¢å¼ºæ§åˆ¶ç»„
                "quality_enhancement": ("BOOLEAN", {"default": True, "label": "Enable Quality Enhancement"}),
                "enhance_quality": ("BOOLEAN", {"default": True, "label": "Enhanced Image Quality"}),
                "smart_resize": ("BOOLEAN", {"default": True, "label": "Smart Resize with Padding"}),
                "fill_color": ("STRING", {
                    "default": "255,255,255",
                    "placeholder": "å¡«å……é¢œè‰² RGB (å¦‚: 255,255,255)"
                }),
                
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                "custom_size": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "placeholder": "è‡ªå®šä¹‰å°ºå¯¸ (å¦‚: 1920x1080)"
                }),
                "api4gpt_service": (["nano-banana"], {
                    "default": "nano-banana",
                    "tooltip": "API4GPTæœåŠ¡ç±»å‹é€‰æ‹©ï¼ˆä»…åœ¨API4GPTé•œåƒç«™æ—¶æœ‰æ•ˆï¼‰"
                }),
                
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {"unique_id": "UNIQUE_ID"}
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    
    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000
            
            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")
            
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt, 
                            "response": display_response, 
                            "response_id": str(random.randint(100000, 999999)), 
                            "timestamp": time.time()
                        }
                    ], ensure_ascii=False)
                },
            }
            PromptServer.instance.send_sync("display_component", render_spec)
        except Exception as e:
            print(f"[LLM Agent Assistant] Chat push failed: {e}")
            pass
    
    def edit_image(self, mirror_site: str, api_key: str, image: torch.Tensor, prompt: str, model: str,
                    proxy: str, size: str, quality: str, style: str, detail_level: str, camera_control: str, lighting_control: str, 
                    template_selection: str, quality_enhancement: bool, enhance_quality: bool, smart_resize: bool, 
                    fill_color: str, temperature: float, top_p: float, top_k: int, max_output_tokens: int, seed: int, 
                    custom_size: str = "", api4gpt_service: str = "nano-banana", custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨é•œåƒç«™APIç¼–è¾‘å›¾ç‰‡"""
        
        # ğŸš€ ç«‹å³è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå»é™¤UIæ ‡è¯†
        model = _normalize_model_name(model)
        
        # æ ¹æ®é•œåƒç«™ä»é…ç½®è·å–URLå’ŒAPI Key
        site_config = get_mirror_site_config(mirror_site) if mirror_site else {"url": "", "api_key": ""}
        api_url = site_config.get("url", "").strip()
        if site_config.get("api_key") and not api_key.strip():
            api_key = site_config["api_key"]
            print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™API Key: {api_key[:8]}...")
        if not api_url:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘è¯¥é•œåƒç«™çš„API URL")
        print(f"ğŸ”— è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™URL: {api_url}")
        
        if not validate_api_url(api_url):
            raise ValueError("API URLæ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†å›¾åƒæ§åˆ¶å‚æ•°
        try:
            from .gemini_banana import process_image_controls, enhance_prompt_with_controls
        except ImportError:
            from gemini_banana import process_image_controls, enhance_prompt_with_controls
        controls = process_image_controls(size, quality, style, custom_size)
        # å¯¹äºnano-bananaæ¨¡å‹ï¼Œè·³è¿‡å°ºå¯¸æç¤ºï¼Œè®©æ¨¡å‹è‡ªç”±ç”Ÿæˆ
        skip_size_hints = model in ["nano-banana", "nano-banana-hd"]
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(), controls, detail_level, camera_control, lighting_control,
            template_selection, quality_enhancement, enhance_quality, smart_resize, fill_color,
            skip_size_hints=skip_size_hints
        )
        
        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: å°ºå¯¸={controls['size']}, è´¨é‡={controls['quality']}, é£æ ¼={controls['style']}")
        if controls['is_custom_size']:
            print(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰å°ºå¯¸: {controls['size']}")
        
        # è½¬æ¢è¾“å…¥å›¾ç‰‡
        pil_image = tensor_to_pil(image)
        
        # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥ç¬¦åˆAPIè¦æ±‚
        pil_image = resize_image_for_api(pil_image)
        
        # è½¬æ¢ä¸ºbase64
        image_base64 = image_to_base64(pil_image, format='JPEG')
        
        # ä»£ç†å¤„ç†ï¼šæœ‰æ•ˆåˆ™è®¾ç½®ï¼Œæ— æ•ˆ/æœªå¡«åˆ™æ¸…é™¤ï¼Œé¿å…æ®‹ç•™ç¯å¢ƒå˜é‡å½±å“è¯·æ±‚
        if proxy and proxy.strip() and "None" not in proxy:
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
            print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
        else:
            existing = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
            if existing:
                print(f"ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼Œæ²¿ç”¨ç³»ç»Ÿä»£ç†: {existing}")
            else:
                print("ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼ˆç³»ç»Ÿæ— ä»£ç†ï¼‰")
        
        # æ£€æŸ¥é•œåƒç«™ç±»å‹ - æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        is_nano_banana_official = mirror_site == "nano-bananaå®˜æ–¹"
        is_t8_mirror = "t8star.cn" in api_url or "ai.t8star.cn" in api_url
        is_api4gpt_mirror = "www.api4gpt.com" in api_url
        is_comfly_mirror = _is_comfly_base(api_url)
        is_openrouter_mirror = "openrouter.ai" in api_url
        is_openai_mirror = "api.openai.com" in api_url or site_config.get("api_format") == "openai"
        
        # æ„å»ºå®Œæ•´çš„API URLï¼ˆOpenRouteré™¤å¤–ï¼Œå› ä¸ºå®ƒåœ¨å„è‡ªçš„å¤„ç†é€»è¾‘ä¸­æ„å»ºï¼‰
        if not is_openrouter_mirror:
            full_url = build_api_url(api_url, model)
            print(f"ğŸŒ ä½¿ç”¨APIåœ°å€: {full_url}")
        else:
            print(f"ğŸŒ OpenRouteré•œåƒç«™ï¼ŒURLå°†åœ¨OpenRouterå¤„ç†é€»è¾‘ä¸­æ„å»º")
        
        # æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºå¤„ç†é•œåƒç«™ï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        
        # 1. nano-bananaå®˜æ–¹é•œåƒç«™å¤„ç†
        if is_nano_banana_official:
            print("ğŸ”— æ£€æµ‹åˆ°nano-bananaå®˜æ–¹é•œåƒç«™ï¼Œä½¿ç”¨Googleå®˜æ–¹API")

            # æ„å»ºå†…å®¹éƒ¨åˆ†ï¼ˆæ–‡æœ¬ + å›¾åƒï¼‰
            content_parts = [
                {"text": enhanced_prompt},
                {
                    "inline_data": {
                        "mime_type": "image/jpeg",
                        "data": image_base64
                    }
                }
            ]

            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
                "responseModalities": ["TEXT", "IMAGE"]
            }

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            try:
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨ï¼ˆå®˜æ–¹APIä¼˜å…ˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST APIï¼‰
                response_json = generate_with_priority_api(
                    api_key=api_key,
                    model=_normalize_model_name(model),
                    content_parts=content_parts,
                    generation_config=generation_config,
                    max_retries=5,
                    proxy=proxy
                )

                if response_json:
                    # æå–ç¼–è¾‘åçš„å›¾åƒ
                    edited_image = process_generated_image_from_response(response_json)

                    # æå–å“åº”æ–‡æœ¬
                    response_text = extract_text_from_response(response_json)

                    if edited_image:
                        # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                        try:
                            edited_image = _apply_full_enhancements(
                                edited_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆnano-bananaå®˜æ–¹ï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)
                    else:
                        print("âš ï¸ nano-bananaå®˜æ–¹APIå“åº”ä¸­æœªæ‰¾åˆ°ç¼–è¾‘åçš„å›¾åƒæ•°æ®")
                        # è¿”å›åŸå›¾åƒ
                        return (image, response_text)
                else:
                    raise Exception("nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥")

            except Exception as e:
                print(f"âŒ nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
                raise e
            
        # 2. Comflyé•œåƒç«™å¤„ç†
        elif is_comfly_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°Comflyé•œåƒç«™ï¼Œä½¿ç”¨Comfly APIæ ¼å¼")
            
            if model in ["nano-banana", "nano-banana-hd"] and pil_image is not None:
                # Comfly nano-banana ç›´è¿ï¼ˆç¼–è¾‘ï¼‰
                try:
                    result = _comfly_nano_banana_edit(api_url, api_key, model, enhanced_prompt, [pil_image], controls['size'], temperature, top_p, max_output_tokens, seed)
                    edited_image = None
                    response_text = ""
                    
                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        response_text = result.get('response_text', "")
                        
                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                # ä¿®å¤base64å¡«å……é—®é¢˜
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception as decode_error:
                                print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                # å°è¯•ç›´æ¥è§£ç 
                                try:
                                    img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                except Exception as e2:
                                    print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                    img = None
                            edited_image = img
                        else:
                            # å¦‚æœæ²¡æœ‰base64æ•°æ®ï¼Œå°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å›¾åƒ
                            import re
                            base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                            matches = re.findall(base64_pattern, response_text)
                            if matches:
                                from base64 import b64decode
                                import io
                                img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                edited_image = img
                    
                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨å°ºå¯¸ä¸è´¨é‡å¢å¼ºåå†è¿”å›
                    if edited_image:
                        # å…¨é‡å¢å¼º
                        try:
                            edited_image = _apply_full_enhancements(
                                edited_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆComfly nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)
                        
                except Exception as e:
                    print(f"âŒ Comfly(nano-banana) ç¼–è¾‘å¤±è´¥: {e}")
                    raise e
            request_data = {
                "model": model,  # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„modelå­—æ®µ
                "contents": [{
                    "parts": [
                        {"text": enhanced_prompt},
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": image_base64
                            }
                        }
                    ]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
                
        # 3. T8é•œåƒç«™å¤„ç†
        elif is_t8_mirror:
            # T8é•œåƒç«™ä¹Ÿæ”¯æŒ nano-bananaï¼Œè°ƒç”¨æ–¹å¼ä¸ Comfly ä¸€è‡´
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨chat/completionsç«¯ç‚¹ (nano-banana ç›´è¿)")
            if _normalize_model_name(model) in ["nano-banana", "nano-banana-hd"] and pil_image is not None:
                try:
                    result = _comfly_nano_banana_edit(full_url, api_key, _normalize_model_name(model), enhanced_prompt, [pil_image], controls['size'], temperature, top_p, max_output_tokens, seed)
                    edited_image = None
                    response_text = ""

                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        response_text = result.get('response_text', "")

                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception:
                                img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                            edited_image = img

                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨å°ºå¯¸ä¸è´¨é‡å¢å¼ºåå†è¿”å›
                    if edited_image:
                        # å…¨é‡å¢å¼º
                        try:
                            edited_image = _apply_full_enhancements(
                                edited_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆT8 nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, _make_chat_summary(response_text or ""), unique_id)
                        return (image_tensor, response_text)

                except Exception as e:
                    print(f"âŒ T8(nano-banana) ç¼–è¾‘å¤±è´¥: {e}")
                    raise e
            # å…¶ä»–æ¨¡å‹ä»èµ°åŸ T8 OpenAI å…¼å®¹æ ¼å¼
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨OpenAIå…¼å®¹çš„APIæ ¼å¼")
            request_data = build_t8_api_request(
                model=_normalize_model_name(model),
                prompt=enhanced_prompt,
                image_base64=image_base64,
                temperature=temperature,
                max_tokens=max_output_tokens
            )
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
            
        # 4. API4GPTé•œåƒç«™å¤„ç†
        elif is_api4gpt_mirror:
            # API4GPTé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°API4GPTé•œåƒç«™")
            
            # ä½¿ç”¨API4GPTæœåŠ¡æ¥å£è¿›è¡Œå›¾åƒç¼–è¾‘
            print("ğŸ”— ä½¿ç”¨API4GPTæœåŠ¡æ¥å£è¿›è¡Œå›¾åƒç¼–è¾‘")
            
            if api4gpt_service == "nano-banana":
                # nano-bananaæœåŠ¡ä½¿ç”¨å®˜æ–¹API4GPTæ ¼å¼è¿›è¡Œå›¾åƒç¼–è¾‘
                print("ğŸ”— ä½¿ç”¨API4GPT nano-bananaæœåŠ¡è¿›è¡Œå›¾åƒç¼–è¾‘")
                
                # æ„å»ºAPI4GPTè¯·æ±‚æ•°æ®ï¼ˆåŒ…å«å›¾åƒï¼‰
                request_data = build_api4gpt_request(
                    service_type=api4gpt_service,
                    model=_normalize_model_name(model),
                    prompt=enhanced_prompt,
                    image_base64=image_base64,  # ä¼ å…¥è¾“å…¥å›¾åƒ
                    size=controls['size'],
                    quality=controls['quality'],
                    style=controls['style'],
                    temperature=temperature,
                    max_tokens=max_output_tokens
                )
                
                # è°ƒç”¨API4GPT APIè¿›è¡Œå›¾åƒç¼–è¾‘
                try:
                    result = call_api4gpt_api(api_url, api_key, api4gpt_service, request_data)
                    print("âœ… API4GPTå›¾åƒç¼–è¾‘APIè°ƒç”¨æˆåŠŸ")
                    
                    # è§£æAPI4GPTå“åº”
                    response_text, edited_image = parse_api4gpt_response(result, api4gpt_service)
                    
                    if edited_image:
                        print(f"âœ… æˆåŠŸæå–API4GPTç¼–è¾‘åçš„å›¾åƒ")
                    else:
                        print("âš ï¸ API4GPTæœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = pil_image
                        if not response_text:
                            response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    
                    # å¤„ç†å›¾åƒå°ºå¯¸è°ƒæ•´
                    if edited_image:
                        try:
                            target_size = controls['size']
                            if 'x' in target_size:
                                target_width, target_height = map(int, target_size.split('x'))
                                
                                if edited_image.size != (target_width, target_height):
                                    print(f"ğŸ”§ ä½¿ç”¨æ‰©å›¾æŠ€æœ¯è°ƒæ•´å›¾åƒå°ºå¯¸: {edited_image.size} -> {target_size}")
                                    
                                    # ğŸš€ ä¼˜åŒ–ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œæ™ºèƒ½è£å‰ªç¡®ä¿ä¸»ä½“å±…ä¸­
                                    if edited_image.size[0]/edited_image.size[1] != target_width/target_height:
                                        print(f"ğŸ“ æ£€æµ‹åˆ°æ¯”ä¾‹å˜åŒ–ï¼Œä½¿ç”¨é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªæ–¹æ³•")
                                        
                                        # ğŸ¯ é«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œä¸æ‹‰ä¼¸å˜å½¢ï¼‰
                                        # è®¡ç®—æœ€ä½³ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ç›®æ ‡åŒºåŸŸ
                                        scale_x = target_width / edited_image.size[0]      # å®½åº¦æ¯”ä¾‹
                                        scale_y = target_height / edited_image.size[1]    # é«˜åº¦æ¯”ä¾‹
                                        scale = max(scale_x, scale_y)  # ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
                                        
                                        # è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œç¡®ä¿è¦†ç›–ç›®æ ‡åŒºåŸŸï¼‰
                                        enlarged_width = int(edited_image.size[0] * scale)
                                        enlarged_height = int(edited_image.size[1] * scale)
                                        
                                        print(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§: {edited_image.size[0]}x{edited_image.size[1]} -> {enlarged_width}x{enlarged_height}")
                                        print(f"ğŸ”§ ç¼©æ”¾æ¯”ä¾‹: {scale:.3f} (ä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ï¼Œç„¶åæ™ºèƒ½è£å‰ª)")
                                        print(f"ğŸ”§ å…³é”®ï¼šç›´æ¥æ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œä¿æŒå›¾åƒæ¸…æ™°åº¦å’Œæ¯”ä¾‹")
                                        
                                        # ğŸ¯ ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒæ¯”ä¾‹ï¼‰
                                        # ä¼˜å…ˆä½¿ç”¨AIæ¨¡å‹ï¼Œå›é€€åˆ°é«˜è´¨é‡é‡é‡‡æ ·
                                        try:
                                            print(f"ğŸ”§ å°è¯•ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ”¾å¤§...")
                                            ai_upscaled_image = smart_ai_upscale(edited_image, enlarged_width, enlarged_height)
                                            
                                            if ai_upscaled_image is not None:
                                                # å¦‚æœAIæ”¾å¤§æˆåŠŸï¼Œè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
                                                if ai_upscaled_image.size != (enlarged_width, enlarged_height):
                                                    print(f"ğŸ”§ AIæ”¾å¤§åè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸: {ai_upscaled_image.size} -> {enlarged_width}x{enlarged_height}")
                                                    enlarged_image = ai_upscaled_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                                else:
                                                    enlarged_image = ai_upscaled_image
                                                print(f"âœ… AIæ”¾å¤§æ¨¡å‹æ”¾å¤§å®Œæˆï¼Œå›¾åƒè´¨é‡å¤§å¹…æå‡")
                                            else:
                                                print(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
                                                enlarged_image = edited_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                                
                                        except Exception as e:
                                            print(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·: {e}")
                                            # å›é€€åˆ° LANCZOS ç®—æ³•
                                            enlarged_image = edited_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                                        
                                        # ğŸ¯ æ™ºèƒ½è£å‰ª - ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
                                        if enlarged_width >= target_width and enlarged_height >= target_height:
                                            print(f"ğŸ”§ æ™ºèƒ½è£å‰ªï¼šä»é«˜æ¸…æ”¾å¤§å›¾åƒä¸­è£å‰ªç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å±…ä¸­")
                                            
                                            # ğŸ¯ ç²¾ç¡®è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“å®Œå…¨å±…ä¸­
                                            crop_x = (enlarged_width - target_width) // 2
                                            crop_y = (enlarged_height - target_height) // 2
                                            
                                            # ğŸ¯ å¾®è°ƒåç§»ï¼Œç¡®ä¿å®Œå…¨å±…ä¸­ï¼ˆé¿å…å¥‡æ•°åƒç´ åå·®ï¼‰
                                            if (enlarged_width - target_width) % 2 == 1:
                                                crop_x += 1
                                            if (enlarged_height - target_height) % 2 == 1:
                                                crop_y += 1
                                            
                                            print(f"ğŸ”§ ç²¾ç¡®å±…ä¸­è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
                                            print(f"ğŸ”§ ç¡®ä¿ä¸»ä½“åœ¨è£å‰ªåå›¾åƒçš„æ­£ä¸­å¿ƒä½ç½®")
                                            
                                            # ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
                                            edited_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
                                            
                                            print(f"âœ… é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªå®Œæˆ")
                                            print(f"âœ… ç»“æœï¼šæ— ç™½è‰²å¡«å……ï¼Œå®Œå…¨ä¸å˜å½¢ï¼Œä¸»ä½“ç²¾ç¡®å±…ä¸­ï¼Œä¿æŒæœ€é«˜æ¸…æ™°åº¦")
                                            print(f"âœ… å›¾åƒè´¨é‡ï¼šé«˜æ¸…æ— æŸï¼Œæ¯”ä¾‹å®Œç¾ï¼Œä¸»ä½“å¯è§")
                                            
                                        else:
                                            print(f"âš ï¸ é«˜æ¸…æ”¾å¤§åå°ºå¯¸ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……ï¼ˆé¿å…æ‹‰ä¼¸å˜å½¢ï¼‰")
                                            # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œä½¿ç”¨å¡«å……è‰²
                                            new_image = Image.new('RGB', (target_width, target_height), (255, 255, 255))
                                            
                                            # å°†é«˜æ¸…æ”¾å¤§çš„å›¾åƒå±…ä¸­æ”¾ç½®
                                            paste_x = (target_width - enlarged_width) // 2
                                            paste_y = (target_height - enlarged_height) // 2
                                            new_image.paste(enlarged_image, (paste_x, paste_y))
                                            
                                            edited_image = new_image
                                            print(f"âœ… æ™ºèƒ½å¡«å……å®Œæˆï¼šé«˜æ¸…æ”¾å¤§å›¾åƒå±…ä¸­æ”¾ç½®ï¼Œè¾¹ç¼˜ç”¨å¡«å……è‰²")
                                        
                                    else:
                                        # æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸ï¼ˆæ— å˜å½¢ï¼‰
                                        print(f"ğŸ“ æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸ï¼ˆæ— å˜å½¢ï¼‰")
                                        edited_image = edited_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
                        except Exception as e:
                            print(f"âš ï¸ å›¾åƒå°ºå¯¸è°ƒæ•´å¤±è´¥: {e}, ä¿æŒåŸå§‹å°ºå¯¸")
                    
                    # åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼ˆå…³é—­è‡ªé€‚åº”ï¼‰
                    if enhance_quality and quality in ['hd', 'ultra_hd']:
                        print(f"âœ¨ åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼Œè´¨é‡ç­‰çº§: {quality}")
                        edited_image = enhance_image_quality(edited_image, quality, "disabled")
                        print(f"âœ… å¢å¼ºå®Œæˆ")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆAPI4GPTï¼‰")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)
                    
                except Exception as e:
                    print(f"âŒ API4GPTå›¾åƒç¼–è¾‘APIè°ƒç”¨å¤±è´¥: {e}")
                    raise ValueError(f"API4GPTå›¾åƒç¼–è¾‘APIè°ƒç”¨å¤±è´¥: {e}")
                    
            else:
                # å…¶ä»–API4GPTæœåŠ¡ç»´æŒåŸæœ‰é€»è¾‘
                    print(f"ğŸ”— ä½¿ç”¨API4GPT {api4gpt_service} æœåŠ¡")
                    request_data = build_api4gpt_request(
                        service_type=api4gpt_service,
                        model=_normalize_model_name(model),
                        prompt=enhanced_prompt,
                        image_base64=image_base64,
                        size=controls['size'],
                        quality=controls['quality'],
                        style=controls['style'],
                        temperature=temperature,
                        max_tokens=max_output_tokens
                    )
                    headers = {
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {api_key.strip()}"
                    }
        elif is_openrouter_mirror:
            # OpenRouteré•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenRouteré•œåƒç«™ï¼Œä½¿ç”¨OpenRouter APIæ ¼å¼")
            
            # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç¼–è¾‘
            # æ„å»ºOpenAIå…¼å®¹çš„è¯·æ±‚æ ¼å¼
            content = []
            
            # æ·»åŠ å›¾åƒå†…å®¹
            image_url = f"data:image/jpeg;base64,{image_base64}"
            content.append({
                "type": "image_url",
                "image_url": {"url": image_url}
            })
            
            # æ·»åŠ æ–‡æœ¬æŒ‡ä»¤
            enhanced_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual edited image, not just text description.

Task: {enhanced_prompt}

REQUIREMENTS:
1. EDIT the provided image based on my request
2. DO NOT just describe what the edited image should look like
3. RETURN the actual edited image file/data
4. The output MUST be a visual image, not text

Execute the image editing task now and return the edited image."""
            
            content.append({
                "type": "text",
                "text": enhanced_instruction
            })
            
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": content
                }],
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_output_tokens,
                "stream": True  # Required for gemini-2.5-flash-image-preview
            }
            
            # ä½¿ç”¨OpenRouterçš„chat/completionsç«¯ç‚¹
            # å¯¹äºOpenRouterï¼Œapi_urlå·²ç»åŒ…å«äº†/v1ï¼Œæ‰€ä»¥ç›´æ¥æ·»åŠ /chat/completions
            if api_url.endswith('/v1'):
                full_url = f"{api_url}/chat/completions"
            else:
                full_url = f"{api_url}/v1/chat/completions"
            print(f"ğŸ”— ä½¿ç”¨OpenRouter chat/completionsç«¯ç‚¹è¿›è¡Œå›¾åƒç¼–è¾‘: {full_url}")
            
            # è®¾ç½®OpenRouterè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}",
                "HTTP-Referer": "https://github.com/ComfyUI-LLM-Prompt",
                "X-Title": "ComfyUI LLM Prompt Plugin"
            }
        elif is_openai_mirror:
            # OpenAIé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": enhanced_prompt
                }],
                "temperature": temperature,
                "max_tokens": max_output_tokens,
                "stream": True
            }
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        else:
            # æ ‡å‡†Gemini APIæ ¼å¼
            request_data = {
                "contents": [{
                    "parts": [
                        {
                            "text": enhanced_prompt
                        },
                        {
                            "inline_data": {
                                "mime_type": "image/jpeg",
                                "data": image_base64
                            }
                        }
                    ]
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        
        # æ™ºèƒ½é‡è¯•æœºåˆ¶ - å®Œå…¨ç§»æ¤å‚è€ƒé¡¹ç›®
        max_retries = 5
        timeout = 120
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç¼–è¾‘å›¾ç‰‡... (å°è¯• {attempt + 1}/{max_retries})")
                print(f"ğŸ“ ç¼–è¾‘æŒ‡ä»¤: {enhanced_prompt[:100]}...") # ä½¿ç”¨å¢å¼ºåçš„æç¤ºè¯
                print(f"ğŸ”— é•œåƒç«™: {api_url}")
                
                # å‘é€è¯·æ±‚
                response = requests.post(full_url, headers=headers, json=request_data, timeout=timeout, stream=True)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
                print(f"ğŸ“¡ å“åº”å¤´: {dict(response.headers)}")
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    # ä¸ºæ‰€æœ‰é•œåƒç«™å®šä¹‰ result å˜é‡
                    if not is_api4gpt_mirror and not is_openrouter_mirror and not is_t8_mirror and not is_openai_mirror:
                        # æ ‡å‡† Gemini API é•œåƒç«™ï¼ˆå¦‚ Comfy APIï¼‰
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£ææ ‡å‡† Gemini API å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ è§£æå“åº”å¤±è´¥: {e}")
                            result = {}
                    elif is_t8_mirror:
                        # T8 é•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ T8 é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ T8 é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ OpenAI é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ OpenAI é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    
                    if is_api4gpt_mirror:
                        # API4GPTé•œåƒç«™å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†API4GPTé•œåƒç«™å“åº”")
                        
                        try:
                            # å°è¯•è§£æJSONå“åº”
                            result = response.json()
                            print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")
                            
                            if api4gpt_service == "nano-banana":
                                # nano-bananaä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
                                response_text, edited_image = parse_openai_compatible_response(result)
                            else:
                                # å…¶ä»–æœåŠ¡ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
                                response_text, edited_image = parse_api4gpt_response(result, api4gpt_service)
                            
                            if edited_image:
                                print(f"âœ… æˆåŠŸæå–API4GPTç¼–è¾‘åçš„å›¾åƒ")
                            else:
                                print("âš ï¸ API4GPTæœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                                edited_image = pil_image
                                if not response_text:
                                    response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                        except Exception as json_error:
                            print(f"âš ï¸ API4GPT JSONè§£æå¤±è´¥: {json_error}")
                            print(f"ğŸ“‹ åŸå§‹å“åº”å†…å®¹: {response.text[:500]}...")
                            edited_image = pil_image
                            response_text = f"API4GPTå“åº”è§£æå¤±è´¥: {json_error}"
                    elif is_openrouter_mirror:
                        # OpenRouteré•œåƒç«™å“åº”å¤„ç† - ä½¿ç”¨æµå¼å“åº”
                        print("ğŸ”— å¤„ç†OpenRouteré•œåƒç«™æµå¼å“åº”")
                        
                        # å¤„ç†æµå¼å“åº”
                        response_text = process_openrouter_stream(response)
                        
                        # æ£€æŸ¥å“åº”æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                        if "data:image/" in response_text:
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouterè¿”å›çš„å›¾åƒæ•°æ®")
                            try:
                                # ä½¿ç”¨æ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–base64å›¾åƒæ•°æ®
                                import re
                                base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                image_matches = re.findall(base64_pattern, response_text)
                                if image_matches:
                                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾åƒæ•°æ®
                                    image_url = image_matches[0]
                                    print(f"ğŸ¯ æˆåŠŸåŒ¹é…OpenRouterå›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_url)}å­—ç¬¦")
                                    
                                    # æå–base64éƒ¨åˆ†
                                    if ';base64,' in image_url:
                                        import io
                                        base64_data = image_url.split(';base64,', 1)[1]
                                        image_bytes = base64.b64decode(base64_data)
                                        edited_image = Image.open(io.BytesIO(image_bytes))
                                        print(f"âœ… æˆåŠŸæå–OpenRouterç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")
                                        
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(base64_pattern, '[å›¾åƒå·²ç¼–è¾‘]', response_text)
                                    else:
                                        print(f"âš ï¸ å›¾åƒæ•°æ®æ ¼å¼ä¸æ­£ç¡®: {image_url[:100]}...")
                                        edited_image = pil_image
                                        response_text = f"OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                                else:
                                    print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾åƒæ•°æ®")
                                    edited_image = pil_image
                                    response_text = f"OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†æœªæ‰¾åˆ°å›¾åƒæ•°æ®"
                            except Exception as e:
                                print(f"âš ï¸ OpenRouterå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                edited_image = pil_image
                                response_text = f"OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†è§£æå¤±è´¥: {e}"
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡
                        if not edited_image:
                            print("âš ï¸ OpenRouteræœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®ï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = pil_image
                            if not response_text:
                                response_text = "OpenRouterå›¾åƒç¼–è¾‘å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    elif is_t8_mirror:
                        # T8é•œåƒç«™OpenAIæ ¼å¼å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†T8é•œåƒç«™OpenAIæ ¼å¼å“åº”")
                        if "choices" in result and result["choices"]:
                            choice = result["choices"][0]
                            if "message" in choice and "content" in choice["message"]:
                                content = choice["message"]["content"]
                                if isinstance(content, str):
                                    response_text = content
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«base64å›¾åƒæ•°æ®
                                    if "![image](data:image/" in content:
                                        print("ğŸ–¼ï¸ æ£€æµ‹åˆ°T8é•œåƒç«™è¿”å›çš„å›¾åƒæ•°æ®")
                                        try:
                                            # æå–base64å›¾åƒæ•°æ®
                                            import re, io
                                            image_match = re.search(r'!\[image\]\(data:image/\w+;base64,([^)]+)\)', content)
                                            if image_match:
                                                image_data = image_match.group(1)
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print("âœ… æˆåŠŸæå–T8é•œåƒç«™ç¼–è¾‘åçš„å›¾åƒ")
                                                # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                                response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç¼–è¾‘]', content)
                                        except Exception as e:
                                            print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                elif isinstance(content, list):
                                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                                    for item in content:
                                        if item.get("type") == "text":
                                            response_text += item.get("text", "")
                                        elif item.get("type") == "image_url":
                                            # å¤„ç†å›¾åƒURLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒURLå“åº”")
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œå†æ£€æŸ¥å…¶ä»–å¯èƒ½çš„ä½ç½®
                        if not edited_image:
                            print("âš ï¸ T8é•œåƒç«™æš‚ä¸æ”¯æŒå›¾åƒç¼–è¾‘ï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = pil_image
                            if not response_text:
                                response_text = "T8é•œåƒç«™æ–‡æœ¬åˆ†æå®Œæˆï¼Œä½†æš‚ä¸æ”¯æŒå›¾åƒç¼–è¾‘åŠŸèƒ½"
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
                        request_data = {
                            "model": _normalize_model_name(model),
                            "messages": [{
                                "role": "user",
                                "content": enhanced_prompt
                            }],
                            "temperature": temperature,
                            "max_tokens": max_output_tokens,
                            "stream": True
                        }
                        
                        # è®¾ç½®è¯·æ±‚å¤´
                        headers = {
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {api_key.strip()}"
                        }
                    else:
                        # æ ‡å‡†Gemini APIå“åº”å¤„ç†
                        try:
                            result = response.json()
                        except Exception as e:
                            print(f"âš ï¸ æ ‡å‡†Gemini JSONè§£æå¤±è´¥: {e}")
                            result = {}
                        if "candidates" in result and result["candidates"]:
                            candidate = result["candidates"][0]
                            if "content" in candidate and "parts" in candidate["content"]:
                                for part in candidate["content"]["parts"]:
                                    # æå–æ–‡æœ¬
                                    if "text" in part:
                                        response_text += part["text"]
                                    
                                    # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                    if "inline_data" in part or "inlineData" in part:
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                # è§£ç å›¾ç‰‡æ•°æ®
                                                import io
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                            except Exception as e:
                                                print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = pil_image
                        if not response_text:
                            response_text = "å›¾ç‰‡ç¼–è¾‘è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"
                    
                    # å¼ºåˆ¶è°ƒæ•´å›¾åƒå°ºå¯¸åˆ°ç”¨æˆ·æŒ‡å®šçš„å°ºå¯¸
                    try:
                        target_width, target_height = map(int, controls['size'].split('x'))
                        current_width, current_height = edited_image.size
                        
                        if (current_width, current_height) != (target_width, target_height):
                            print(f"ğŸ”„ å¼ºåˆ¶è°ƒæ•´å›¾åƒå°ºå¯¸: {current_width}x{current_height} -> {target_width}x{target_height}")
                            
                            # ğŸš€ ä½¿ç”¨æ— ç™½è‰²å¡«å……ä¸å˜å½¢çš„æ‰©å›¾æŠ€æœ¯
                            edited_image = smart_resize_with_padding(edited_image, (target_width, target_height))
                            
                            print(f"âœ… æ‰©å›¾æŠ€æœ¯å®Œæˆ: {edited_image.size}")
                        else:
                            print(f"âœ… å›¾åƒå°ºå¯¸å·²ç¬¦åˆè¦æ±‚: {edited_image.size}")
                            
                    except Exception as e:
                        print(f"âš ï¸ å°ºå¯¸è°ƒæ•´å¤±è´¥: {e}, ä¿æŒåŸå§‹å°ºå¯¸")
                    
                    # åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼ˆå…³é—­è‡ªé€‚åº”ï¼‰
                    if enhance_quality and quality in ['hd', 'ultra_hd']:
                        print(f"âœ¨ åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼Œè´¨é‡ç­‰çº§: {quality}")
                        edited_image = enhance_image_quality(edited_image, quality, "disabled")
                        print(f"âœ… å¢å¼ºå®Œæˆ")
                    
                    # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
                    if not response_text:
                        response_text = "å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„ç¼–è¾‘æŒ‡ä»¤ä¿®æ”¹åçš„å›¾åƒã€‚"
                        print("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆ")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id) # ä½¿ç”¨å¢å¼ºåçš„æç¤ºè¯
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢é”™è¯¯
                        if response.status_code == 429:
                            error_message = error_detail.get("error", {}).get("message", "")
                            if "quota" in error_message.lower():
                                print("âš ï¸ æ£€æµ‹åˆ°é…é¢é™åˆ¶é”™è¯¯ï¼Œå»ºè®®:")
                                print("   1. ç­‰å¾…æ›´é•¿æ—¶é—´å†è¯•")
                                print("   2. æ£€æŸ¥APIé…é¢è®¾ç½®")
                                print("   3. è€ƒè™‘å‡çº§APIè®¡åˆ’")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    # æ™ºèƒ½ç­‰å¾…
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å›¾ç‰‡ç¼–è¾‘å¤±è´¥: {error_msg}")



    def get_mirror_config(self):
        """è·å–å½“å‰é•œåƒç«™é…ç½®"""
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        mirror_sites = config.get('mirror_sites', {})
        
        # æŸ¥æ‰¾API4GPTé•œåƒç«™é…ç½®
        for site_name, site_config in mirror_sites.items():
            if "api4gpt.com" in site_config.get("url", ""):
                return site_config
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›é»˜è®¤é…ç½®
        return {
            "url": "https://www.api4gpt.com",
            "api_key": "",
            "api_format": "api4gpt"
        }


class KenChenLLMGeminiBananaMultiImageEditNode:
    """
    Gemini Banana å¤šå›¾åƒç¼–è¾‘èŠ‚ç‚¹
    
    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒå¤šå¼ è¾“å…¥å›¾åƒï¼ˆæœ€å¤š4å¼ ï¼‰
    - ä¸“ä¸šçš„å›¾åƒç¼–è¾‘æç¤ºè¯
    - æ”¯æŒå°ºå¯¸ã€è´¨é‡ã€é£æ ¼æ§åˆ¶
    - æ™ºèƒ½å›¾åƒç»„åˆç¼–è¾‘
    - æ”¯æŒå¤šä¸ªé•œåƒç«™
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        try:
            from .gemini_banana import get_gemini_banana_config
        except ImportError:
            from gemini_banana import get_gemini_banana_config
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        image_settings = config.get('image_settings', {})
        
        # è·å–é•œåƒç«™é…ç½®
        mirror_sites = config.get('mirror_sites', {})
        mirror_options = list(mirror_sites.keys())
        if not mirror_options:
            mirror_options = ["official", "comfly", "custom"]
        
        # è·å–é»˜è®¤é•œåƒç«™é…ç½®
        default_site = "comfly" if "comfly" in mirror_options else mirror_options[0] if mirror_options else "official"
        default_config = get_mirror_site_config(default_site)
        
        # ğŸš€ è¶…è¶Šå‚è€ƒé¡¹ç›®çš„å›¾åƒæ§åˆ¶é¢„è®¾
        size_presets = image_settings.get('size_presets', [
            "Original size", "512x512", "768x768", "1024x1024", "1024x1792", "1792x1024",
            "1920x1080", "2560x1440", "3840x2160"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é«˜åˆ†è¾¨ç‡é€‰é¡¹
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„è¶…é«˜æ¸…é€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "mirror_site": (mirror_options, {"default": default_site}),
                "api_key": ("STRING", {
                    "default": "",
                    "multiline": False,
                    "placeholder": "é•œåƒç«™APIå¯†é’¥ï¼ˆå¯é€‰ï¼Œç•™ç©ºæ—¶è‡ªåŠ¨è·å–ï¼‰"
                }),
                "prompt": ("STRING", {"default": "è¯·æ ¹æ®è¿™äº›å›¾ç‰‡è¿›è¡Œä¸“ä¸šçš„å›¾åƒç¼–è¾‘", "multiline": True}),
                # æ”¯æŒå¤šç§AIæ¨¡å‹å’Œå¤šå›¾åƒç¼–è¾‘æœåŠ¡: nano-bananaæ”¯æŒComflyå’ŒT8é•œåƒç«™, OpenRouteræ¨¡å‹: google/gemini-2.5-flash-image-preview (ä»˜è´¹)
                "model": (["nano-banana [Comfly-T8]", "nano-banana-hd [Comfly-T8]", "gemini-2.5-flash-image-preview", "gemini-2.0-flash", "fal-ai/nano-banana/edit [Comfly-T8]", "google/gemini-2.5-flash-image-preview [OpenRouter]"], {"default": "nano-banana [Comfly-T8]"}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "size": (size_presets, {"default": image_settings.get('default_size', "1024x1024")}),
                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),
                
                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),
                
                # ğŸš€ è´¨é‡å¢å¼ºæ§åˆ¶ç»„
                "quality_enhancement": ("BOOLEAN", {"default": True, "label": "Enable Quality Enhancement"}),
                "enhance_quality": ("BOOLEAN", {"default": True, "label": "Enhanced Image Quality"}),
                "smart_resize": ("BOOLEAN", {"default": True, "label": "Smart Resize with Padding"}),
                "fill_color": ("STRING", {
                    "default": "255,255,255",
                    "placeholder": "å¡«å……é¢œè‰² RGB (å¦‚: 255,255,255)"
                }),
                
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 8192), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 999999}),
            },
            "optional": {
                "custom_size": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "placeholder": "è‡ªå®šä¹‰å°ºå¯¸ (å¦‚: 1920x1080)"
                }),
                "api4gpt_service": (["nano-banana"], {
                    "default": "nano-banana",
                    "tooltip": "API4GPTæœåŠ¡ç±»å‹é€‰æ‹©ï¼ˆä»…åœ¨API4GPTé•œåƒç«™æ—¶æœ‰æ•ˆï¼‰"
                }),
                "image1": ("IMAGE",),
                "image2": ("IMAGE",),
                "image3": ("IMAGE",),
                "image4": ("IMAGE",),
                
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {"unique_id": "UNIQUE_ID"}
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_multiple_images"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            return
        try:
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": user_prompt,
                            "response": response_text,
                            "response_id": str(random.randint(100000, 999999)),
                            "timestamp": time.time(),
                        }
                    ])
                },
            }
            PromptServer.instance.send_sync("display_component", render_spec)
        except Exception:
            pass

    def edit_multiple_images(self, mirror_site: str, api_key: str, prompt: str, model: str,
                           proxy: str, size: str, quality: str, style: str, detail_level: str, camera_control: str, lighting_control: str, 
                           template_selection: str, quality_enhancement: bool, enhance_quality: bool, smart_resize: bool, 
                           fill_color: str, temperature: float, top_p: float, top_k: int, max_output_tokens: int, seed: int, 
                           custom_size: str = "", api4gpt_service: str = "nano-banana", image1=None, image2=None, image3=None, image4=None, custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨é•œåƒç«™APIè¿›è¡Œå¤šå›¾åƒç¼–è¾‘"""
        
        # ğŸš€ ç«‹å³è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå»é™¤UIæ ‡è¯†
        model = _normalize_model_name(model)
        
        # æ ¹æ®é•œåƒç«™ä»é…ç½®è·å–URLå’ŒAPI Key
        site_config = get_mirror_site_config(mirror_site) if mirror_site else {"url": "", "api_key": ""}
        api_url = site_config.get("url", "").strip()
        if site_config.get("api_key") and not api_key.strip():
            api_key = site_config["api_key"]
            print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™API Key: {api_key[:8]}...")
        if not api_url:
            raise ValueError("é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘è¯¥é•œåƒç«™çš„API URL")
        print(f"ğŸ”— è‡ªåŠ¨ä½¿ç”¨é•œåƒç«™URL: {api_url}")
        
        if not validate_api_url(api_url):
            raise ValueError("API URLæ ¼å¼æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
        
        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        # å¤„ç†å›¾åƒæ§åˆ¶å‚æ•°
        try:
            from .gemini_banana import process_image_controls, enhance_prompt_with_controls
        except ImportError:
            from gemini_banana import process_image_controls, enhance_prompt_with_controls
        controls = process_image_controls(size, quality, style, custom_size)
        # å¯¹äºnano-bananaæ¨¡å‹ï¼Œè·³è¿‡å°ºå¯¸æç¤ºï¼Œè®©æ¨¡å‹è‡ªç”±ç”Ÿæˆ
        skip_size_hints = model in ["nano-banana", "nano-banana-hd"]
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(), controls, detail_level, camera_control, lighting_control,
            template_selection, quality_enhancement, enhance_quality, smart_resize, fill_color,
            skip_size_hints=skip_size_hints
        )
        
        # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
        if custom_additions and custom_additions.strip():
            enhanced_prompt += f"\n\nCUSTOM INSTRUCTIONS: {custom_additions.strip()}"
            print(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_additions[:100]}...")
        
        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: å°ºå¯¸={controls['size']}, è´¨é‡={controls['quality']}, é£æ ¼={controls['style']}")
        if controls['is_custom_size']:
            print(f"ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰å°ºå¯¸: {controls['size']}")
        
        # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„å›¾åƒ
        all_input_pils = []
        input_images = [image1, image2, image3, image4]
        
        for i, img_tensor in enumerate(input_images):
            if img_tensor is not None:
                try:
                    pil_image = tensor_to_pil(img_tensor)
                    if pil_image:
                        all_input_pils.append(pil_image)
                        print(f"ğŸ“¸ æ·»åŠ è¾“å…¥å›¾åƒ {i+1}: {pil_image.size}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒ {i+1} å¤„ç†å¤±è´¥: {e}")
        
        if not all_input_pils:
            raise ValueError("é”™è¯¯ï¼šè¯·è¾“å…¥è‡³å°‘ä¸€å¼ è¦ç¼–è¾‘çš„å›¾åƒ")
        
        print(f"ğŸ–¼ï¸ æ€»å…±æ”¶é›†åˆ° {len(all_input_pils)} å¼ è¾“å…¥å›¾åƒ")
        
        # æ™ºèƒ½ç”Ÿæˆå¤šå›¾ç¼–è¾‘æç¤ºè¯
        # é¦–å…ˆå¤„ç†å›¾ç‰‡å¼•ç”¨è½¬æ¢ï¼Œç¡®ä¿æ‰€æœ‰æƒ…å†µä¸‹éƒ½èƒ½ä½¿ç”¨
        user_intent = prompt.strip()
        converted_prompt = user_intent
        
        # è½¬æ¢æ‰€æœ‰å›¾ç‰‡å¼•ç”¨ - é€šç”¨åŒ–å¤„ç†
        if len(all_input_pils) >= 1:
                            converted_prompt = converted_prompt.replace("å›¾1", "å·¦è¾¹å›¾ç‰‡")
        if len(all_input_pils) >= 2:
                            converted_prompt = converted_prompt.replace("å›¾2", "å³è¾¹å›¾ç‰‡")
        if len(all_input_pils) >= 3:
            converted_prompt = converted_prompt.replace("å›¾3", "ç¬¬ä¸‰å¼ å›¾ç‰‡")
        if len(all_input_pils) >= 4:
            converted_prompt = converted_prompt.replace("å›¾4", "ç¬¬å››å¼ å›¾ç‰‡")
        
        # æ ¹æ®å›¾ç‰‡æ•°é‡ç”Ÿæˆä¸åŒçš„æç¤ºè¯ - å®Œå…¨é€šç”¨åŒ–
        if len(all_input_pils) == 2:
            # 2å¼ å›¾ç‰‡ï¼šé€šç”¨ç»„åˆç¼–è¾‘
            if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
                full_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

é‡è¦è¯´æ˜ï¼š
- è¯·ä»”ç»†åˆ†æä¸¤å¼ å›¾ç‰‡çš„å†…å®¹å’Œå…³ç³»
- æ ¹æ®ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤ï¼Œå°†ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ åº”ç”¨åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡ä¸­
- ä¿æŒç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ¸å¿ƒç‰¹å¾å’ŒèƒŒæ™¯ç¯å¢ƒ
- ç¡®ä¿ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ ä¸ç¬¬ä¸€å¼ å›¾ç‰‡å®Œç¾èåˆ
- ç¼–è¾‘ç»“æœåº”è¯¥çœ‹èµ·æ¥è‡ªç„¶çœŸå®ï¼Œç¬¦åˆç”¨æˆ·æ„å›¾

{enhanced_prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚æ‰§è¡Œï¼Œç¡®ä¿ç¼–è¾‘ç»“æœå®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚"""
            else:
                # æ ‡å‡†æç¤ºè¯
                full_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

é‡è¦è¯´æ˜ï¼š
- è¯·ä»”ç»†åˆ†æä¸¤å¼ å›¾ç‰‡çš„å†…å®¹å’Œå…³ç³»
- æ ¹æ®ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤ï¼Œå°†ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ åº”ç”¨åˆ°ç¬¬ä¸€å¼ å›¾ç‰‡ä¸­
- ä¿æŒç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ¸å¿ƒç‰¹å¾å’ŒèƒŒæ™¯ç¯å¢ƒ
- ç¡®ä¿ç¬¬äºŒå¼ å›¾ç‰‡ä¸­çš„å…ƒç´ ä¸ç¬¬ä¸€å¼ å›¾ç‰‡å®Œç¾èåˆ
- ç¼–è¾‘ç»“æœåº”è¯¥çœ‹èµ·æ¥è‡ªç„¶çœŸå®ï¼Œç¬¦åˆç”¨æˆ·æ„å›¾

{enhanced_prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚æ‰§è¡Œï¼Œç¡®ä¿ç¼–è¾‘ç»“æœå®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚"""
        elif len(all_input_pils) == 1:
            # 1å¼ å›¾ç‰‡ï¼šæ ‡å‡†å›¾åƒç¼–è¾‘
            full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ç¼–è¾‘è¿™å¼ å›¾ç‰‡ï¼š

{enhanced_prompt}

è¯·ä½¿ç”¨ä½ çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚"""
        else:
            # 3-4å¼ å›¾ç‰‡ï¼šå¤æ‚ç»„åˆç¼–è¾‘
            if "t8star.cn" in api_url or "ai.t8star.cn" in api_url:
                full_prompt = f"""è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œå¤šå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

é‡è¦è¯´æ˜ï¼š
- è¯·ä»”ç»†åˆ†ææ‰€æœ‰è¾“å…¥å›¾ç‰‡çš„å†…å®¹å’Œå…³ç³»
- æ ¹æ®ç”¨æˆ·çš„å…·ä½“æŒ‡ä»¤ï¼Œå°†ç›¸å…³å›¾ç‰‡ä¸­çš„å…ƒç´ è¿›è¡Œç²¾ç¡®ç»„åˆ
- ä¿æŒç¬¬ä¸€å¼ å›¾ç‰‡çš„æ ¸å¿ƒç‰¹å¾å’ŒèƒŒæ™¯ç¯å¢ƒ
- ç¡®ä¿æ‰€æœ‰ç¼–è¾‘å…ƒç´ ä¸å‚è€ƒå›¾ç‰‡å®Œå…¨ä¸€è‡´
- ç¼–è¾‘ç»“æœåº”è¯¥çœ‹èµ·æ¥è‡ªç„¶çœŸå®ï¼Œç¬¦åˆç”¨æˆ·æ„å›¾

{enhanced_prompt}

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°è¦æ±‚æ‰§è¡Œï¼Œç¡®ä¿ç¼–è¾‘ç»“æœå®Œå…¨ç¬¦åˆç”¨æˆ·æ„å›¾ã€‚"""
            else:
                full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å›¾åƒç¼–è¾‘ä¸“å®¶ã€‚è¯·æ ¹æ®è¿™äº›å›¾ç‰‡å’Œä»¥ä¸‹æŒ‡ä»¤è¿›è¡Œå›¾åƒç¼–è¾‘ï¼š

{converted_prompt}

{enhanced_prompt}

è¯·ä½¿ç”¨ä½ çš„å›¾åƒç¼–è¾‘èƒ½åŠ›ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç¼–è¾‘ç»“æœã€‚ç¡®ä¿ç¼–è¾‘åçš„å›¾åƒç¬¦åˆæ‰€æœ‰è¦æ±‚ã€‚"""
        
        # è½¬æ¢æ‰€æœ‰è¾“å…¥å›¾ç‰‡ - ä¿®å¤å…³é”®é—®é¢˜
        # ç¡®ä¿æ‰€æœ‰å›¾ç‰‡éƒ½è¢«ä¼ é€’ç»™æ¨¡å‹ï¼Œè®©æ¨¡å‹èƒ½çœ‹åˆ°å®Œæ•´ä¿¡æ¯
        all_image_parts = []
        
        for i, pil_image in enumerate(all_input_pils):
            # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥ç¬¦åˆAPIè¦æ±‚
            pil_image = resize_image_for_api(pil_image)
            # è½¬æ¢ä¸ºbase64
            image_base64 = image_to_base64(pil_image, format='JPEG')
            all_image_parts.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            })
            print(f"ğŸ“¸ å‡†å¤‡ä¼ é€’å›¾åƒ {i+1}: {pil_image.size}")
        
        # ä»£ç†å¤„ç†ï¼šæœ‰æ•ˆåˆ™è®¾ç½®ï¼Œæ— æ•ˆ/æœªå¡«åˆ™æ¸…é™¤ï¼Œé¿å…æ®‹ç•™ç¯å¢ƒå˜é‡å½±å“è¯·æ±‚
        if proxy and proxy.strip() and "None" not in proxy:
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
            print(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
        else:
            existing = os.environ.get('HTTPS_PROXY') or os.environ.get('HTTP_PROXY')
            if existing:
                print(f"ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼Œæ²¿ç”¨ç³»ç»Ÿä»£ç†: {existing}")
            else:
                print("ğŸ”Œ æœªæŒ‡å®šä»£ç†ï¼ˆç³»ç»Ÿæ— ä»£ç†ï¼‰")
        
        # æ£€æŸ¥é•œåƒç«™ç±»å‹ - æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        is_nano_banana_official = mirror_site == "nano-bananaå®˜æ–¹"
        is_t8_mirror = "t8star.cn" in api_url or "ai.t8star.cn" in api_url
        is_api4gpt_mirror = "www.api4gpt.com" in api_url
        is_comfly_mirror = _is_comfly_base(api_url)
        is_openrouter_mirror = "openrouter.ai" in api_url
        is_openai_mirror = "api.openai.com" in api_url or site_config.get("api_format") == "openai"
        
        # æ„å»ºå®Œæ•´çš„API URLï¼ˆOpenRouteré™¤å¤–ï¼Œå› ä¸ºå®ƒåœ¨å„è‡ªçš„å¤„ç†é€»è¾‘ä¸­æ„å»ºï¼‰
        if not is_openrouter_mirror:
            full_url = build_api_url(api_url, model)
            print(f"ğŸŒ ä½¿ç”¨APIåœ°å€: {full_url}")
        else:
            print(f"ğŸŒ OpenRouteré•œåƒç«™ï¼ŒURLå°†åœ¨OpenRouterå¤„ç†é€»è¾‘ä¸­æ„å»º")
        
        # æŒ‰ç…§ä¼˜å…ˆçº§é¡ºåºå¤„ç†é•œåƒç«™ï¼šnano-bananaå®˜æ–¹ â†’ Comfly â†’ T8 â†’ API4GPT â†’ OpenRouter â†’ OpenAI â†’ custom
        
        # 1. nano-bananaå®˜æ–¹é•œåƒç«™å¤„ç†
        if is_nano_banana_official:
            print("ğŸ”— æ£€æµ‹åˆ°nano-bananaå®˜æ–¹é•œåƒç«™ï¼Œä½¿ç”¨Googleå®˜æ–¹API")

            # æ„å»ºå†…å®¹éƒ¨åˆ†ï¼ˆæ–‡æœ¬ + å¤šå›¾åƒï¼‰
            content_parts = [{"text": full_prompt}] + all_image_parts

            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
                "responseModalities": ["TEXT", "IMAGE"]
            }

            # æ·»åŠ seedï¼ˆå¦‚æœæœ‰æ•ˆï¼‰
            if seed and seed > 0:
                generation_config["seed"] = seed

            try:
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨ï¼ˆå®˜æ–¹APIä¼˜å…ˆï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST APIï¼‰
                response_json = generate_with_priority_api(
                    api_key=api_key,
                    model=_normalize_model_name(model),
                    content_parts=content_parts,
                    generation_config=generation_config,
                    max_retries=5,
                    proxy=proxy
                )

                if response_json:
                    # æå–ç¼–è¾‘åçš„å›¾åƒ
                    edited_image = process_generated_image_from_response(response_json)

                    # æå–å“åº”æ–‡æœ¬
                    response_text = extract_text_from_response(response_json)

                    if edited_image:
                        # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                        try:
                            edited_image = _apply_full_enhancements(
                                edited_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆnano-bananaå®˜æ–¹ï¼‰")
                        self._push_chat(enhanced_prompt, response_text or "", unique_id)
                        return (image_tensor, response_text)
                    else:
                        print("âš ï¸ nano-bananaå®˜æ–¹APIå“åº”ä¸­æœªæ‰¾åˆ°ç¼–è¾‘åçš„å›¾åƒæ•°æ®")
                        # è¿”å›ç¬¬ä¸€å¼ è¾“å…¥å›¾åƒ
                        if all_input_pils:
                            return (pil_to_tensor(all_input_pils[0]), response_text)
                        else:
                            # åˆ›å»ºé»˜è®¤å›¾åƒ
                            default_image = Image.new('RGB', (1024, 1024), color='black')
                            return (pil_to_tensor(default_image), response_text)
                else:
                    raise Exception("nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥")

            except Exception as e:
                print(f"âŒ nano-bananaå®˜æ–¹APIè°ƒç”¨å¤±è´¥: {e}")
                raise e
            
        # 2. Comflyé•œåƒç«™å¤„ç†
        elif is_comfly_mirror:
            print("ğŸ”— æ£€æµ‹åˆ°Comflyé•œåƒç«™ï¼Œä½¿ç”¨Comfly APIæ ¼å¼")
            
            if model in ["nano-banana", "nano-banana-hd"] and all_input_pils:
                # Comfly nano-banana å¤šå›¾åƒç¼–è¾‘
                try:
                    # ä½¿ç”¨æ‰€æœ‰è¾“å…¥å›¾åƒ
                    result = _comfly_nano_banana_edit(api_url, api_key, model, enhanced_prompt, all_input_pils, controls['size'], temperature, top_p, max_output_tokens, seed)
                    edited_image = None
                    response_text = ""
                    
                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        response_text = result.get('response_text', "")
                        
                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                # ä¿®å¤base64å¡«å……é—®é¢˜
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception as decode_error:
                                print(f"âš ï¸ base64è§£ç å¤±è´¥: {decode_error}")
                                # å°è¯•ç›´æ¥è§£ç 
                                try:
                                    img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                                except Exception as e2:
                                    print(f"âš ï¸ ç›´æ¥è§£ç ä¹Ÿå¤±è´¥: {e2}")
                                    img = None
                            edited_image = img
                        else:
                            # å¦‚æœæ²¡æœ‰base64æ•°æ®ï¼Œå°è¯•ä»å“åº”æ–‡æœ¬ä¸­æå–å›¾åƒ
                            import re
                            base64_pattern = r'data:image\/[^;]+;base64,([A-Za-z0-9+/=]+)'
                            matches = re.findall(base64_pattern, response_text)
                            if matches:
                                from base64 import b64decode
                                import io
                                img = Image.open(io.BytesIO(b64decode(matches[0]))).convert('RGB')
                                edited_image = img
                    
                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œç›´æ¥è¿”å›ç»“æœ
                    if edited_image:
                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆComfly nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, response_text or "", unique_id)
                        return (image_tensor, response_text)
                        
                except Exception as e:
                    print(f"âŒ Comfly(nano-banana) å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {e}")
                    raise e
            request_data = {
                "contents": [{
                    "parts": [
                        {"text": full_prompt}
                    ] + all_image_parts
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
                
        # 3. T8é•œåƒç«™å¤„ç†
        elif is_t8_mirror:
            # T8é•œåƒç«™ä¹Ÿæ”¯æŒ nano-bananaï¼Œè°ƒç”¨æ–¹å¼ä¸ Comfly ä¸€è‡´
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨chat/completionsç«¯ç‚¹ (nano-banana ç›´è¿)")
            if _normalize_model_name(model) in ["nano-banana", "nano-banana-hd"] and all_input_pils:
                try:
                    # ä½¿ç”¨æ‰€æœ‰è¾“å…¥å›¾åƒ
                    result = _comfly_nano_banana_edit(full_url, api_key, _normalize_model_name(model), enhanced_prompt, all_input_pils, controls['size'], temperature, top_p, max_output_tokens, seed)
                    edited_image = None
                    response_text = ""

                    if isinstance(result, dict) and 'data' in result and result['data']:
                        b64 = result['data'][0].get('b64_json')
                        response_text = result.get('response_text', "")

                        if b64:
                            from base64 import b64decode
                            import io
                            try:
                                b64_fixed = b64 + '=' * (4 - len(b64) % 4)
                                img = Image.open(io.BytesIO(b64decode(b64_fixed))).convert('RGB')
                            except Exception:
                                img = Image.open(io.BytesIO(b64decode(b64))).convert('RGB')
                            edited_image = img

                    # å¦‚æœæˆåŠŸå¤„ç†ï¼Œåº”ç”¨å…¨é‡å¢å¼ºåå†è¿”å›
                    if edited_image:
                        # åº”ç”¨å…¨é‡å¢å¼ºï¼ˆåŒ…æ‹¬æ™ºèƒ½ä¸»ä½“æ£€æµ‹å’Œå±…ä¸­æŠ€æœ¯ï¼‰
                        try:
                            edited_image = _apply_full_enhancements(
                                edited_image,
                                controls['size'],
                                quality,
                                enhance_quality,
                                smart_resize
                            )
                            try:
                                print(f"ğŸ”§ Final output size: {edited_image.size[0]}x{edited_image.size[1]}")
                            except Exception:
                                pass
                        except Exception:
                            pass

                        image_tensor = pil_to_tensor(edited_image)
                        print("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼ˆT8 nano-bananaï¼‰")
                        self._push_chat(enhanced_prompt, response_text or "", unique_id)
                        return (image_tensor, response_text)

                except Exception as e:
                    print(f"âŒ T8(nano-banana) å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {e}")
                    raise e
            # å…¶ä»–æ¨¡å‹ä»èµ°åŸ T8 OpenAI å…¼å®¹æ ¼å¼
            print("ğŸ”— æ£€æµ‹åˆ°T8é•œåƒç«™ï¼Œä½¿ç”¨OpenAIå…¼å®¹çš„APIæ ¼å¼")

            # ä¸ºT8é•œåƒç«™ä¼˜åŒ–å¤šå›¾ç‰‡å¤„ç†
            # ç¡®ä¿å›¾ç‰‡é¡ºåºæ­£ç¡®ï¼Œå¹¶æ·»åŠ æ˜ç¡®çš„å›¾ç‰‡æ ‡è¯†
            optimized_image_parts = []
            for i, image_part in enumerate(all_image_parts):
                # ä¸ºæ¯å¼ å›¾ç‰‡æ·»åŠ æ˜ç¡®çš„æ ‡è¯†
                optimized_image_parts.append({
                    "type": "image_url",
                    "image_url": {
                        "url": image_part["inline_data"]["data"],
                        "detail": "high"  # ç¡®ä¿é«˜åˆ†è¾¨ç‡å¤„ç†
                    }
                })
                print(f"ğŸ”— T8é•œåƒç«™å›¾ç‰‡ {i+1}: å·²æ·»åŠ åˆ°è¯·æ±‚ä¸­")

            # æ„å»ºT8é•œåƒç«™è¯·æ±‚æ•°æ®
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": full_prompt
                        }
                    ] + optimized_image_parts  # æ·»åŠ ä¼˜åŒ–åçš„å›¾ç‰‡
                }],
                "temperature": temperature,
                "max_tokens": max_output_tokens
            }
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
            
        # 4. API4GPTé•œåƒç«™å¤„ç†
        elif is_api4gpt_mirror:
            # API4GPTé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°API4GPTé•œåƒç«™ï¼Œä½¿ç”¨API4GPT APIæ ¼å¼")
            
            # API4GPTçš„å¤šå›¾ç‰‡ç¼–è¾‘å¤„ç†
            if api4gpt_service == "nano-banana":
                # nano-bananaæœåŠ¡ä½¿ç”¨å®˜æ–¹API4GPTæ ¼å¼
                print(f"ğŸ”— ä½¿ç”¨API4GPT nano-bananaæœåŠ¡è¿›è¡Œå¤šå›¾ç‰‡ç¼–è¾‘")
                
                # å¯¹äºå¤šå›¾ç‰‡ç¼–è¾‘ï¼ŒAPI4GPT nano-bananaå¯èƒ½éœ€è¦ç‰¹æ®Šå¤„ç†
                # ç›®å‰API4GPTå®˜æ–¹æ–‡æ¡£ä¸»è¦æ”¯æŒå•å›¾ç‰‡ç¼–è¾‘
                # æˆ‘ä»¬ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºä¸»è¦è¾“å…¥ï¼Œå…¶ä»–å›¾ç‰‡ä½œä¸ºå‚è€ƒ
                print("âš ï¸ API4GPT nano-bananaå¤šå›¾ç‰‡ç¼–è¾‘åŠŸèƒ½æœ‰é™ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºä¸»è¦è¾“å…¥")
                
                primary_image = all_image_parts[0] if all_image_parts else None
                if primary_image:
                    # æ„å»ºåŒ…å«ç¬¬ä¸€å¼ å›¾ç‰‡çš„ç¼–è¾‘è¯·æ±‚
                    request_data = build_api4gpt_request(
                        service_type=api4gpt_service,
                        model=_normalize_model_name(model),
                        prompt=full_prompt,
                        image_base64=primary_image["inline_data"]["data"],
                        size=controls['size'],
                        quality=controls['quality'],
                        style=controls['style'],
                        temperature=temperature,
                        max_tokens=max_output_tokens
                    )
                    
                    # è°ƒç”¨API4GPT APIè¿›è¡Œå›¾åƒç¼–è¾‘
                    try:
                        result = call_api4gpt_api(api_url, api_key, api4gpt_service, request_data)
                        print("âœ… API4GPTå¤šå›¾ç‰‡ç¼–è¾‘APIè°ƒç”¨æˆåŠŸ")
                        
                        # è§£æAPI4GPTå“åº”
                        response_text, edited_image = parse_api4gpt_response(result, api4gpt_service)
                        
                        if edited_image:
                            print(f"âœ… æˆåŠŸæå–API4GPTç¼–è¾‘åçš„å›¾åƒ")
                        else:
                            print("âš ï¸ API4GPTæœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = all_input_pils[0]
                            if not response_text:
                                response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                        
                        # å¤„ç†å›¾åƒå°ºå¯¸è°ƒæ•´
                        if edited_image:
                            try:
                                target_size = controls['size']
                                if 'x' in target_size:
                                    target_width, target_height = map(int, target_size.split('x'))
                                    
                                    if edited_image.size != (target_width, target_height):
                                        print(f"ğŸ”§ ä½¿ç”¨æ‰©å›¾æŠ€æœ¯è°ƒæ•´å›¾åƒå°ºå¯¸: {edited_image.size} -> {target_size}")
                                        
                                        # ğŸš€ ä½¿ç”¨æ— ç™½è‰²å¡«å……ä¸å˜å½¢çš„æ‰©å›¾æŠ€æœ¯
                                        edited_image = smart_resize_with_padding(edited_image, (target_width, target_height))
                                        
                                        print(f"âœ… æ‰©å›¾æŠ€æœ¯å®Œæˆ: {edited_image.size}")
                                    else:
                                        print(f"âœ… å›¾åƒå°ºå¯¸å·²ç¬¦åˆè¦æ±‚: {edited_image.size}")
                                        
                            except Exception as e:
                                print(f"âš ï¸ å°ºå¯¸è°ƒæ•´å¤±è´¥: {e}, ä¿æŒåŸå§‹å°ºå¯¸")
                        
                        # åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼ˆå…³é—­è‡ªé€‚åº”ï¼‰
                        if enhance_quality and quality in ['hd', 'ultra_hd']:
                            print(f"âœ¨ åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼Œè´¨é‡ç­‰çº§: {quality}")
                            edited_image = enhance_image_quality(edited_image, quality, "disabled")
                            print(f"âœ… å¢å¼ºå®Œæˆ")
                        
                        # è½¬æ¢ä¸ºtensor
                        image_tensor = pil_to_tensor(edited_image)
                        
                        print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼ˆAPI4GPTï¼‰")
                        print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
                        print(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")
                        self._push_chat(enhanced_prompt, response_text or "", unique_id)
                        return (image_tensor, response_text)
                        
                    except Exception as e:
                        print(f"âŒ API4GPTå¤šå›¾ç‰‡ç¼–è¾‘APIè°ƒç”¨å¤±è´¥: {e}")
                        raise ValueError(f"API4GPTå¤šå›¾ç‰‡ç¼–è¾‘APIè°ƒç”¨å¤±è´¥: {e}")
                else:
                    raise ValueError("æ²¡æœ‰å¯ç”¨çš„è¾“å…¥å›¾åƒè¿›è¡Œç¼–è¾‘")
                
            else:
                # å…¶ä»–æœåŠ¡ï¼ˆDALL-E 3, Stable-Diffusion, Fluxï¼‰ä¸æ”¯æŒå¤šå›¾ç‰‡ç¼–è¾‘
                # ä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡ä½œä¸ºä¸»è¦è¾“å…¥
                print(f"âš ï¸ API4GPT {api4gpt_service} æœåŠ¡ä¸æ”¯æŒå¤šå›¾ç‰‡ç¼–è¾‘ï¼Œä½¿ç”¨ç¬¬ä¸€å¼ å›¾ç‰‡")
                primary_image = all_image_parts[0] if all_image_parts else None
                
                request_data = build_api4gpt_request(
                    service_type=api4gpt_service,
                    model=_normalize_model_name(model),
                    prompt=full_prompt,
                    image_base64=primary_image["inline_data"]["data"] if primary_image else None,
                    size=controls['size'],
                    quality=controls['quality'],
                    style=controls['style'],
                    temperature=temperature,
                    max_tokens=max_output_tokens
                )
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        elif is_openrouter_mirror:
            # OpenRouteré•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenRouteré•œåƒç«™ï¼Œä½¿ç”¨OpenRouter APIæ ¼å¼")
            
            # OpenRouterä½¿ç”¨chat/completionsç«¯ç‚¹è¿›è¡Œå¤šå›¾åƒç¼–è¾‘
            # æ„å»ºOpenAIå…¼å®¹çš„è¯·æ±‚æ ¼å¼
            content = []
            
            # æ·»åŠ æ‰€æœ‰è¾“å…¥å›¾åƒ
            for i, pil_image in enumerate(all_input_pils):
                # è½¬æ¢ä¸ºbase64
                image_base64 = image_to_base64(pil_image, format='JPEG')
                image_url = f"data:image/jpeg;base64,{image_base64}"
                
                # æ·»åŠ å›¾ç‰‡æ ‡è¯†
                content.append({
                    "type": "text",
                    "text": f"[è¿™æ˜¯ç¬¬{i+1}å¼ å›¾ç‰‡]"
                })
                
                # æ·»åŠ å›¾ç‰‡
                content.append({
                    "type": "image_url",
                    "image_url": {"url": image_url}
                })
            
            # æ·»åŠ æ–‡æœ¬æŒ‡ä»¤
            enhanced_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual edited image, not just text description.

Task: {full_prompt}

Image References:
- When I mention "ç¬¬1å¼ å›¾ç‰‡", I mean the first image provided above
- When I mention "ç¬¬2å¼ å›¾ç‰‡", I mean the second image provided above
- When I mention "ç¬¬3å¼ å›¾ç‰‡", I mean the third image provided above
- When I mention "ç¬¬4å¼ å›¾ç‰‡", I mean the fourth image provided above

REQUIREMENTS:
1. EDIT the provided images based on my request
2. DO NOT just describe what the edited image should look like
3. RETURN the actual edited image file/data
4. The output MUST be a visual image, not text

Execute the multi-image editing task now and return the edited image."""
            
            content.append({
                "type": "text",
                "text": enhanced_instruction
            })
            
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": content
                }],
                "temperature": temperature,
                "top_p": top_p,
                "max_tokens": max_output_tokens,
                "stream": True  # Required for gemini-2.5-flash-image-preview
            }
            
            # ä½¿ç”¨OpenRouterçš„chat/completionsç«¯ç‚¹
            # å¯¹äºOpenRouterï¼Œapi_urlå·²ç»åŒ…å«äº†/v1ï¼Œæ‰€ä»¥ç›´æ¥æ·»åŠ /chat/completions
            if api_url.endswith('/v1'):
                full_url = f"{api_url}/chat/completions"
            else:
                full_url = f"{api_url}/v1/chat/completions"
            print(f"ğŸ”— ä½¿ç”¨OpenRouter chat/completionsç«¯ç‚¹è¿›è¡Œå¤šå›¾åƒç¼–è¾‘: {full_url}")
            
            # è®¾ç½®OpenRouterè¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}",
                "HTTP-Referer": "https://github.com/ComfyUI-LLM-Prompt",
                "X-Title": "ComfyUI LLM Prompt Plugin"
            }
        elif is_openai_mirror:
            # OpenAIé•œåƒç«™
            print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
            request_data = {
                "model": _normalize_model_name(model),
                "messages": [{
                    "role": "user",
                    "content": enhanced_prompt
                }],
                "temperature": temperature,
                "max_tokens": max_output_tokens,
                "stream": True
            }
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        else:
            # æ ‡å‡†Gemini APIæ ¼å¼
            request_data = {
                "contents": [{
                    "parts": [
                        {
                            "text": full_prompt
                        }
                    ] + all_image_parts  # æ·»åŠ æ‰€æœ‰å›¾ç‰‡
                }],
                "generationConfig": {
                    "temperature": temperature,
                    "topP": top_p,
                    "topK": top_k,
                    "maxOutputTokens": max_output_tokens,
                    "responseModalities": ["TEXT", "IMAGE"],
                    "seed": seed if seed and seed > 0 else None
                }
            }
            
            # æ¸…ç† None å€¼
            if request_data["generationConfig"]["seed"] is None:
                del request_data["generationConfig"]["seed"]
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key.strip()}"
            }
        
        # æ™ºèƒ½é‡è¯•æœºåˆ¶
        max_retries = 5
        timeout = 120
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç¼–è¾‘å›¾ç‰‡... (å°è¯• {attempt + 1}/{max_retries})")
                print(f"ğŸ“ ç¼–è¾‘æŒ‡ä»¤: {enhanced_prompt[:100]}...")
                print(f"ğŸ”— é•œåƒç«™: {api_url}")
                
                # å‘é€è¯·æ±‚
                response = requests.post(full_url, headers=headers, json=request_data, timeout=timeout)
                
                # æ£€æŸ¥å“åº”çŠ¶æ€å’Œå†…å®¹
                print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
                print(f"ğŸ“¡ å“åº”å¤´: {dict(response.headers)}")
                print(f"ğŸ“¡ å“åº”å†…å®¹é•¿åº¦: {len(response.text)}")
                
                # æ£€æŸ¥å“åº”å†…å®¹
                if not response.text.strip():
                    print("âš ï¸ APIè¿”å›ç©ºå“åº”")
                    raise ValueError("APIè¿”å›ç©ºå“åº”")
                
                # æˆåŠŸå“åº”
                if response.status_code == 200:
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    # ä¸ºæ‰€æœ‰é•œåƒç«™å®šä¹‰ result å˜é‡
                    if not is_api4gpt_mirror and not is_openrouter_mirror and not is_t8_mirror and not is_openai_mirror:
                        # æ ‡å‡† Gemini API é•œåƒç«™ï¼ˆå¦‚ Comfy APIï¼‰
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£ææ ‡å‡† Gemini API å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ è§£æå“åº”å¤±è´¥: {e}")
                            result = {}
                    elif is_t8_mirror:
                        # T8 é•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ T8 é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ T8 é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        try:
                            result = response.json()
                            print("âœ… æˆåŠŸè§£æ OpenAI é•œåƒç«™å“åº”")
                        except Exception as e:
                            print(f"âš ï¸ OpenAI é•œåƒç«™å“åº”è§£æå¤±è´¥: {e}")
                            result = {}
                    
                    if is_api4gpt_mirror:
                        # API4GPTé•œåƒç«™å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†API4GPTé•œåƒç«™å“åº”")
                        
                        try:
                            # å°è¯•è§£æJSONå“åº”
                            result = response.json()
                            print(f"ğŸ“‹ API4GPTå“åº”ç»“æ„: {list(result.keys())}")
                            
                            if api4gpt_service == "nano-banana":
                                # nano-bananaä½¿ç”¨OpenAIå…¼å®¹æ ¼å¼
                                response_text, edited_image = parse_openai_compatible_response(result)
                            else:
                                # å…¶ä»–æœåŠ¡ä½¿ç”¨åŸæœ‰çš„è§£æé€»è¾‘
                                response_text, edited_image = parse_api4gpt_response(result, api4gpt_service)
                            
                            if edited_image:
                                print(f"âœ… æˆåŠŸæå–API4GPTç¼–è¾‘åçš„å›¾åƒ")
                            else:
                                print("âš ï¸ API4GPTæœªè¿”å›ç¼–è¾‘åçš„å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡")
                                edited_image = all_input_pils[0]
                                if not response_text:
                                    response_text = f"API4GPT {api4gpt_service} æœåŠ¡å“åº”å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                        except Exception as json_error:
                            print(f"âš ï¸ API4GPT JSONè§£æå¤±è´¥: {json_error}")
                            print(f"ğŸ“‹ åŸå§‹å“åº”å†…å®¹: {response.text[:500]}...")
                            raise ValueError(f"API4GPTå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {json_error}")
                            
                    elif is_openrouter_mirror:
                        # OpenRouteré•œåƒç«™å“åº”å¤„ç† - ä½¿ç”¨æµå¼å“åº”
                        print("ğŸ”— å¤„ç†OpenRouteré•œåƒç«™æµå¼å“åº”")
                        
                        # å¤„ç†æµå¼å“åº”
                        response_text = process_openrouter_stream(response)
                        
                        # æ£€æŸ¥å“åº”æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®
                        if "data:image/" in response_text:
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenRouterè¿”å›çš„å›¾åƒæ•°æ®")
                            try:
                                # ä½¿ç”¨æ­£ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼æå–base64å›¾åƒæ•°æ®
                                import re
                                base64_pattern = r'data:image/[^;]+;base64,[A-Za-z0-9+/=]+'
                                image_matches = re.findall(base64_pattern, response_text)
                                if image_matches:
                                    # å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾åƒæ•°æ®
                                    image_url = image_matches[0]
                                    print(f"ğŸ¯ æˆåŠŸåŒ¹é…OpenRouterå›¾åƒæ•°æ®ï¼Œé•¿åº¦: {len(image_url)}å­—ç¬¦")
                                    
                                    # æå–base64éƒ¨åˆ†
                                    if ';base64,' in image_url:
                                        import io
                                        base64_data = image_url.split(';base64,', 1)[1]
                                        image_bytes = base64.b64decode(base64_data)
                                        edited_image = Image.open(io.BytesIO(image_bytes))
                                        print(f"âœ… æˆåŠŸæå–OpenRouterç¼–è¾‘åçš„å›¾åƒ: {edited_image.size}")
                                        
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(base64_pattern, '[å›¾åƒå·²ç¼–è¾‘]', response_text)
                                    else:
                                        print(f"âš ï¸ å›¾åƒæ•°æ®æ ¼å¼ä¸æ­£ç¡®: {image_url[:100]}...")
                                        edited_image = all_input_pils[0]
                                        response_text = f"OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†æ•°æ®æ ¼å¼ä¸æ­£ç¡®"
                                else:
                                    print(f"âš ï¸ æ­£åˆ™è¡¨è¾¾å¼æœªæ‰¾åˆ°åŒ¹é…çš„å›¾åƒæ•°æ®")
                                    edited_image = all_input_pils[0]
                                    response_text = f"OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†æœªæ‰¾åˆ°å›¾åƒæ•°æ®"
                            except Exception as e:
                                print(f"âš ï¸ OpenRouterå›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                edited_image = all_input_pils[0]
                                response_text = f"OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†è§£æå¤±è´¥: {e}"
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œè¿”å›åŸå›¾ç‰‡
                        if not edited_image:
                            print("âš ï¸ OpenRouteræœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®ï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = all_input_pils[0]
                            if not response_text:
                                response_text = "OpenRouterå¤šå›¾ç‰‡ç¼–è¾‘å®Œæˆï¼Œä½†æœªè¿”å›ç¼–è¾‘åçš„å›¾åƒæ•°æ®"
                    elif is_t8_mirror:
                        # T8é•œåƒç«™OpenAIæ ¼å¼å“åº”å¤„ç†
                        print("ğŸ”— å¤„ç†T8é•œåƒç«™OpenAIæ ¼å¼å“åº”")
                        if "choices" in result and result["choices"]:
                            choice = result["choices"][0]
                            if "message" in choice and "content" in choice["message"]:
                                content = choice["message"]["content"]
                                if isinstance(content, str):
                                    response_text = content
                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«base64å›¾åƒæ•°æ®
                                    if "![image](data:image/" in content:
                                        print("ğŸ–¼ï¸ æ£€æµ‹åˆ°T8é•œåƒç«™è¿”å›çš„å›¾åƒæ•°æ®")
                                        try:
                                            # æå–base64å›¾åƒæ•°æ®
                                            import re, io
                                            image_match = re.search(r'!\[image\]\(data:image/\w+;base64,([^)]+)\)', content)
                                            if image_match:
                                                image_data = image_match.group(1)
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print("âœ… æˆåŠŸæå–T8é•œåƒç«™ç¼–è¾‘åçš„å›¾åƒ")
                                                # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                                response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç¼–è¾‘]', content)
                                        except Exception as e:
                                            print(f"âš ï¸ T8é•œåƒç«™å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                elif isinstance(content, list):
                                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                                    for item in content:
                                        if item.get("type") == "text":
                                            response_text += item.get("text", "")
                                        elif item.get("type") == "image_url":
                                            # å¤„ç†å›¾åƒURLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒURLå“åº”")
                        
                        # å¦‚æœæ²¡æœ‰æˆåŠŸæå–å›¾åƒï¼Œå†æ£€æŸ¥å…¶ä»–å¯èƒ½çš„ä½ç½®
                        if not edited_image:
                            print("âš ï¸ T8é•œåƒç«™æš‚ä¸æ”¯æŒå›¾åƒç¼–è¾‘ï¼Œè¿”å›åŸå›¾ç‰‡")
                            edited_image = all_input_pils[0]
                            if not response_text:
                                response_text = "T8é•œåƒç«™æ–‡æœ¬åˆ†æå®Œæˆï¼Œä½†æš‚ä¸æ”¯æŒå›¾åƒç¼–è¾‘åŠŸèƒ½"
                    elif is_openai_mirror:
                        # OpenAIé•œåƒç«™
                        print("ğŸ”— æ£€æµ‹åˆ°OpenAIé•œåƒç«™ï¼Œä½¿ç”¨OpenAI APIæ ¼å¼")
                        request_data = {
                            "model": _normalize_model_name(model),
                            "messages": [{
                                "role": "user",
                                "content": enhanced_prompt
                            }],
                            "temperature": temperature,
                            "max_tokens": max_output_tokens,
                            "stream": True
                        }
                        
                        # è®¾ç½®è¯·æ±‚å¤´
                        headers = {
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {api_key.strip()}"
                        }
                    else:
                        # æ ‡å‡†Gemini APIå“åº”å¤„ç†
                        try:
                            result = response.json()
                        except Exception as e:
                            print(f"âš ï¸ æ ‡å‡†Gemini JSONè§£æå¤±è´¥: {e}")
                            result = {}
                        if "candidates" in result and result["candidates"]:
                            candidate = result["candidates"][0]
                            if "content" in candidate and "parts" in candidate["content"]:
                                for part in candidate["content"]["parts"]:
                                    # æå–æ–‡æœ¬
                                    if "text" in part:
                                        response_text += part["text"]
                                    
                                    # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                    if "inline_data" in part or "inlineData" in part:
                                        inline_data = part.get("inline_data") or part.get("inlineData")
                                        if inline_data and "data" in inline_data:
                                            try:
                                                # è§£ç å›¾ç‰‡æ•°æ®
                                                import io
                                                image_data = inline_data["data"]
                                                image_bytes = base64.b64decode(image_data)
                                                edited_image = Image.open(io.BytesIO(image_bytes))
                                                print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                            except Exception as e:
                                                print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = all_input_pils[0]
                        if not response_text:
                            response_text = "å›¾ç‰‡ç¼–è¾‘è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"
                    
                    # å¼ºåˆ¶è°ƒæ•´å›¾åƒå°ºå¯¸åˆ°ç”¨æˆ·æŒ‡å®šçš„å°ºå¯¸
                    try:
                        target_width, target_height = map(int, controls['size'].split('x'))
                        current_width, current_height = edited_image.size
                        
                        if (current_width, current_height) != (target_width, target_height):
                            print(f"ğŸ”„ å¼ºåˆ¶è°ƒæ•´å›¾åƒå°ºå¯¸: {current_width}x{current_height} -> {target_width}x{target_height}")
                            
                            # ğŸš€ ä½¿ç”¨æ— ç™½è‰²å¡«å……ä¸å˜å½¢çš„æ‰©å›¾æŠ€æœ¯
                            edited_image = smart_resize_with_padding(edited_image, (target_width, target_height))
                            
                            print(f"âœ… æ‰©å›¾æŠ€æœ¯å®Œæˆ: {edited_image.size}")
                        else:
                            print(f"âœ… å›¾åƒå°ºå¯¸å·²ç¬¦åˆè¦æ±‚: {edited_image.size}")
                            
                    except Exception as e:
                        print(f"âš ï¸ å°ºå¯¸è°ƒæ•´å¤±è´¥: {e}, ä¿æŒåŸå§‹å°ºå¯¸")
                    
                    # åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼ˆå…³é—­è‡ªé€‚åº”ï¼‰
                    if enhance_quality and quality in ['hd', 'ultra_hd']:
                        print(f"âœ¨ åº”ç”¨ä¼ ç»Ÿè´¨é‡å¢å¼ºï¼Œè´¨é‡ç­‰çº§: {quality}")
                        edited_image = enhance_image_quality(edited_image, quality, "disabled")
                        print(f"âœ… å¢å¼ºå®Œæˆ")
                    
                    # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
                    if not response_text:
                        response_text = "å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„æŒ‡ä»¤å’Œå‚è€ƒå›¾åƒç”Ÿæˆçš„ç¼–è¾‘ç»“æœã€‚"
                        print("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")
                    
                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆ")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ HTTPçŠ¶æ€ç : {response.status_code}")
                    try:
                        error_detail = response.json()
                        print(f"âŒ é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    except:
                        print(f"âŒ é”™è¯¯æ–‡æœ¬: {response.text}")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt == max_retries - 1:
                        response.raise_for_status()
                    
                    # æ™ºèƒ½ç­‰å¾…
                    delay = smart_retry_delay(attempt, response.status_code)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except requests.exceptions.RequestException as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    raise ValueError(f"APIè¯·æ±‚å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ å¤„ç†å¤±è´¥: {error_msg}")
                raise ValueError(f"å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {error_msg}")


def parse_openai_compatible_response(response_data):
    """è§£æOpenAIå…¼å®¹æ ¼å¼çš„å“åº”æ•°æ®"""
    response_text = ""
    generated_image = None
    
    try:
        if "choices" in response_data and response_data["choices"]:
            choice = response_data["choices"][0]
            if "message" in choice and "content" in choice["message"]:
                content = choice["message"]["content"]
                
                if isinstance(content, str):
                    response_text = content
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å›¾åƒæ•°æ®ï¼ˆbase64æˆ–URLï¼‰
                    if "![image](" in content:
                        print("ğŸ–¼ï¸ æ£€æµ‹åˆ°OpenAIå…¼å®¹æ ¼å¼è¿”å›çš„å›¾åƒæ•°æ®")
                        try:
                            import re
                            # æå–æ‰€æœ‰å›¾åƒæ ‡è®°
                            image_matches = re.findall(r'!\[image\]\(([^)]+)\)', content)
                            
                            for image_url in image_matches:
                                if image_url.startswith("data:image/"):
                                    # å¤„ç†base64å›¾åƒæ•°æ®
                                    try:
                                        image_data = image_url.split(",")[1]
                                        image_bytes = base64.b64decode(image_data)
                                        generated_image = Image.open(io.BytesIO(image_bytes))
                                        print("âœ… æˆåŠŸæå–base64å›¾åƒæ•°æ®")
                                        # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤base64æ•°æ®
                                        response_text = re.sub(r'!\[image\]\(data:image/\w+;base64,[^)]+\)', '[å›¾åƒå·²ç”Ÿæˆ]', content)
                                        break
                                    except Exception as e:
                                        print(f"âš ï¸ base64å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                                elif image_url.startswith("http"):
                                    # å¤„ç†å¤–éƒ¨å›¾åƒURL
                                    try:
                                        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å›¾åƒ: {image_url}")
                                        response = requests.get(image_url, timeout=30)
                                        if response.status_code == 200:
                                            image_bytes = response.content
                                            generated_image = Image.open(io.BytesIO(image_bytes))
                                            print("âœ… æˆåŠŸä¸‹è½½å¹¶æå–å¤–éƒ¨å›¾åƒ")
                                            # æ¸…ç†å“åº”æ–‡æœ¬ï¼Œç§»é™¤URL
                                            response_text = re.sub(r'!\[image\]\([^)]+\)', '[å›¾åƒå·²ç”Ÿæˆ]', content)
                                            break
                                        else:
                                            print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                    except Exception as e:
                                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
                                else:
                                    print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾åƒæ ¼å¼: {image_url}")
                        except Exception as e:
                            print(f"âš ï¸ å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                            
                elif isinstance(content, list):
                    # å¤„ç†å¤šæ¨¡æ€å†…å®¹
                    for item in content:
                        if item.get("type") == "text":
                            response_text += item.get("text", "")
                        elif item.get("type") == "image_url":
                            # å¤„ç†å›¾åƒURLï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                            print("ğŸ–¼ï¸ æ£€æµ‹åˆ°å›¾åƒURLå“åº”")
                            image_url = item.get("image_url", {}).get("url", "")
                            if image_url.startswith("data:image/"):
                                try:
                                    # æå–base64æ•°æ®
                                    image_data = image_url.split(",")[1]
                                    image_bytes = base64.b64decode(image_data)
                                    generated_image = Image.open(io.BytesIO(image_bytes))
                                    print("âœ… æˆåŠŸæå–base64å›¾åƒæ•°æ®")
                                except Exception as e:
                                    print(f"âš ï¸ base64å›¾åƒæ•°æ®è§£æå¤±è´¥: {e}")
                            elif image_url.startswith("http"):
                                try:
                                    # ä¸‹è½½å¤–éƒ¨å›¾åƒURL
                                    print(f"ğŸ“¥ æ­£åœ¨ä¸‹è½½å›¾åƒ: {image_url}")
                                    response = requests.get(image_url, timeout=30)
                                    if response.status_code == 200:
                                        image_bytes = response.content
                                        generated_image = Image.open(io.BytesIO(image_bytes))
                                        print("âœ… æˆåŠŸä¸‹è½½å¹¶æå–å¤–éƒ¨å›¾åƒ")
                                    else:
                                        print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                                except Exception as e:
                                    print(f"âš ï¸ å›¾åƒä¸‹è½½å¤±è´¥: {e}")
                            else:
                                print(f"âš ï¸ ä¸æ”¯æŒçš„å›¾åƒURLæ ¼å¼: {image_url}")
        
        if not response_text:
            response_text = "OpenAIå…¼å®¹æ ¼å¼å“åº”å¤„ç†å®Œæˆ"
            
    except Exception as e:
        print(f"âš ï¸ OpenAIå…¼å®¹æ ¼å¼å“åº”è§£æå¤±è´¥: {e}")
        response_text = f"å“åº”è§£æå¤±è´¥: {e}"
    
    return response_text, generated_image


# èŠ‚ç‚¹æ˜ å°„ - ä¿æŒä¸å‚è€ƒé¡¹ç›®ä¸€è‡´çš„å‘½åé£æ ¼
NODE_CLASS_MAPPINGS = {
    "KenChenLLMGeminiBananaMirrorImageGenNode": KenChenLLMGeminiBananaMirrorImageGenNode,
    "KenChenLLMGeminiBananaMirrorImageEditNode": KenChenLLMGeminiBananaMirrorImageEditNode,
    "GeminiBananaMirrorMultiImageEdit": KenChenLLMGeminiBananaMultiImageEditNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "KenChenLLMGeminiBananaMirrorImageGenNode": "Gemini Banana å›¾åƒç”Ÿæˆ",
    "KenChenLLMGeminiBananaMirrorImageEditNode": "Gemini Banana å›¾ç‰‡ç¼–è¾‘",
    "GeminiBananaMirrorMultiImageEdit": "Gemini Banana Mirror å¤šå›¾åƒç¼–è¾‘",
}

