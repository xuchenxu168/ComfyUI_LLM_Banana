import os
import base64
import io
import json
import torch
import numpy as np
from PIL import Image
import requests
from io import BytesIO
import traceback
import tempfile
import wave
import random
import time
from typing import Tuple, Optional

# ğŸš€ nano-bananaå®˜æ–¹è°ƒç”¨æ–¹å¼å·²é›†æˆ
# gemini_banana.py å·²ç»åŒ…å«äº†å®Œæ•´çš„nano-bananaå®˜æ–¹è°ƒç”¨å®ç°
# åŒ…æ‹¬ï¼šgenerate_with_priority_api, generate_with_official_api, generate_with_rest_api ç­‰
NANO_BANANA_OFFICIAL_AVAILABLE = True
print("âœ… nano-bananaå®˜æ–¹è°ƒç”¨æ–¹å¼å·²é›†æˆåˆ°gemini_bananaæ¨¡å—")

# ğŸŒ å¯¼å…¥ç‹¬ç«‹çš„ç¿»è¯‘æ¨¡å—
try:
    from gemini_banana_translation import (
        KenChenLLMGeminiBananaTextTranslationNode as TranslationNode,
        NODE_CLASS_MAPPINGS as TRANSLATION_NODE_MAPPINGS,
        NODE_DISPLAY_NAME_MAPPINGS as TRANSLATION_DISPLAY_MAPPINGS
    )
    TRANSLATION_MODULE_AVAILABLE = True
    print("âœ… æˆåŠŸå¯¼å…¥ç‹¬ç«‹ç¿»è¯‘æ¨¡å—")
except ImportError:
    try:
        from .gemini_banana_translation import (
            KenChenLLMGeminiBananaTextTranslationNode as TranslationNode,
            NODE_CLASS_MAPPINGS as TRANSLATION_NODE_MAPPINGS,
            NODE_DISPLAY_NAME_MAPPINGS as TRANSLATION_DISPLAY_MAPPINGS
        )
        TRANSLATION_MODULE_AVAILABLE = True
        print("âœ… æˆåŠŸå¯¼å…¥ç‹¬ç«‹ç¿»è¯‘æ¨¡å—")
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥ç¿»è¯‘æ¨¡å—ï¼Œç¿»è¯‘åŠŸèƒ½å°†ä¸å¯ç”¨")
        TRANSLATION_MODULE_AVAILABLE = False
        TranslationNode = None
        TRANSLATION_NODE_MAPPINGS = {}
        TRANSLATION_DISPLAY_MAPPINGS = {}

# ğŸš€ AIæ”¾å¤§æ¨¡å‹é›†æˆ
def detect_available_upscale_models():
    """
    è‡ªåŠ¨æ£€æµ‹å¯ç”¨çš„AIæ”¾å¤§æ¨¡å‹
    """
    available_models = []
    
    # æ£€æµ‹ Real-ESRGAN
    try:
        import realesrgan
        available_models.append("Real-ESRGAN")
        print(f"âœ… æ£€æµ‹åˆ° Real-ESRGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ Real-ESRGAN æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ ESRGAN
    try:
        import esrgan
        available_models.append("ESRGAN")
        print(f"âœ… æ£€æµ‹åˆ° ESRGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ ESRGAN æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ Waifu2x
    try:
        import waifu2x
        available_models.append("Waifu2x")
        print(f"âœ… æ£€æµ‹åˆ° Waifu2x æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ Waifu2x æ¨¡å‹æœªå®‰è£…")
    
    # æ£€æµ‹ GFPGAN
    try:
        import gfpgan
        available_models.append("GFPGAN")
        print(f"âœ… æ£€æµ‹åˆ° GFPGAN æ¨¡å‹")
    except ImportError:
        print(f"âš ï¸ GFPGAN æ¨¡å‹æœªå®‰è£…")
    
    print(f"ğŸ” å¯ç”¨AIæ”¾å¤§æ¨¡å‹: {available_models}")
    return available_models

def ai_upscale_with_realesrgan(image, target_width, target_height, gigapixel_model="High Fidelity"):
    """
    ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰ï¼Œä¼˜å…ˆ2xï¼Œå¤±è´¥å›é€€LANCZOSã€‚
    """
    try:
        from .banana_upscale import smart_upscale as _smart
        res = _smart(image, target_width, target_height, gigapixel_model)
        if res is not None:
            return res
        print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
        return image.resize((target_width, target_height), Image.Resampling.LANCZOS)
    except Exception as e:
        print(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥ï¼Œå›é€€é‡é‡‡æ ·: {e}")
        return image.resize((target_width, target_height), Image.Resampling.LANCZOS)
def ai_upscale_with_esrgan(image, target_width, target_height):
    """
    ä½¿ç”¨ ESRGAN è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§
    """
    try:
        print(f"ğŸš€ ä½¿ç”¨ ESRGAN è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§...")
        
        # ESRGAN å®ç°ä»£ç 
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„ESRGANå®ç°æ¥ç¼–å†™
        
        print(f"âœ… ESRGAN AIæ”¾å¤§å®Œæˆ")
        return image  # ä¸´æ—¶è¿”å›åŸå›¾
        
    except Exception as e:
        print(f"âŒ ESRGAN æ”¾å¤§å¤±è´¥: {e}")
        raise e

def ai_upscale_with_waifu2x(image, target_width, target_height):
    """
    ä½¿ç”¨ Waifu2x è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§
    """
    try:
        print(f"ğŸš€ ä½¿ç”¨ Waifu2x è¿›è¡ŒAIé«˜æ¸…æ”¾å¤§...")
        
        # Waifu2x å®ç°ä»£ç 
        # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“çš„Waifu2xå®ç°æ¥ç¼–å†™
        
        print(f"âœ… Waifu2x AIæ”¾å¤§å®Œæˆ")
        return image  # ä¸´æ—¶è¿”å›åŸå›¾
        
    except Exception as e:
        print(f"âŒ Waifu2x æ”¾å¤§å¤±è´¥: {e}")
        raise e

def smart_ai_upscale(image, target_width, target_height, gigapixel_model="High Fidelity"):
	"""
	ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰
	"""
	try:
		from .banana_upscale import smart_upscale as _smart
		return _smart(image, target_width, target_height, gigapixel_model)
	except Exception as e:
		_log_warning(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥: {e}")
		return None

try:
    from server import PromptServer
except Exception:
    PromptServer = None

def _log_info(message):
    pass  # å…³é—­è°ƒè¯•ä¿¡æ¯

def _log_warning(message):
    pass  # å…³é—­è°ƒè¯•ä¿¡æ¯

def _log_error(message):
    try:
        print(f"[LLM Agent Assistant][Gemini-Banana] ERROR: {message}")
    except UnicodeEncodeError:
        print(f"[LLM Prompt][Gemini-Banana] ERROR: {repr(message)}")

def smart_retry_delay(attempt, error_code=None):
    """æ™ºèƒ½é‡è¯•å»¶è¿Ÿ - æ ¹æ®é”™è¯¯ç±»å‹è°ƒæ•´ç­‰å¾…æ—¶é—´"""
    base_delay = 2 ** attempt  # æŒ‡æ•°é€€é¿
    
    if error_code == 429:  # é™æµé”™è¯¯
        # å¯¹äº429é”™è¯¯ï¼Œä½¿ç”¨æ›´é•¿çš„ç­‰å¾…æ—¶é—´
        rate_limit_delay = 60 + random.uniform(10, 30)  # 60-90ç§’éšæœºç­‰å¾…
        return max(base_delay, rate_limit_delay)
    elif error_code in [500, 502, 503, 504]:  # æœåŠ¡å™¨é”™è¯¯
        return base_delay + random.uniform(1, 5)  # æ·»åŠ éšæœºæŠ–åŠ¨
    else:
        return base_delay

def resize_image_for_api(image, max_size=2048):
    """è°ƒæ•´å›¾åƒå¤§å°ä»¥æ»¡è¶³APIé™åˆ¶"""
    if max(image.size) > max_size:
        ratio = max_size / max(image.size)
        new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))
        image = image.resize(new_size, Image.Resampling.LANCZOS)
        _log_info(f"Image resized to {new_size} for API compatibility")
    return image

def remove_white_areas(image: Image.Image, white_threshold: int = 240) -> Image.Image:
    """
    æ£€æµ‹å¹¶å»é™¤å›¾åƒä¸­çš„ç™½è‰²åŒºåŸŸ

    Args:
        image: è¾“å…¥å›¾åƒ
        white_threshold: ç™½è‰²é˜ˆå€¼ï¼Œåƒç´ å€¼å¤§äºæ­¤å€¼è¢«è®¤ä¸ºæ˜¯ç™½è‰² (0-255)

    Returns:
        å»é™¤ç™½è‰²åŒºåŸŸåçš„å›¾åƒ
    """
    try:
        import numpy as np

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        img_array = np.array(image)
        height, width = img_array.shape[:2]

        _log_info(f"ğŸ” å¼€å§‹æ£€æµ‹ç™½è‰²åŒºåŸŸï¼Œé˜ˆå€¼: {white_threshold}")

        # å¤šç§ç™½è‰²æ£€æµ‹ç­–ç•¥
        white_masks = []

        # ç­–ç•¥1: ä¸¥æ ¼ç™½è‰²æ£€æµ‹ (RGBä¸‰ä¸ªé€šé“éƒ½å¤§äºé˜ˆå€¼)
        if len(img_array.shape) == 3:  # RGBå›¾åƒ
            strict_white_mask = np.all(img_array >= white_threshold, axis=2)
            white_masks.append(strict_white_mask)

            # ç­–ç•¥2: è¿‘ä¼¼ç™½è‰²æ£€æµ‹ (RGBå·®å¼‚å°ä¸”å¹³å‡å€¼é«˜)
            rgb_mean = np.mean(img_array, axis=2)
            rgb_std = np.std(img_array, axis=2)
            approx_white_mask = (rgb_mean >= white_threshold - 20) & (rgb_std <= 25)
            white_masks.append(approx_white_mask)

            # ç­–ç•¥3: çº¯ç™½è‰²æ£€æµ‹ (RGB = 255)
            pure_white_mask = np.all(img_array == 255, axis=2)
            white_masks.append(pure_white_mask)

        else:  # ç°åº¦å›¾åƒ
            white_mask = img_array >= white_threshold
            white_masks.append(white_mask)

        # åˆå¹¶æ‰€æœ‰ç™½è‰²æ£€æµ‹ç»“æœ
        combined_white_mask = np.logical_or.reduce(white_masks)

        # è®¡ç®—ç™½è‰²åƒç´ æ¯”ä¾‹
        white_ratio = np.sum(combined_white_mask) / (height * width)
        _log_info(f"ğŸ” ç™½è‰²åƒç´ æ¯”ä¾‹: {white_ratio:.2%}")

        # é™ä½ç™½è‰²åƒç´ æ¯”ä¾‹é˜ˆå€¼ï¼Œæ›´å®¹æ˜“æ£€æµ‹åˆ°ç™½è‰²åŒºåŸŸ
        if white_ratio < 0.01:  # å°äº1%
            _log_info(f"â„¹ï¸ ç™½è‰²åƒç´ æ¯”ä¾‹è¾ƒä½({white_ratio:.2%})ï¼Œè·³è¿‡å¤„ç†")
            return image

        # æ‰¾åˆ°éç™½è‰²åŒºåŸŸçš„è¾¹ç•Œæ¡†
        non_white_mask = ~combined_white_mask

        # æ‰¾åˆ°éç™½è‰²åƒç´ çš„è¡Œå’Œåˆ—
        non_white_rows = np.any(non_white_mask, axis=1)
        non_white_cols = np.any(non_white_mask, axis=0)

        # å¦‚æœæ²¡æœ‰éç™½è‰²åƒç´ ï¼Œè¿”å›åŸå›¾
        if not np.any(non_white_rows) or not np.any(non_white_cols):
            _log_warning(f"âš ï¸ å›¾åƒå‡ ä¹å…¨æ˜¯ç™½è‰²ï¼Œä¿æŒåŸå›¾")
            return image

        # æ‰¾åˆ°è¾¹ç•Œ
        top = np.argmax(non_white_rows)
        bottom = len(non_white_rows) - 1 - np.argmax(non_white_rows[::-1])
        left = np.argmax(non_white_cols)
        right = len(non_white_cols) - 1 - np.argmax(non_white_cols[::-1])

        # æ£€æµ‹è¾¹ç¼˜ç™½è‰²åŒºåŸŸçš„åšåº¦
        edge_thickness = {
            'top': top,
            'bottom': height - 1 - bottom,
            'left': left,
            'right': width - 1 - right
        }

        _log_info(f"ğŸ” è¾¹ç¼˜ç™½è‰²åšåº¦: ä¸Š{edge_thickness['top']}, ä¸‹{edge_thickness['bottom']}, å·¦{edge_thickness['left']}, å³{edge_thickness['right']}")

        # æ›´æ™ºèƒ½çš„ç™½è‰²è¾¹æ¡†æ£€æµ‹é€»è¾‘
        min_edge_thickness = max(20, width // 15, height // 15)  # é™ä½é˜ˆå€¼ï¼šè‡³å°‘20åƒç´ æˆ–å›¾åƒå°ºå¯¸çš„6.7%

        # æ£€æŸ¥æ˜¯å¦æ˜¯çœŸæ­£çš„è¾¹æ¡†
        thick_edges = [k for k, v in edge_thickness.items() if v >= min_edge_thickness]

        # ç‰¹åˆ«æ£€æŸ¥åº•éƒ¨ç™½è‰²åŒºåŸŸï¼ˆå¸¸è§çš„ç”Ÿæˆå›¾åƒé—®é¢˜ï¼‰
        bottom_white_ratio = edge_thickness['bottom'] / height if height > 0 else 0

        _log_info(f"ğŸ” åšè¾¹æ£€æµ‹: {thick_edges}, åº•éƒ¨ç™½è‰²æ¯”ä¾‹: {bottom_white_ratio:.2%}")

        # æ›´å®½æ¾çš„è¾¹æ¡†æ£€æµ‹æ¡ä»¶ï¼š
        # 1. å››è¾¹éƒ½æœ‰åšç™½è¾¹
        # 2. å¯¹è¾¹éƒ½æœ‰åšç™½è¾¹ï¼ˆä¸Šä¸‹æˆ–å·¦å³ï¼‰
        # 3. ä¸‰è¾¹æœ‰åšç™½è¾¹
        # 4. åº•éƒ¨æœ‰å¤§é¢ç§¯ç™½è‰²åŒºåŸŸï¼ˆ>15%ï¼‰
        is_border = (
            len(thick_edges) >= 3 or  # ä¸‰è¾¹æˆ–å››è¾¹æœ‰åšç™½è¾¹
            ('top' in thick_edges and 'bottom' in thick_edges) or  # ä¸Šä¸‹éƒ½æœ‰
            ('left' in thick_edges and 'right' in thick_edges) or  # å·¦å³éƒ½æœ‰
            bottom_white_ratio > 0.15  # åº•éƒ¨ç™½è‰²åŒºåŸŸè¶…è¿‡15%
        )

        if not is_border:
            _log_info(f"â„¹ï¸ ä¸æ˜¯çœŸæ­£çš„ç™½è‰²è¾¹æ¡†ï¼Œè·³è¿‡è£å‰ªã€‚åšè¾¹: {thick_edges}, åº•éƒ¨ç™½è‰²æ¯”ä¾‹: {bottom_white_ratio:.2%}")
            return image

        _log_info(f"âœ… æ£€æµ‹åˆ°ç™½è‰²è¾¹æ¡†ï¼Œåšè¾¹: {thick_edges}")

        # æ·»åŠ ä¸€äº›è¾¹è·ï¼Œé¿å…è£å‰ªè¿‡ç´§
        margin = min(5, width // 50, height // 50)  # æœ€å¤š5åƒç´ æˆ–å›¾åƒå°ºå¯¸çš„2%
        top = max(0, top - margin)
        bottom = min(height - 1, bottom + margin)
        left = max(0, left - margin)
        right = min(width - 1, right + margin)

        # æ£€æŸ¥è£å‰ªåŒºåŸŸæ˜¯å¦æœ‰æ•ˆ
        crop_width = right - left + 1
        crop_height = bottom - top + 1

        if crop_width <= 0 or crop_height <= 0:
            _log_warning(f"âš ï¸ è£å‰ªåŒºåŸŸæ— æ•ˆï¼Œä¿æŒåŸå›¾")
            return image

        # è®¡ç®—è£å‰ªæ¯”ä¾‹
        crop_ratio = (crop_width * crop_height) / (width * height)

        # å¦‚æœè£å‰ªåçš„åŒºåŸŸå¤ªå°ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤
        if crop_ratio < 0.3:  # å°äº30%
            _log_warning(f"âš ï¸ è£å‰ªååŒºåŸŸè¿‡å°({crop_ratio:.2%})ï¼Œå¯èƒ½æ˜¯è¯¯åˆ¤ï¼Œä¿æŒåŸå›¾")
            return image

        _log_info(f"âœ… æ£€æµ‹åˆ°ç™½è‰²è¾¹æ¡†ï¼Œè£å‰ªåŒºåŸŸ: ({left}, {top}) -> ({right}, {bottom})")
        _log_info(f"âœ… è£å‰ªå°ºå¯¸: {crop_width}x{crop_height} (ä¿ç•™{crop_ratio:.1%})")

        # è£å‰ªå›¾åƒ
        cropped_image = image.crop((left, top, right + 1, bottom + 1))

        return cropped_image

    except Exception as e:
        _log_warning(f"âŒ ç™½è‰²åŒºåŸŸæ£€æµ‹å¤±è´¥: {e}")
        import traceback
        _log_warning(f"ğŸ” è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return image

def smart_resize_with_padding(image: Image.Image, target_size: Tuple[int, int],
                             fill_color: Tuple[int, int, int] = (255, 255, 255),
                             fill_strategy: str = "smart", gigapixel_model: str = "High Fidelity") -> Image.Image:
    """
    ğŸš€ ç›´æ¥ç›®æ ‡å°ºå¯¸æ‰©å›¾æŠ€æœ¯ï¼ŒæŒ‰æ§åˆ¶å°ºå¯¸è¦æ±‚ç›´æ¥æ‰©å›¾
    å½»åº•è§£å†³è¿‡åº¦æ‰©å›¾é—®é¢˜ï¼Œç›´æ¥æ‰©åˆ°ç›®æ ‡å°ºå¯¸
    
    Args:
        image: è¾“å…¥å›¾åƒ
        target_size: ç›®æ ‡å°ºå¯¸ (width, height)
        fill_color: å¡«å……é¢œè‰²ï¼Œé»˜è®¤ç™½è‰²
        fill_strategy: å¡«å……ç­–ç•¥
            - "smart": æ™ºèƒ½é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼ˆä¼˜å…ˆä½¿ç”¨ cropï¼Œé¿å…é‡å ä¸”æ— ç™½è¾¹ï¼Œå‚è€ƒå®˜æ–¹åˆ†æ”¯ï¼‰
            - "direct": ç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡å°ºå¯¸ï¼ˆå¯èƒ½å˜å½¢ï¼Œè°¨æ…ä½¿ç”¨ï¼‰
            - "crop": è£å‰ªæ¨¡å¼ï¼ˆæ— å¡«å……ï¼Œæ— é‡å ï¼Œé«˜æ¸…æ”¾å¤§åè£å‰ªï¼‰
            - "paste": ç²˜è´´æ¨¡å¼ï¼ˆæœ‰å¡«å……ï¼Œä¸»ä½“å®Œå…¨å¯è§ï¼‰
            - "extend": èƒŒæ™¯æ‰©å±•æ¨¡å¼ï¼ˆç­‰æ¯”ç¼©æ”¾è´´ä¸­é—´ + èƒŒæ™¯å†…å®¹æ‰©å±•ï¼Œå¯èƒ½æœ‰é‡å ï¼‰
    """
    img_width, img_height = image.size
    target_width, target_height = target_size

    _log_info(f"ğŸ¯ å¼€å§‹ç›´æ¥ç›®æ ‡å°ºå¯¸æ‰©å›¾æŠ€æœ¯: {img_width}x{img_height} -> {target_width}x{target_height}")
    _log_info(f"ğŸ¯ å¡«å……ç­–ç•¥: {fill_strategy}")

    # ğŸš€ ç¬¬ä¸€æ­¥ï¼šæ£€æµ‹å¹¶å»é™¤ç™½è‰²åŒºåŸŸ
    processed_image = remove_white_areas(image)
    if processed_image.size != image.size:
        _log_info(f"âœ… ç™½è‰²åŒºåŸŸå·²å»é™¤: {image.size} -> {processed_image.size}")
        image = processed_image
        img_width, img_height = image.size

        # å¦‚æœè¿˜æœ‰ç™½è‰²åŒºåŸŸï¼Œå°è¯•æ›´æ¿€è¿›çš„æ£€æµ‹
        processed_image2 = remove_white_areas(image, white_threshold=230)
        if processed_image2.size != image.size:
            _log_info(f"âœ… æ¿€è¿›æ¨¡å¼å†æ¬¡å»é™¤ç™½è‰²åŒºåŸŸ: {image.size} -> {processed_image2.size}")
            image = processed_image2
            img_width, img_height = image.size
    else:
        _log_info(f"â„¹ï¸ æœªæ£€æµ‹åˆ°éœ€è¦å»é™¤çš„ç™½è‰²åŒºåŸŸ")

    # ğŸ¯ ç­–ç•¥1ï¼šæ¯”ä¾‹ç›¸åŒæ—¶ï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸
    if abs(img_width/img_height - target_width/target_height) < 0.01:
        _log_info(f"ğŸ¯ æ¯”ä¾‹ç›¸åŒï¼Œç›´æ¥è°ƒæ•´å°ºå¯¸")
        resized_img = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        return resized_img
    
    # ğŸ¯ ç­–ç•¥2ï¼šæ¯”ä¾‹ä¸åŒæ—¶
    _log_info(f"ğŸ¯ æ¯”ä¾‹ä¸åŒï¼Œé€‰æ‹©åˆé€‚ç­–ç•¥")
    
    # é»˜è®¤æ™ºèƒ½ç­–ç•¥ï¼šèµ° cropï¼Œé¿å…é‡å ä¸”æ— ç™½è¾¹ï¼ˆå‚è€ƒå®˜æ–¹åˆ†æ”¯ï¼‰
    if fill_strategy == "smart":
        fill_strategy = "crop"
    
    if fill_strategy == "extend":
        # ç­‰æ¯”ç¼©æ”¾è‡³ä¸è¶…è¿‡ç›®æ ‡å°ºå¯¸
        scale_x = target_width / img_width
        scale_y = target_height / img_height
        scale = min(scale_x, scale_y)
        new_width = max(1, int(img_width * scale))
        new_height = max(1, int(img_height * scale))
        _log_info(f"ğŸ¯ extend ç¼©æ”¾å°ºå¯¸: {new_width}x{new_height} (scale={scale:.3f})")
        fg = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        
        # èƒŒæ™¯ï¼šå…ˆæŒ‰ cover ç”Ÿæˆä¸€å¼ é“ºæ»¡ç›®æ ‡çš„èƒŒæ™¯ï¼Œå†é«˜æ–¯æ¨¡ç³Šï¼Œé¿å…ç™½è¾¹
        cover_scale = max(scale_x, scale_y)
        bg_w = max(1, int(img_width * cover_scale))
        bg_h = max(1, int(img_height * cover_scale))
        bg = image.resize((bg_w, bg_h), Image.Resampling.LANCZOS)
        crop_x = (bg_w - target_width) // 2
        crop_y = (bg_h - target_height) // 2
        bg = bg.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
        try:
            from PIL import ImageFilter
            bg = bg.filter(ImageFilter.GaussianBlur(radius=24))
        except Exception:
            pass
        
        # å°†å‰æ™¯ç­‰æ¯”ç¼©æ”¾å›¾ç²˜è´´åˆ°ä¸­å¿ƒï¼ˆç¡®ä¿å®Œå…¨è¦†ç›–èƒŒæ™¯ï¼Œé¿å…é‡å ï¼‰
        paste_x = (target_width - new_width) // 2
        paste_y = (target_height - new_height) // 2

        _log_info(f"ğŸ¯ å‰æ™¯å°ºå¯¸: {new_width}x{new_height}, ç²˜è´´ä½ç½®: ({paste_x}, {paste_y})")

        # åˆ›å»ºä¸€ä¸ªæ–°çš„ç”»å¸ƒï¼Œç¡®ä¿æ²¡æœ‰é‡å é—®é¢˜
        result = Image.new('RGB', (target_width, target_height))
        result.paste(bg, (0, 0))  # å…ˆç²˜è´´èƒŒæ™¯

        # ç¡®ä¿å‰æ™¯å›¾åƒæ˜¯RGBæ¨¡å¼ï¼Œé¿å…é€æ˜åº¦é—®é¢˜
        if fg.mode != 'RGB':
            fg = fg.convert('RGB')

        # ğŸ”§ å…³é”®ä¿®å¤ï¼šåˆ›å»ºä¸€ä¸ªé®ç½©æ¥ç¡®ä¿å‰æ™¯å®Œå…¨è¦†ç›–èƒŒæ™¯çš„ç›¸åº”åŒºåŸŸ
        # å…ˆåœ¨å‰æ™¯åŒºåŸŸå¡«å……çº¯è‰²ï¼Œç„¶åç²˜è´´å‰æ™¯ï¼Œé¿å…ä»»ä½•é‡å æ•ˆæœ
        if paste_x >= 0 and paste_y >= 0 and paste_x + new_width <= target_width and paste_y + new_height <= target_height:
            # åœ¨å‰æ™¯åŒºåŸŸå…ˆå¡«å……èƒŒæ™¯è‰²ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
            from PIL import ImageDraw
            draw = ImageDraw.Draw(result)
            # è·å–èƒŒæ™¯çš„å¹³å‡é¢œè‰²ä½œä¸ºå¡«å……è‰²
            try:
                bg_sample = bg.resize((1, 1), Image.Resampling.LANCZOS)
                avg_color = bg_sample.getpixel((0, 0))
                if isinstance(avg_color, int):
                    avg_color = (avg_color, avg_color, avg_color)
            except:
                avg_color = fill_color

            # åœ¨å‰æ™¯åŒºåŸŸå¡«å……å¹³å‡è‰²ï¼Œç¡®ä¿æ²¡æœ‰é‡å 
            draw.rectangle([paste_x, paste_y, paste_x + new_width, paste_y + new_height], fill=avg_color)

            # ç„¶åç²˜è´´å‰æ™¯å›¾åƒ
            result.paste(fg, (paste_x, paste_y))
        else:
            _log_warning(f"âš ï¸ å‰æ™¯å›¾åƒè¶…å‡ºè¾¹ç•Œï¼Œè°ƒæ•´ç²˜è´´ä½ç½®")
            # å¦‚æœè¶…å‡ºè¾¹ç•Œï¼Œç›´æ¥å±…ä¸­ç²˜è´´ï¼Œå¯èƒ½ä¼šè£å‰ª
            safe_paste_x = max(0, min(paste_x, target_width - new_width))
            safe_paste_y = max(0, min(paste_y, target_height - new_height))

            # åŒæ ·å…ˆå¡«å……å†ç²˜è´´
            from PIL import ImageDraw
            draw = ImageDraw.Draw(result)
            try:
                bg_sample = bg.resize((1, 1), Image.Resampling.LANCZOS)
                avg_color = bg_sample.getpixel((0, 0))
                if isinstance(avg_color, int):
                    avg_color = (avg_color, avg_color, avg_color)
            except:
                avg_color = fill_color

            draw.rectangle([safe_paste_x, safe_paste_y, safe_paste_x + new_width, safe_paste_y + new_height], fill=avg_color)
            result.paste(fg, (safe_paste_x, safe_paste_y))

        _log_info(f"âœ… extend å®Œæˆï¼šæ— ç™½è¾¹ã€ä¸å˜å½¢ï¼Œè¾“å‡º {result.size}")
        return result
    
    if fill_strategy in ["direct"]:
        # ğŸ¯ ç›´æ¥æ‰©å›¾æ¨¡å¼ï¼šç›´æ¥æ‰©åˆ°ç›®æ ‡å°ºå¯¸ï¼ˆå¯èƒ½å˜å½¢ï¼Œè°¨æ…ä½¿ç”¨ï¼‰
        _log_info(f"âš ï¸ direct æ¨¡å¼ï¼šç›´æ¥ç¼©æ”¾åˆ°ç›®æ ‡ï¼Œå¯èƒ½å˜å½¢")
        final_image = image.resize((target_width, target_height), Image.Resampling.LANCZOS)
        return final_image
        
    elif fill_strategy == "crop":
        # ğŸ¯ è£å‰ªæ¨¡å¼ï¼šä½¿ç”¨é«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œç„¶åæ™ºèƒ½è£å‰ª
        _log_info(f"ğŸ¯ è£å‰ªæ¨¡å¼ï¼šé«˜æ¸…æ— æŸæ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œç„¶åæ™ºèƒ½è£å‰ª")
        
        # ğŸš€ é«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œä¸æ‹‰ä¼¸å˜å½¢ï¼‰
        # è®¡ç®—æœ€ä½³ç¼©æ”¾æ¯”ä¾‹ï¼Œä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ç›®æ ‡åŒºåŸŸ
        scale_x = target_width / img_width      # å®½åº¦æ¯”ä¾‹
        scale_y = target_height / img_height    # é«˜åº¦æ¯”ä¾‹
        scale = max(scale_x, scale_y)  # ä½¿ç”¨è¾ƒå¤§çš„ç¼©æ”¾æ¯”ä¾‹ï¼Œç¡®ä¿å®Œå…¨è¦†ç›–
        
        # è®¡ç®—æ”¾å¤§åçš„å°ºå¯¸ï¼ˆä¿æŒåŸå§‹æ¯”ä¾‹ï¼Œç¡®ä¿è¦†ç›–ç›®æ ‡åŒºåŸŸï¼‰
        enlarged_width = int(img_width * scale)
        enlarged_height = int(img_height * scale)
        
        _log_info(f"ğŸ”§ é«˜æ¸…æ— æŸæ”¾å¤§: {img_width}x{img_height} -> {enlarged_width}x{enlarged_height}")
        _log_info(f"ğŸ”§ ç¼©æ”¾æ¯”ä¾‹: {scale:.3f} (ä½¿ç”¨maxç¡®ä¿å®Œå…¨è¦†ç›–ï¼Œç„¶åæ™ºèƒ½è£å‰ª)")
        _log_info(f"ğŸ”§ å…³é”®ï¼šç›´æ¥æ”¾å¤§åˆ°æœ€å¤§è¾¹ï¼Œä¿æŒå›¾åƒæ¸…æ™°åº¦å’Œæ¯”ä¾‹")
        
        # ğŸ¯ ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ— æŸæ”¾å¤§ï¼ˆä¿æŒæ¯”ä¾‹ï¼‰
        # ä¼˜å…ˆä½¿ç”¨AIæ¨¡å‹ï¼Œå›é€€åˆ°é«˜è´¨é‡é‡é‡‡æ ·
        try:
            _log_info(f"ğŸ”§ å°è¯•ä½¿ç”¨AIæ”¾å¤§æ¨¡å‹è¿›è¡Œé«˜æ¸…æ”¾å¤§...")
            ai_upscaled_image = smart_ai_upscale(image, enlarged_width, enlarged_height, gigapixel_model)
            
            if ai_upscaled_image is not None:
                # å¦‚æœAIæ”¾å¤§æˆåŠŸï¼Œè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸
                if ai_upscaled_image.size != (enlarged_width, enlarged_height):
                    _log_info(f"ğŸ”§ AIæ”¾å¤§åè°ƒæ•´åˆ°ç›®æ ‡å°ºå¯¸: {ai_upscaled_image.size} -> {enlarged_width}x{enlarged_height}")
                    enlarged_image = ai_upscaled_image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                else:
                    enlarged_image = ai_upscaled_image
                _log_info(f"âœ… AIæ”¾å¤§æ¨¡å‹æ”¾å¤§å®Œæˆï¼Œå›¾åƒè´¨é‡å¤§å¹…æå‡")
            else:
                _log_warning(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·")
                enlarged_image = image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
                
        except Exception as e:
            _log_warning(f"âš ï¸ AIæ”¾å¤§æ¨¡å‹å¤±è´¥ï¼Œä½¿ç”¨é«˜è´¨é‡é‡é‡‡æ ·: {e}")
            # å›é€€åˆ° LANCZOS ç®—æ³•
            enlarged_image = image.resize((enlarged_width, enlarged_height), Image.Resampling.LANCZOS)
        
        # ğŸ¯ æ™ºèƒ½è£å‰ª - ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
        if enlarged_width >= target_width and enlarged_height >= target_height:
            _log_info(f"ğŸ”§ æ™ºèƒ½è£å‰ªï¼šä»é«˜æ¸…æ”¾å¤§å›¾åƒä¸­è£å‰ªç›®æ ‡å°ºå¯¸ï¼Œç¡®ä¿ä¸»ä½“å±…ä¸­")
            
            # ğŸ¯ ç²¾ç¡®è®¡ç®—è£å‰ªåŒºåŸŸï¼Œç¡®ä¿ä¸»ä½“å®Œå…¨å±…ä¸­
            crop_x = (enlarged_width - target_width) // 2
            crop_y = (enlarged_height - target_height) // 2
            
            # ğŸ¯ å¾®è°ƒåç§»ï¼Œç¡®ä¿å®Œå…¨å±…ä¸­ï¼ˆé¿å…å¥‡æ•°åƒç´ åå·®ï¼‰
            if (enlarged_width - target_width) % 2 == 1:
                crop_x += 1
            if (enlarged_height - target_height) % 2 == 1:
                crop_y += 1
            
            _log_info(f"ğŸ”§ ç²¾ç¡®å±…ä¸­è£å‰ªåŒºåŸŸ: ({crop_x}, {crop_y}) -> ({crop_x + target_width}, {crop_y + target_height})")
            _log_info(f"ğŸ”§ ç¡®ä¿ä¸»ä½“åœ¨è£å‰ªåå›¾åƒçš„æ­£ä¸­å¿ƒä½ç½®")
            
            # ä»é«˜æ¸…æ”¾å¤§çš„å›¾åƒä¸­è£å‰ªå‡ºç›®æ ‡å°ºå¯¸
            final_image = enlarged_image.crop((crop_x, crop_y, crop_x + target_width, crop_y + target_height))
            
            _log_info(f"âœ… é«˜æ¸…æ— æŸæ”¾å¤§ + æ™ºèƒ½è£å‰ªå®Œæˆ")
            _log_info(f"âœ… ç»“æœï¼šæ— ç™½è‰²å¡«å……ï¼Œå®Œå…¨ä¸å˜å½¢ï¼Œä¸»ä½“ç²¾ç¡®å±…ä¸­ï¼Œä¿æŒæœ€é«˜æ¸…æ™°åº¦")
            _log_info(f"âœ… å›¾åƒè´¨é‡ï¼šé«˜æ¸…æ— æŸï¼Œæ¯”ä¾‹å®Œç¾ï¼Œä¸»ä½“å¯è§")
            
            return final_image
            
        else:
            _log_warning(f"âš ï¸ é«˜æ¸…æ”¾å¤§åå°ºå¯¸ä¸è¶³ï¼Œä½¿ç”¨æ™ºèƒ½å¡«å……ï¼ˆé¿å…æ‹‰ä¼¸å˜å½¢ï¼‰")
            # åˆ›å»ºç›®æ ‡å°ºå¯¸çš„ç”»å¸ƒï¼Œä½¿ç”¨å¡«å……è‰²
            final_image = Image.new('RGB', (target_width, target_height), fill_color)
            
            # å°†é«˜æ¸…æ”¾å¤§çš„å›¾åƒå±…ä¸­æ”¾ç½®
            paste_x = (target_width - enlarged_width) // 2
            paste_y = (target_height - enlarged_height) // 2
            final_image.paste(enlarged_image, (paste_x, paste_y))
            
            _log_info(f"âœ… æ™ºèƒ½å¡«å……å®Œæˆï¼šé«˜æ¸…æ”¾å¤§å›¾åƒå±…ä¸­æ”¾ç½®ï¼Œè¾¹ç¼˜ç”¨å¡«å……è‰²")
            return final_image
    
    else:
        # ğŸ¯ ç²˜è´´æ¨¡å¼ï¼šä½¿ç”¨min(scale_x, scale_y)ä¿æŠ¤ä¸»ä½“ï¼Œç•™è¾¹ï¼ˆå¯èƒ½å‡ºç°å¡«å……è‰²ï¼‰
        scale_x = target_width / img_width
        scale_y = target_height / img_height
        scale = min(scale_x, scale_y)
        new_width = int(img_width * scale)
        new_height = int(img_height * scale)
        resized_img = image.resize((new_width, new_height), Image.Resampling.LANCZOS)
        final_image = Image.new('RGB', target_size, fill_color)
        paste_x = (target_width - new_width) // 2
        paste_y = (target_height - new_height) // 2
        final_image.paste(resized_img, (paste_x, paste_y))
        return final_image

def smart_ai_upscale(image, target_width, target_height, gigapixel_model="High Fidelity"):
    """
    ğŸš€ æ™ºèƒ½AIæ”¾å¤§æŠ€æœ¯ - ç»Ÿä¸€å§”æ‰˜åˆ°é€šç”¨æ”¾å¤§å™¨ï¼ˆbanana_upscale.smart_upscaleï¼‰
    """
    try:
        try:
            from .banana_upscale import smart_upscale as _smart
        except ImportError:
            from banana_upscale import smart_upscale as _smart

        # è°ƒç”¨AIæ”¾å¤§
        result = _smart(image, target_width, target_height, gigapixel_model)

        # éªŒè¯ç»“æœæ˜¯å¦æœ‰æ•ˆ
        if result is not None and hasattr(result, 'size') and result.size[0] > 0 and result.size[1] > 0:
            _log_info(f"âœ… AIæ”¾å¤§æˆåŠŸ: {image.size} -> {result.size}")
            return result
        else:
            _log_warning(f"âš ï¸ AIæ”¾å¤§è¿”å›æ— æ•ˆç»“æœï¼Œå›é€€åˆ°æ™®é€šé‡é‡‡æ ·")
            return None

    except Exception as e:
        _log_warning(f"âš ï¸ æ™ºèƒ½æ”¾å¤§å™¨å¤±è´¥: {e}")
        import traceback
        _log_warning(f"âš ï¸ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None


def _analyze_image_type_simple(image: Image.Image) -> str:
    """
    ğŸ¯ ç®€å•å›¾åƒç±»å‹åˆ†æï¼Œç”¨äºé€‰æ‹©AIå¢å¼ºç­–ç•¥
    """
    try:
        # åŸºäºå›¾åƒå°ºå¯¸å’Œç‰¹å¾çš„ç®€å•åˆ†æ
        width, height = image.size

        # å°å°ºå¯¸å›¾åƒå¯èƒ½æ˜¯å¤´åƒæˆ–å›¾æ ‡
        if width <= 512 and height <= 512:
            return "face"

        # æå®½æˆ–æé«˜çš„å›¾åƒå¯èƒ½åŒ…å«æ–‡å­—
        aspect_ratio = max(width, height) / min(width, height)
        if aspect_ratio > 3.0:
            return "text"

        # ä¸­ç­‰å°ºå¯¸çš„æ­£æ–¹å½¢æˆ–æ¥è¿‘æ­£æ–¹å½¢å›¾åƒ
        if 0.7 <= width/height <= 1.3:
            return "art"

        # é»˜è®¤ä¸ºé€šç”¨å›¾åƒ
        return "general"

    except Exception:
        return "general"


def _apply_ai_super_resolution(image: Image.Image, image_type: str = "general", gigapixel_model: str = "High Fidelity") -> Optional[Image.Image]:
    """
    ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºï¼šæ ¹æ®å›¾åƒç±»å‹é€‰æ‹©æœ€ä½³ç­–ç•¥
    """
    try:
        if not image:
            return None

        # æ ¹æ®å›¾åƒç±»å‹ç¡®å®šå¢å¼ºç­–ç•¥
        if image_type == "face":
            # äººè„¸å›¾åƒï¼šé€‚åº¦æ”¾å¤§ï¼Œä¿æŒè‡ªç„¶
            target_scale = 1.8
        elif image_type == "text":
            # æ–‡å­—å›¾åƒï¼šé«˜å€æ”¾å¤§ï¼Œæå‡æ¸…æ™°åº¦
            target_scale = 2.5
        elif image_type == "art":
            # è‰ºæœ¯å›¾åƒï¼šä¸­ç­‰æ”¾å¤§ï¼Œä¿æŒé£æ ¼
            target_scale = 2.0
        else:
            # é€šç”¨å›¾åƒï¼šæ ‡å‡†æ”¾å¤§
            target_scale = 2.0

        # è®¡ç®—ç›®æ ‡å°ºå¯¸
        target_w = int(image.width * target_scale)
        target_h = int(image.height * target_scale)

        _log_info(f"ğŸš€ å°è¯•AIè¶…åˆ†è¾¨ç‡å¢å¼º: {image.size} -> ({target_w}, {target_h})")

        # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§ç³»ç»Ÿ
        enhanced = smart_ai_upscale(image, target_w, target_h, gigapixel_model)
        if enhanced:
            # å¦‚æœæ”¾å¤§æˆåŠŸï¼Œç¼©å›åŸå°ºå¯¸ä»¥ä¿æŒç»†èŠ‚æå‡
            final = enhanced.resize(image.size, Image.Resampling.LANCZOS)
            _log_info(f"âœ… AIè¶…åˆ†è¾¨ç‡å¢å¼ºæˆåŠŸ")
            return final

        return None

    except Exception as e:
        _log_warning(f"AIè¶…åˆ†è¾¨ç‡å¢å¼ºå¤±è´¥: {e}")
        return None


def enhance_image_quality(image: Image.Image, quality: str = "hd", adaptive_mode: str = "disabled", gigapixel_model: str = "High Fidelity") -> Image.Image:
    """
    ğŸš€ å›¾åƒè´¨é‡å¢å¼ºï¼ˆé›†æˆAIè¶…åˆ†è¾¨ç‡æŠ€æœ¯ï¼‰
    - ä¼ ç»Ÿå¢å¼ºï¼šé”åŒ–ã€å¯¹æ¯”åº¦ã€è‰²å½©ã€äº®åº¦å¾®è°ƒ
    - AIå¢å¼ºï¼šæ™ºèƒ½è¶…åˆ†è¾¨ç‡æ”¾å¤§æŠ€æœ¯
    """

    # ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
    if quality in ["ai_enhanced", "ai_ultra"]:
        _log_info(f"ğŸš€ å¯ç”¨AIè¶…åˆ†è¾¨ç‡å¢å¼ºæ¨¡å¼: {quality}")

        # åˆ†æå›¾åƒç±»å‹ä»¥é€‰æ‹©æœ€ä½³AIå¢å¼ºç­–ç•¥
        image_type = _analyze_image_type_simple(image)

        # åº”ç”¨AIè¶…åˆ†è¾¨ç‡å¢å¼º
        ai_enhanced = _apply_ai_super_resolution(image, image_type, gigapixel_model)
        if ai_enhanced:
            image = ai_enhanced
            _log_info(f"âœ… AIè¶…åˆ†è¾¨ç‡å¢å¼ºå®Œæˆ")
        else:
            _log_warning(f"âš ï¸ AIå¢å¼ºå¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿå¢å¼º")

    # ä¼ ç»Ÿè´¨é‡å¢å¼º
    if quality in ["hd", "ai_enhanced"]:
        # åº”ç”¨æ™ºèƒ½é”åŒ–æ»¤é•œ
        from PIL import ImageEnhance
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.3)  # æ¯”å‚è€ƒé¡¹ç›®æ›´å¼º

        # å¯¹æ¯”åº¦å¢å¼º
        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.15)  # æ¯”å‚è€ƒé¡¹ç›®æ›´å¼º

        # è‰²å½©é¥±å’Œåº¦å¢å¼º
        enhancer = ImageEnhance.Color(image)
        image = enhancer.enhance(1.08)  # æ¯”å‚è€ƒé¡¹ç›®æ›´å¼º

        # äº®åº¦ä¼˜åŒ–
        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.02)

    elif quality in ["ultra_hd", "ai_ultra"]:
        from PIL import ImageEnhance
        # è¶…é«˜æ¸…è´¨é‡å¢å¼º
        enhancer = ImageEnhance.Sharpness(image)
        image = enhancer.enhance(1.5)

        enhancer = ImageEnhance.Contrast(image)
        image = enhancer.enhance(1.2)

        enhancer = ImageEnhance.Color(image)
        image = enhancer.enhance(1.12)

        enhancer = ImageEnhance.Brightness(image)
        image = enhancer.enhance(1.05)

    return image

def image_to_base64_enhanced(image: Image.Image, format: str = "PNG") -> str:
    """è¶…è¶Šå‚è€ƒé¡¹ç›®çš„base64è½¬æ¢ï¼Œä¿æŒæœ€é«˜è´¨é‡"""
    buffered = BytesIO()
    image.save(buffered, format=format, quality=95, optimize=True)
    return base64.b64encode(buffered.getvalue()).decode('utf-8')

def process_image_controls(quality: str, style: str) -> dict:
    """
    å¤„ç†å›¾åƒæ§åˆ¶å‚æ•°ï¼Œè¿”å›æ ‡å‡†åŒ–çš„æ§åˆ¶é…ç½®

    æ³¨æ„ï¼šæ ¹æ®Geminiå®˜æ–¹APIæ–‡æ¡£ï¼Œå°ºå¯¸æ§åˆ¶åº”è¯¥é€šè¿‡generationConfig.imageConfig.aspectRatioå‚æ•°å®ç°ï¼Œ
    è€Œä¸æ˜¯é€šè¿‡æç¤ºè¯ã€‚å› æ­¤è¿™ä¸ªå‡½æ•°åªå¤„ç†qualityå’Œstyleå‚æ•°ã€‚

    ä¸ºäº†å‘åå…¼å®¹ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„sizeå€¼ï¼ˆ"Original size"ï¼‰ï¼Œä½†è¿™ä¸ªå€¼ä¸åº”è¯¥è¢«ä½¿ç”¨ã€‚
    æ–°ä»£ç åº”è¯¥ä½¿ç”¨upscale_factorå‚æ•°è¿›è¡Œæ”¾å¤§ã€‚

    Args:
        quality: è´¨é‡è®¾ç½®
        style: é£æ ¼è®¾ç½®

    Returns:
        dict: åŒ…å«å¤„ç†åçš„å›¾åƒæ§åˆ¶å‚æ•°
    """
    # æ„å»ºæ§åˆ¶é…ç½®ï¼ˆåŒ…å«qualityã€styleå’Œé»˜è®¤çš„sizeï¼‰
    controls = {
        "quality": quality,
        "style": style,
        "size": "Original size",  # é»˜è®¤å€¼ï¼Œç”¨äºå‘åå…¼å®¹
        "is_custom_size": False   # é»˜è®¤å€¼ï¼Œç”¨äºå‘åå…¼å®¹
    }

    return controls


def enhance_prompt_with_controls(prompt: str, controls: dict, detail_level: str = "Professional Detail",
                               camera_control: str = "Auto Select", lighting_control: str = "Auto Settings",
                               template_selection: str = "Auto Select", quality_enhancement: bool = True,
                               enhance_quality: bool = True, smart_resize: bool = True, fill_color: str = "255,255,255") -> str:
    """
    ğŸš€ æ™ºèƒ½æç¤ºè¯å¢å¼ºç³»ç»Ÿ

    æ ¹æ®Geminiå®˜æ–¹APIæ–‡æ¡£ï¼Œå›¾åƒå°ºå¯¸åº”è¯¥é€šè¿‡generationConfig.imageConfig.aspectRatioå‚æ•°æ§åˆ¶ï¼Œ
    è€Œä¸æ˜¯é€šè¿‡æç¤ºè¯ã€‚å› æ­¤è¿™ä¸ªå‡½æ•°åªå¤„ç†è´¨é‡å’Œé£æ ¼çš„å¢å¼ºã€‚

    é›†æˆçš„å¢å¼ºåŠŸèƒ½ï¼š
    - æ™ºèƒ½é£æ ¼è¯†åˆ«å’Œæ¨¡æ¿
    - åŠ¨æ€è´¨é‡æ§åˆ¶æŒ‡ä»¤
    - è‰ºæœ¯é£æ ¼å¢å¼º
    - ä¸“ä¸šæ‘„å½±å‚æ•°
    """
    
    # ğŸš€ è¶…è¶Šå‚è€ƒé¡¹ç›®çš„å®Œæ•´é£æ ¼æ¨¡æ¿ç³»ç»Ÿ
    style_templates = {
        # åŸºç¡€é£æ ¼ï¼ˆè¶…è¶Šå‚è€ƒé¡¹ç›®ï¼‰
        "vivid": {
            "prefix": "Create a vivid, high-contrast image with",
            "suffix": "Use vibrant colors and dramatic lighting to make the image pop.",
            "quality_boost": "Ensure maximum visual impact and artistic appeal.",
            "camera_settings": "Use high contrast and saturation settings.",
            "lighting": "Dramatic lighting with strong shadows and highlights."
        },
        "natural": {
            "prefix": "Generate a natural, realistic image of",
            "suffix": "Use natural lighting and authentic colors for a lifelike appearance.",
            "quality_boost": "Focus on realism and natural beauty.",
            "camera_settings": "Natural color balance and realistic exposure.",
            "lighting": "Soft, natural lighting with subtle shadows."
        },
        
        # ğŸ¨ ä¸“ä¸šè‰ºæœ¯é£æ ¼ï¼ˆå‚è€ƒé¡¹ç›®æ ¸å¿ƒåŠŸèƒ½ï¼‰
        "professional_portrait": {
            "prefix": "Create a professional portrait photograph of",
            "suffix": "Use studio lighting, professional composition, and high-end camera settings.",
            "quality_boost": "Achieve magazine-quality portrait photography.",
            "camera_settings": "85mm lens, f/2.8 aperture, professional color grading.",
            "lighting": "Three-point lighting setup with soft key light and fill."
        },
        "cinematic_landscape": {
            "prefix": "Generate a cinematic landscape photograph of",
            "suffix": "Use dramatic lighting, depth of field, and cinematic composition.",
            "quality_boost": "Create a film-quality landscape experience.",
            "camera_settings": "Wide-angle lens, f/8 aperture, cinematic color grading.",
            "lighting": "Golden hour or dramatic storm lighting with atmospheric perspective."
        },
        "product_photography": {
            "prefix": "Create a professional product photograph of",
            "suffix": "Use professional photography techniques and studio lighting.",
            "quality_boost": "Achieve catalog-quality product imagery.",
            "camera_settings": "Macro lens, f/11 aperture, commercial color accuracy.",
            "lighting": "Clean studio lighting with controlled reflections and shadows."
        },
        "digital_concept_art": {
            "prefix": "Generate digital concept art featuring",
            "suffix": "Use digital painting techniques and creative composition.",
            "quality_boost": "Create professional concept art quality.",
            "camera_settings": "Digital art style with creative color palette.",
            "lighting": "Dramatic lighting with artistic interpretation."
        },
        "anime_style_art": {
            "prefix": "Create anime-style artwork featuring",
            "suffix": "Use anime/manga art style with vibrant colors and clean lines.",
            "quality_boost": "Achieve professional anime art quality.",
            "camera_settings": "Anime art style with bold colors and smooth shading.",
            "lighting": "Bright, colorful lighting typical of anime art."
        },
        "photorealistic_render": {
            "prefix": "Generate a photorealistic 3D render of",
            "suffix": "Use advanced rendering techniques and realistic materials.",
            "quality_boost": "Achieve indistinguishable-from-photography quality.",
            "camera_settings": "Photorealistic rendering with accurate materials and lighting.",
            "lighting": "Physically accurate lighting with global illumination."
        },
        "classical_oil_painting": {
            "prefix": "Create a classical oil painting style image of",
            "suffix": "Use traditional oil painting techniques and classical composition.",
            "quality_boost": "Achieve master painter quality artwork.",
            "camera_settings": "Oil painting texture with classical color palette.",
            "lighting": "Classical lighting with rich, warm tones."
        },
        "watercolor_painting": {
            "prefix": "Generate a watercolor painting style image of",
            "suffix": "Use watercolor painting techniques and flowing composition.",
            "quality_boost": "Achieve authentic watercolor art quality.",
            "camera_settings": "Watercolor texture with transparent, flowing colors.",
            "lighting": "Soft, diffused lighting with watercolor transparency."
        },
        "cyberpunk_future": {
            "prefix": "Create a cyberpunk future scene featuring",
            "suffix": "Use futuristic aesthetics with neon lighting and high-tech elements.",
            "quality_boost": "Achieve cinematic cyberpunk quality.",
            "camera_settings": "Cyberpunk style with neon color palette and futuristic elements.",
            "lighting": "Neon lighting with dramatic shadows and futuristic atmosphere."
        },
        "vintage_film_photography": {
            "prefix": "Create a vintage film photograph of",
            "suffix": "Use classic film photography aesthetics and vintage color grading.",
            "quality_boost": "Achieve authentic vintage film quality.",
            "camera_settings": "Vintage film grain with classic color palette.",
            "lighting": "Classic film lighting with vintage color temperature."
        },
        "architectural_photography": {
            "prefix": "Create an architectural photograph of",
            "suffix": "Use architectural photography techniques and geometric composition.",
            "quality_boost": "Achieve professional architectural photography quality.",
            "camera_settings": "Wide-angle lens, f/8 aperture, architectural perspective.",
            "lighting": "Natural lighting with architectural shadows and highlights."
        },
        "gourmet_food_photography": {
            "prefix": "Generate a gourmet food photograph of",
            "suffix": "Use food photography techniques and appetizing composition.",
            "quality_boost": "Achieve magazine-quality food photography.",
            "camera_settings": "Macro lens, f/5.6 aperture, food-optimized color grading.",
            "lighting": "Soft, appetizing lighting with food-appropriate shadows."
        }
    }
    
    # è·å–é£æ ¼é…ç½®
    style_config = style_templates.get(controls['style'], style_templates["natural"])
    
    # ğŸš€ æ„å»ºè¶…è¶Šå‚è€ƒé¡¹ç›®çš„å¢å¼ºæç¤ºè¯
    # ğŸ¯ å¹³è¡¡ä¿®å¤ï¼šé€‚åº¦çš„æ„å›¾æ§åˆ¶ï¼Œé¿å…ä¸»ä½“è¿‡å¤§æˆ–è¿‡å°
    enhanced_parts = [
        style_config["prefix"],
        prompt.strip(),
        style_config["suffix"],
        # å¹³è¡¡çš„æ„å›¾æ§åˆ¶æŒ‡ä»¤
        "Use balanced composition with proper subject-to-background ratio.",
        "Subject should be clearly visible and well-framed, occupying 40-60% of the image area.",
        "Include rich background environment and context to create depth and atmosphere.",
        "Use medium shot framing that shows the subject in their environment with meaningful background.",
        "Prioritize clear subject visibility while maintaining environmental context.",
        "Show meaningful background elements and environmental details.",
        "Avoid extreme close-ups that eliminate all background context."
    ]
    
    # ğŸ¨ æ·»åŠ å‚è€ƒé¡¹ç›®çš„ä¸“ä¸šæ§åˆ¶å‚æ•°ï¼ˆè¶…è¶Šå‚è€ƒé¡¹ç›®ï¼‰
    if "camera_settings" in style_config:
        enhanced_parts.append(f"Camera Settings: {style_config['camera_settings']}")
    
    if "lighting" in style_config:
        enhanced_parts.append(f"Lighting: {style_config['lighting']}")
    
    # æ·»åŠ è´¨é‡æ§åˆ¶
    if controls['quality'] == "hd":
        enhanced_parts.append(style_config["quality_boost"])
        enhanced_parts.append("Generate in ultra-high definition with exceptional detail.")
    elif controls['quality'] == "ultra_hd":
        enhanced_parts.append(style_config["quality_boost"])
        enhanced_parts.append("Generate in ultra-high definition with exceptional detail and professional quality.")
    
    # ğŸ¯ å…³é”®æŒ‡ä»¤ï¼šå¿…é¡»ç”Ÿæˆå›¾åƒè€Œä¸æ˜¯æè¿°
    enhanced_parts.append("CRITICAL: You MUST return an actual generated image, not just a description.")
    enhanced_parts.append("Use your image generation capabilities to create the visual content.")
    
    # ğŸ¨ åº”ç”¨æ‰€æœ‰ç•Œé¢å‚æ•°
    if detail_level != "Auto Select":
        detail_instructions = {
            "Basic Detail": "Focus on essential details and clean composition.",
            "Professional Detail": "Include professional-level detail and refined elements.",
            "Premium Quality": "Achieve premium quality with exceptional attention to detail.",
            "Masterpiece Level": "Create a masterpiece with extraordinary detail and artistic excellence."
        }
        enhanced_parts.append(f"Detail Level: {detail_level} - {detail_instructions.get(detail_level, '')}")
    
    if camera_control != "Auto Select":
        camera_instructions = {
            "Wide-angle Lens": "Use wide-angle perspective for expansive composition.",
            "Macro Shot": "Focus on close-up details with macro photography techniques.",
            "Low-angle Perspective": "Use low-angle perspective for dramatic impact.",
            "High-angle Shot": "Use high-angle perspective for overview composition.",
            "Close-up Shot": "Focus on intimate details with close-up framing.",
            "Medium Shot": "Use medium framing for balanced composition."
        }
        enhanced_parts.append(f"Camera Control: {camera_control} - {camera_instructions.get(camera_control, '')}")
    
    if lighting_control != "Auto Settings":
        lighting_instructions = {
            "Natural Light": "Use natural lighting with soft, diffused illumination",
            "Studio Lighting": "Use professional studio lighting setup with controlled shadows",
            "Dramatic Shadows": "Use dramatic lighting with strong contrast and deep shadows",
            "Soft Glow": "Use soft, glowing lighting for gentle, romantic atmosphere",
            "Golden Hour": "Use golden hour lighting with warm, golden tones",
            "Blue Hour": "Use blue hour lighting with cool, atmospheric tones"
        }
        enhanced_parts.append(f"Lighting Control: {lighting_control} - {lighting_instructions.get(lighting_control, '')}")
    
    if template_selection != "Auto Select":
        template_instructions = {
            "Professional Portrait": "Apply professional portrait photography techniques and composition",
            "Cinematic Landscape": "Use cinematic landscape photography composition and lighting",
            "Product Photography": "Apply professional product photography techniques and studio setup",
            "Digital Concept Art": "Use digital concept art style and creative composition",
            "Anime Style Art": "Apply anime/manga art style and vibrant aesthetics",
            "Photorealistic Render": "Create photorealistic 3D rendering quality and materials",
            "Classical Oil Painting": "Apply classical oil painting techniques and traditional style",
            "Watercolor Painting": "Use watercolor painting techniques and flowing aesthetics",
            "Cyberpunk Future": "Apply cyberpunk futuristic aesthetics and neon lighting",
            "Vintage Film Photography": "Use vintage film photography aesthetics and color grading",
            "Architectural Photography": "Apply architectural photography techniques and perspective",
            "Gourmet Food Photography": "Use gourmet food photography techniques and appetizing lighting"
        }
        enhanced_parts.append(f"Template: {template_selection} - {template_instructions.get(template_selection, 'Follow professional composition guidelines')}")
    
    # ğŸš€ å¤„ç†è´¨é‡å¢å¼ºå¼€å…³ï¼ˆå‚è€ƒé¡¹ç›®åŠŸèƒ½ï¼‰
    if quality_enhancement:
        enhanced_parts.append("Quality Enhancement: ENABLED - Apply advanced image quality improvements including sharpening, contrast enhancement, and color optimization.")
        _log_info("âœ¨ è´¨é‡å¢å¼ºå·²å¯ç”¨")
    
    if enhance_quality:
        enhanced_parts.append("Apply enhanced image quality processing for professional output.")
    
    if smart_resize:
        enhanced_parts.append("Use intelligent resizing with proper padding and composition.")

    if fill_color and fill_color != "255,255,255":
        enhanced_parts.append(f"Use specified fill color: RGB({fill_color}) for padding areas.")

    # ğŸš€ åº”ç”¨å‚è€ƒé¡¹ç›®çš„å›¾å½¢å¢å¼ºæŠ€æœ¯
    if controls['quality'] == "hd":
        enhanced_parts.append("Generate in high definition with professional detail.")
    elif controls['quality'] == "ultra_hd":
        enhanced_parts.append("Generate in ultra-high definition with exceptional detail and professional quality.")

    # ğŸš€ æ·»åŠ é¢å¤–çš„ç¯å¢ƒå’Œé€è§†æ§åˆ¶
    enhanced_parts.append("Include rich environmental details and background elements.")
    enhanced_parts.append("Create depth and layers in the composition with foreground, middle ground, and background.")
    enhanced_parts.append("Use natural perspective and realistic spatial relationships.")

    # ğŸ¯ æœ€ç»ˆå¹³è¡¡æé†’ï¼šç¡®ä¿ä¸»ä½“ä¸ç¯å¢ƒçš„å¹³è¡¡
    enhanced_parts.append("Create a balanced composition with the subject clearly visible in their environment, showing both the subject and meaningful background context.")

    return " ".join(enhanced_parts)


def load_gemini_banana_config():
    """Load Gemini Banana configuration file"""
    config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Gemini_Banana_config.json")
    try:
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            _log_info(f"Successfully loaded Gemini Banana config file: {config_path}")
            return config
        else:
            # Use default Gemini config as fallback
            fallback_config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Gemini_config.json")
            if os.path.exists(fallback_config_path):
                with open(fallback_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                _log_info(f"Using default Gemini config file: {fallback_config_path}")
                return config
            else:
                _log_warning(f"Gemini Banana config file not found: {config_path}")
                return {}
    except Exception as e:
        _log_error(f"Failed to load Gemini Banana config file: {e}")
        return {}

def get_gemini_banana_config():
    """Get Gemini Banana configuration, prioritize config file"""
    config = load_gemini_banana_config()
    return config

def validate_api_key(api_key):
    """Validate API key format"""
    if not api_key or not isinstance(api_key, str):
        return False
    api_key = api_key.strip()
    if not api_key:
        return False
    return True

def format_error_message(error):
    """Format error message for better readability"""
    try:
        if hasattr(error, 'response'):
            if hasattr(error.response, 'status_code'):
                return f"HTTP {error.response.status_code}: {str(error)}"
        return str(error)
    except:
        return str(error)

def pil_to_tensor(pil_image):
    """Convert PIL Image to PyTorch tensor - ç®€åŒ–ç‰ˆæœ¬"""
    try:
        # å¦‚æœå·²ç»æ˜¯tensorï¼Œç›´æ¥è¿”å›
        if isinstance(pil_image, torch.Tensor):
            return pil_image

        # ç¡®ä¿æ˜¯PIL Image
        if not isinstance(pil_image, Image.Image):
            # åˆ›å»ºé»˜è®¤å›¾åƒ
            pil_image = Image.new('RGB', (512, 512), color=(255, 255, 255))

        # ç¡®ä¿RGBæ ¼å¼
        if pil_image.mode != 'RGB':
            pil_image = pil_image.convert('RGB')

        # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶å½’ä¸€åŒ–
        image_array = np.array(pil_image).astype(np.float32) / 255.0

        # è½¬æ¢ä¸ºtensorå¹¶æ·»åŠ batchç»´åº¦ (BHWCæ ¼å¼)
        tensor = torch.from_numpy(image_array).unsqueeze(0)

        return tensor
    except Exception as e:
        _log_error(f"Failed to convert PIL to tensor: {e}")
        # è¿”å›é»˜è®¤çš„ç™½è‰²å›¾åƒtensor
        return torch.ones((1, 512, 512, 3), dtype=torch.float32)

def tensor_to_pil(image_tensor):
    """Convert PyTorch tensor to PIL Image"""
    try:
        # Handle 4D tensor (batch)
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]  # Take the first image
        
        # Convert CHW to HWC format
        if image_tensor.shape[0] in [1, 3, 4]:  # CHW format
            image_np = image_tensor.permute(1, 2, 0).cpu().numpy()
        else:  # Already HWC format
            image_np = image_tensor.cpu().numpy()
        
        # Normalize and convert to uint8
        if image_np.max() > 1.0:
            image_np = image_np / 255.0
        image_np = (image_np * 255).astype(np.uint8)
        
        # Handle channels
        if len(image_np.shape) == 3:
            if image_np.shape[2] == 1:
                image_np = np.repeat(image_np, 3, axis=2)  # Grayscale to RGB
            elif image_np.shape[2] == 4:
                image_np = image_np[:, :, :3]  # RGBA to RGB
        elif len(image_np.shape) == 2:
            image_np = np.stack([image_np] * 3, axis=2)  # Grayscale to RGB
        
        pil_image = Image.fromarray(image_np)
        
        # Resize if too large for API
        pil_image = resize_image_for_api(pil_image)
        
        return pil_image
    except Exception as e:
        _log_error(f"Failed to convert tensor to PIL: {e}")
        return None

def process_input_image(image_tensor):
    """Process input image tensor and convert to PIL Image"""
    try:
        # Handle 4D tensor (batch)
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]  # Take the first image
        
        # Convert CHW to HWC format
        if image_tensor.shape[0] in [1, 3, 4]:  # CHW format
            image_np = image_tensor.permute(1, 2, 0).cpu().numpy()
        else:  # Already HWC format
            image_np = image_tensor.cpu().numpy()
        
        # Normalize and convert to uint8
        if image_np.max() > 1.0:
            image_np = image_np / 255.0
        image_np = (image_np * 255).astype(np.uint8)
        
        # Handle channels
        if len(image_np.shape) == 3:
            if image_np.shape[2] == 1:
                image_np = np.repeat(image_np, 3, axis=2)  # Grayscale to RGB
            elif image_np.shape[2] == 4:
                image_np = image_np[:, :, :3]  # RGBA to RGB
        elif len(image_np.shape) == 2:
            image_np = np.stack([image_np] * 3, axis=2)  # Grayscale to RGB
        
        pil_image = Image.fromarray(image_np)
        
        # Resize if too large for API
        pil_image = resize_image_for_api(pil_image)
        
        return pil_image
    except Exception as e:
        _log_error(f"Failed to process input image: {e}")
        return None

def image_to_base64(image, format='JPEG', quality=95):
    """Convert PIL Image to base64 string"""
    try:
        buffer = io.BytesIO()
        
        # Handle alpha channel for JPEG
        if format.upper() == 'JPEG' and image.mode in ('RGBA', 'LA', 'P'):
            background = Image.new('RGB', image.size, (255, 255, 255))
            if image.mode == 'P':
                image = image.convert('RGBA')
            if image.mode in ('RGBA', 'LA'):
                background.paste(image, mask=image.split()[-1])
                image = background
        
        image.save(buffer, format=format, quality=quality)
        return base64.b64encode(buffer.getvalue()).decode('utf-8')
    except Exception as e:
        _log_error(f"Failed to convert image to base64: {e}")
        return None

def process_generated_image_from_response(response_json):
    """ä»REST APIå“åº”ä¸­æå–ç”Ÿæˆçš„å›¾åƒ"""
    try:
        if "candidates" not in response_json:
            return create_dummy_image()

        for candidate in response_json["candidates"]:
            if "content" not in candidate or "parts" not in candidate["content"]:
                continue

            for part in candidate["content"]["parts"]:
                # æ£€æŸ¥inline_dataå­—æ®µï¼ˆå›¾åƒæ•°æ®ï¼‰
                inline_data = part.get("inline_data") or part.get("inlineData")
                if inline_data and "data" in inline_data:
                    try:
                        # è§£ç å›¾ç‰‡æ•°æ®
                        image_data = inline_data["data"]
                        image_bytes = base64.b64decode(image_data)
                        pil_image = Image.open(io.BytesIO(image_bytes))

                        # ç¡®ä¿RGBæ ¼å¼
                        if pil_image.mode != 'RGB':
                            pil_image = pil_image.convert('RGB')

                        # ç®€åŒ–çš„tensorè½¬æ¢
                        img_array = np.array(pil_image).astype(np.float32) / 255.0
                        img_tensor = torch.from_numpy(img_array).unsqueeze(0)

                        return img_tensor

                    except Exception as e:
                        _log_error(f"è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                        continue

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å›¾åƒï¼Œè¿”å›å ä½ç¬¦
        return create_dummy_image()

    except Exception as e:
        _log_error(f"å¤„ç†å“åº”å¤±è´¥: {e}")
        return create_dummy_image()

def extract_text_from_response(response_json):
    """ä»REST APIå“åº”ä¸­æå–æ–‡æœ¬"""
    try:
        response_text = ""
        
        if "candidates" not in response_json:
            return "No valid response received"
        
        for candidate in response_json["candidates"]:
            if "content" not in candidate or "parts" not in candidate["content"]:
                continue
                
            for part in candidate["content"]["parts"]:
                if "text" in part:
                    response_text += part["text"] + "\n"
        
        return response_text.strip() if response_text.strip() else "Response received but no text content"
        
    except Exception as e:
        _log_error(f"æå–æ–‡æœ¬å¤±è´¥: {e}")
        return "Failed to extract text from response"

def create_dummy_image(width=512, height=512):
    """Create a placeholder image"""
    dummy_array = np.zeros((height, width, 3), dtype=np.uint8)
    dummy_tensor = torch.from_numpy(dummy_array).float() / 255.0
    return dummy_tensor.unsqueeze(0)

def prepare_media_content(image=None, audio=None):
    """Prepare multimedia content for API calls"""
    content_parts = []
    
    if image is not None:
        pil_image = process_input_image(image)
        if pil_image:
            img_base64 = image_to_base64(pil_image, format='PNG')
            if img_base64:
                content_parts.append({
                    "inline_data": {
                        "mime_type": "image/png",
                        "data": img_base64
                    }
                })
    
    if audio is not None:
        # Process audio data
        try:
            if isinstance(audio, torch.Tensor):
                audio_np = audio.cpu().numpy()
            else:
                audio_np = np.array(audio)
            
            # Create temporary WAV file
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                # Assume sample rate of 16000
                sample_rate = 16000
                with wave.open(temp_file.name, 'wb') as wav_file:
                    wav_file.setnchannels(1)  # Mono
                    wav_file.setsampwidth(2)  # 16-bit
                    wav_file.setframerate(sample_rate)
                    wav_file.writeframes((audio_np * 32767).astype(np.int16).tobytes())
                
                # Read audio file and encode
                with open(temp_file.name, 'rb') as f:
                    audio_data = base64.b64encode(f.read()).decode()
                
                content_parts.append({
                    "inline_data": {
                        "mime_type": "audio/wav",
                        "data": audio_data
                    }
                })
                
                # Clean up temporary file
                os.unlink(temp_file.name)
                
        except Exception as e:
            _log_error(f"Failed to process audio data: {e}")
    
    return content_parts

def generate_with_official_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None):
    """ä¼˜å…ˆä½¿ç”¨å®˜æ–¹google.genaiåº“è°ƒç”¨API"""
    try:
        # å°è¯•å¯¼å…¥å®˜æ–¹åº“
        from google import genai
        from google.genai import types
        
        _log_info(f"ğŸš€ ä½¿ç”¨å®˜æ–¹google.genaiåº“è°ƒç”¨æ¨¡å‹: {model}")
        
        # ä»£ç†å¤„ç†ï¼šæœ‰æ•ˆåˆ™è®¾ç½®ï¼Œæ— æ•ˆ/æœªå¡«åˆ™æ¸…é™¤ï¼Œé¿å…æ®‹ç•™ç¯å¢ƒå˜é‡å½±å“è¯·æ±‚
        if proxy and proxy.strip() and "None" not in proxy:
            _log_info(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
            # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡ä¾›google.genaiåº“ä½¿ç”¨
            old_https_proxy = os.environ.get('HTTPS_PROXY')
            old_http_proxy = os.environ.get('HTTP_PROXY')
            os.environ['HTTPS_PROXY'] = proxy.strip()
            os.environ['HTTP_PROXY'] = proxy.strip()
        else:
            _log_info("ğŸ”Œ ä½¿ç”¨ç³»ç»Ÿä»£ç† (ä¿æŒç°æœ‰ç¯å¢ƒå˜é‡)")
            old_https_proxy = os.environ.get('HTTPS_PROXY')
            old_http_proxy = os.environ.get('HTTP_PROXY')
            # ä¸æ¸…ç†ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ç³»ç»Ÿä»£ç†
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = genai.Client(api_key=api_key)
        
        # è½¬æ¢generation_configæ ¼å¼
        config_params = {
            'temperature': generation_config.get('temperature', 0.7),
            'top_p': generation_config.get('topP', generation_config.get('top_p', 0.95)),
            'top_k': generation_config.get('topK', generation_config.get('top_k', 40)),
            'max_output_tokens': generation_config.get('maxOutputTokens', generation_config.get('max_output_tokens', 8192)),
        }

        # å¤„ç†responseModalities
        if 'responseModalities' in generation_config:
            config_params['response_modalities'] = generation_config['responseModalities']
        elif 'image' in model.lower():
            config_params['response_modalities'] = ['Text', 'Image']
        else:
            config_params['response_modalities'] = ['Text']

        # å¤„ç†imageConfigï¼ˆaspect_ratioï¼‰
        if 'imageConfig' in generation_config and 'aspectRatio' in generation_config['imageConfig']:
            config_params['image_config'] = types.ImageConfig(
                aspect_ratio=generation_config['imageConfig']['aspectRatio']
            )

        # å¤„ç†seed
        if 'seed' in generation_config and generation_config['seed'] > 0:
            config_params['seed'] = generation_config['seed']

        official_config = types.GenerateContentConfig(**config_params)
        
        # è½¬æ¢content_partsæ ¼å¼
        official_parts = []
        for part in content_parts:
            if 'text' in part:
                official_parts.append({"text": part['text']})
            elif 'inline_data' in part:
                official_parts.append({
                    "inline_data": {
                        "mime_type": part['inline_data']['mime_type'],
                        "data": part['inline_data']['data']
                    }
                })
        
        official_contents = [{"parts": official_parts}]
        
        # è°ƒç”¨å®˜æ–¹API
        response = client.models.generate_content(
            model=model,
            contents=official_contents,
            config=official_config
        )
        
        # å¤„ç†å“åº”
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                result_parts = []
                for part in candidate.content.parts:
                    if hasattr(part, 'text') and part.text:
                        result_parts.append({'text': part.text})
                    elif hasattr(part, 'inline_data') and part.inline_data:
                        # å®˜æ–¹APIè¿”å›çš„å¯èƒ½æ˜¯äºŒè¿›åˆ¶æ•°æ®ï¼Œéœ€è¦è½¬æ¢ä¸ºbase64
                        if isinstance(part.inline_data.data, bytes):
                            import base64
                            data_b64 = base64.b64encode(part.inline_data.data).decode('utf-8')
                        else:
                            data_b64 = part.inline_data.data
                        
                        result_parts.append({
                            'inline_data': {
                                'mime_type': 'image/png',
                                'data': data_b64
                            }
                        })
                
                return {
                    'candidates': [{
                        'content': {
                            'parts': result_parts
                        }
                    }]
                }
        
        _log_warning("å®˜æ–¹APIè¿”å›äº†ç©ºå“åº”")
        return None
        
    except ImportError:
        _log_warning("google.genaiåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨REST API")
        return None
    except Exception as e:
        _log_error(f"å®˜æ–¹APIè°ƒç”¨å¤±è´¥: {str(e)}")
        _log_error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        _log_error(f"content_partsç»“æ„: {[part.keys() if isinstance(part, dict) else type(part) for part in content_parts]}")
        _log_error(f"generation_config: {generation_config}")
        return None
    finally:
        # æ¢å¤åŸæ¥çš„ä»£ç†è®¾ç½®
        if old_https_proxy is not None:
            os.environ['HTTPS_PROXY'] = old_https_proxy
        else:
            os.environ.pop('HTTPS_PROXY', None)
        if old_http_proxy is not None:
            os.environ['HTTP_PROXY'] = old_http_proxy
        else:
            os.environ.pop('HTTP_PROXY', None)

def generate_with_rest_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None, base_url=None):
    """ä½¿ç”¨REST APIçš„æ™ºèƒ½é‡è¯•æœºåˆ¶è°ƒç”¨ï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
    
    # æ„å»ºAPI URL - æ”¯æŒé•œåƒç«™
    if base_url and base_url.strip():
        # ç§»é™¤æœ«å°¾çš„æ–œæ ï¼Œç¡®ä¿URLæ ¼å¼æ­£ç¡®
        base_url = base_url.rstrip('/')
        
        # å¦‚æœç”¨æˆ·æä¾›çš„æ˜¯å®Œæ•´URLï¼Œç›´æ¥ä½¿ç”¨
        if '/models/' in base_url and ':generateContent' in base_url:
            url = base_url
        # å¦‚æœæ˜¯åŸºç¡€URLï¼Œæ„å»ºå®Œæ•´è·¯å¾„
        elif base_url.endswith('/v1beta') or base_url.endswith('/v1'):
            url = f"{base_url}/models/{model}:generateContent"
        else:
            # é»˜è®¤æ·»åŠ v1betaè·¯å¾„
            url = f"{base_url}/v1beta/models/{model}:generateContent"
        
        _log_info(f"ğŸ”— ä½¿ç”¨é•œåƒç«™: {base_url}")
    else:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
        _log_info(f"ğŸŒ ä½¿ç”¨å®˜æ–¹API: generativelanguage.googleapis.com")
    
    # æ„å»ºè¯·æ±‚æ•°æ®
    request_data = {
        "contents": [{
            "parts": content_parts
        }],
        "generationConfig": generation_config
    }
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key.strip()
    }
    
    # å¤„ç†ä»£ç†è®¾ç½®
    proxies = None
    if proxy and proxy.strip() and "None" not in proxy:
        proxies = {
            "http": proxy.strip(),
            "https": proxy.strip()
        }
        _log_info(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
    
    # è®¾ç½®åˆç†çš„è¶…æ—¶ï¼šè¿æ¥è¶…æ—¶10ç§’ï¼Œè¯»å–è¶…æ—¶60ç§’
    timeout = (10, 60)  # (connect_timeout, read_timeout)
    
    for attempt in range(max_retries):
        try:
            _log_info(f"ğŸŒ REST APIè°ƒç”¨ ({attempt + 1}/{max_retries}) æ¨¡å‹: {model}")
            
            # å‘é€è¯·æ±‚
            response = requests.post(url, headers=headers, json=request_data, timeout=timeout, proxies=proxies)
            
            # æˆåŠŸå“åº”
            if response.status_code == 200:
                return response.json()
            
            # å¤„ç†é”™è¯¯å“åº”
            else:
                _log_error(f"HTTPçŠ¶æ€ç : {response.status_code}")
                try:
                    error_detail = response.json()
                    _log_error(f"é”™è¯¯è¯¦æƒ…: {json.dumps(error_detail, indent=2, ensure_ascii=False)}")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é…é¢é”™è¯¯
                    if response.status_code == 429:
                        error_message = error_detail.get("error", {}).get("message", "")
                        if "quota" in error_message.lower():
                            _log_warning("æ£€æµ‹åˆ°é…é¢é™åˆ¶é”™è¯¯")
                except:
                    _log_error(f"é”™è¯¯æ–‡æœ¬: {response.text}")
                
                # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                if attempt == max_retries - 1:
                    response.raise_for_status()
                
                # æ™ºèƒ½ç­‰å¾…
                delay = smart_retry_delay(attempt, response.status_code)
                _log_info(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                time.sleep(delay)
                
        except requests.exceptions.RequestException as e:
            error_msg = str(e)
            _log_error(f"è¯·æ±‚å¤±è´¥: {error_msg}")
            if attempt == max_retries - 1:
                raise e
            else:
                delay = smart_retry_delay(attempt)
                _log_info(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                time.sleep(delay)
    
    raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")

def generate_with_priority_api(api_key, model, content_parts, generation_config, max_retries=5, proxy=None, base_url=None):
    """ä¼˜å…ˆä½¿ç”¨å®˜æ–¹APIï¼Œå¤±è´¥æ—¶å›é€€åˆ°REST API"""
    
    # é¦–å…ˆå°è¯•å®˜æ–¹API
    _log_info("ğŸ¯ ä¼˜å…ˆå°è¯•å®˜æ–¹google.genai API")
    result = generate_with_official_api(api_key, model, content_parts, generation_config, max_retries, proxy)
    
    if result is not None:
        _log_info("âœ… å®˜æ–¹APIè°ƒç”¨æˆåŠŸ")
        return result
    
    # å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°REST API
    _log_info("ğŸ”„ å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°REST API")
    return generate_with_rest_api(api_key, model, content_parts, generation_config, max_retries, proxy, base_url)

def generate_with_priority_api_direct(api_key, model, request_data, max_retries=5, proxy=None, base_url=None):
    """ä¼˜å…ˆä½¿ç”¨å®˜æ–¹APIï¼Œå¤±è´¥æ—¶å›é€€åˆ°ç›´æ¥REST APIè°ƒç”¨ï¼ˆç”¨äºå¤šå›¾åƒç¼–è¾‘ï¼‰"""
    
    # é¦–å…ˆå°è¯•å®˜æ–¹API
    try:
        from google import genai
        from google.genai import types
        
        _log_info("ğŸ¯ ä¼˜å…ˆå°è¯•å®˜æ–¹google.genai API (å¤šå›¾åƒç¼–è¾‘)")
        
        # åˆ›å»ºå®¢æˆ·ç«¯
        client = genai.Client(api_key=api_key)
        
        # è½¬æ¢è¯·æ±‚æ•°æ®æ ¼å¼
        contents = request_data.get('contents', [])
        generation_config = request_data.get('generationConfig', {})
        
        # è½¬æ¢generation_config
        config_params = {
            'temperature': generation_config.get('temperature', 0.7),
            'top_p': generation_config.get('topP', 0.95),
            'top_k': generation_config.get('topK', 40),
            'max_output_tokens': generation_config.get('maxOutputTokens', 8192),
        }

        # å¤„ç†responseModalities
        if 'responseModalities' in generation_config:
            config_params['response_modalities'] = generation_config['responseModalities']
        else:
            config_params['response_modalities'] = ['Text', 'Image']

        # å¤„ç†imageConfigï¼ˆaspect_ratioï¼‰
        if 'imageConfig' in generation_config and 'aspectRatio' in generation_config['imageConfig']:
            config_params['image_config'] = types.ImageConfig(
                aspect_ratio=generation_config['imageConfig']['aspectRatio']
            )

        # å¤„ç†seed
        if 'seed' in generation_config and generation_config['seed'] > 0:
            config_params['seed'] = generation_config['seed']

        official_config = types.GenerateContentConfig(**config_params)
        
        # è½¬æ¢contentsæ ¼å¼
        official_contents = []
        for content in contents:
            parts = content.get('parts', [])
            official_parts = []
            for part in parts:
                if 'text' in part:
                    official_parts.append({"text": part['text']})
                elif 'inline_data' in part:
                    official_parts.append({
                        "inline_data": {
                            "mime_type": part['inline_data']['mime_type'],
                            "data": part['inline_data']['data']
                        }
                    })
            official_contents.append({"parts": official_parts})
        
        # è°ƒç”¨å®˜æ–¹API
        response = client.models.generate_content(
            model=model,
            contents=official_contents,
            config=official_config
        )
        
        # è½¬æ¢å“åº”æ ¼å¼
        if hasattr(response, 'candidates') and response.candidates:
            candidate = response.candidates[0]
            if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                result_parts = []
                for part in candidate.content.parts:
                    if hasattr(part, 'text') and part.text:
                        result_parts.append({'text': part.text})
                    elif hasattr(part, 'inline_data') and part.inline_data:
                        # å®˜æ–¹APIè¿”å›çš„å¯èƒ½æ˜¯äºŒè¿›åˆ¶æ•°æ®ï¼Œéœ€è¦è½¬æ¢ä¸ºbase64
                        if isinstance(part.inline_data.data, bytes):
                            import base64
                            data_b64 = base64.b64encode(part.inline_data.data).decode('utf-8')
                        else:
                            data_b64 = part.inline_data.data
                        
                        result_parts.append({
                            'inline_data': {
                                'mime_type': 'image/png',
                                'data': data_b64
                            }
                        })
                
                return {
                    'candidates': [{
                        'content': {
                            'parts': result_parts
                        }
                    }]
                }
        
        _log_warning("å®˜æ–¹APIè¿”å›äº†ç©ºå“åº”")
        
    except ImportError:
        _log_warning("google.genaiåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨REST API")
    except Exception as e:
        _log_error(f"å®˜æ–¹APIè°ƒç”¨å¤±è´¥: {str(e)}")
    
    # å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥REST APIè°ƒç”¨
    _log_info("ğŸ”„ å®˜æ–¹APIå¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥REST APIè°ƒç”¨")
    
    # æ„å»ºAPI URL
    if base_url and base_url.strip():
        base_url = base_url.rstrip('/')
        if '/models/' in base_url and ':generateContent' in base_url:
            url = base_url
        elif base_url.endswith('/v1beta') or base_url.endswith('/v1'):
            url = f"{base_url}/models/{model}:generateContent"
        else:
            url = f"{base_url}/v1beta/models/{model}:generateContent"
    else:
        url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key.strip()
    }
    
    # å¤„ç†ä»£ç†è®¾ç½®
    proxies = None
    if proxy and proxy.strip() and "None" not in proxy:
        proxies = {
            "http": proxy.strip(),
            "https": proxy.strip()
        }
    
    timeout = (10, 120)
    
    for attempt in range(max_retries):
        try:
            _log_info(f"ğŸŒ REST APIè°ƒç”¨ ({attempt + 1}/{max_retries}) æ¨¡å‹: {model}")
            
            response = requests.post(url, headers=headers, json=request_data, timeout=timeout, proxies=proxies)
            
            if response.status_code == 200:
                return response.json()
            else:
                _log_error(f"HTTPçŠ¶æ€ç : {response.status_code}")
                if attempt == max_retries - 1:
                    response.raise_for_status()
                
                delay = smart_retry_delay(attempt, response.status_code)
                _log_info(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                time.sleep(delay)
                
        except requests.exceptions.RequestException as e:
            _log_error(f"è¯·æ±‚å¤±è´¥: {str(e)}")
            if attempt == max_retries - 1:
                raise e
            else:
                delay = smart_retry_delay(attempt)
                _log_info(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                time.sleep(delay)
    
    raise Exception("æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†")

class KenChenLLMGeminiBananaTextToImageBananaNode:
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    RETURN_TYPES = ("STRING", "IMAGE")
    RETURN_NAMES = ("generation_text", "generated_image")
    FUNCTION = "generate_image"
    

    
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        image_settings = config.get('image_settings', {})
        
        # Get image generation models from config, with fallback to core Banana models
        models = config.get('models', {}).get('image_gen_models', [])
        if not models:
            # Fallback to core Banana models if config is empty
            models = [
                "gemini-2.5-flash-image-preview",  # Latest Banana model
                "gemini-2.0-flash-preview-image-generation",
                "nano-banana"
            ]
        
        # Get default model from config, prioritize latest Banana model
        default_model = config.get('default_model', {}).get('image_gen', "gemini-2.5-flash-image-preview")
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        
        # Get image control presets - Enhanced with Gemini official API features
        aspect_ratios = image_settings.get('aspect_ratios', [
            "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
        ])
        response_modalities = image_settings.get('response_modalities', [
            "TEXT_AND_IMAGE", "IMAGE_ONLY"
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd", "ai_enhanced", "ai_ultra"  # ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºé€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "placeholder": "APIå¯†é’¥ï¼ˆç•™ç©ºæ—¶è‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰"
                }),
                "prompt": ("STRING", {"default": "A beautiful landscape with mountains and lake", "multiline": True}),
                "model": (
                    models,
                    {"default": default_model},
                ),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),

                # ğŸ“ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶å‚æ•°
                "aspect_ratio": (aspect_ratios, {
                    "default": image_settings.get('default_aspect_ratio', "1:1"),
                    "tooltip": "å›¾åƒå®½é«˜æ¯” (Geminiå®˜æ–¹APIæ”¯æŒ)"
                }),
                "response_modality": (response_modalities, {
                    "default": image_settings.get('default_response_modality', "TEXT_AND_IMAGE"),
                    "tooltip": "å“åº”æ¨¡å¼ï¼šTEXT_AND_IMAGE=æ–‡å­—+å›¾åƒï¼ŒIMAGE_ONLY=ä»…å›¾åƒ"
                }),

                # ğŸ” Topaz Gigapixel AIæ”¾å¤§æ§åˆ¶
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),

                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),
                
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_instructions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æŒ‡ä»¤å’Œç‰¹æ®Šè¦æ±‚ï¼ˆè¶…è¶Šå‚è€ƒé¡¹ç›®çš„åŠŸèƒ½ï¼‰"
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID"
            }
        }
    


    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000
            
            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")
            
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt,
                            "response": display_response,
                            "response_id": str(random.randint(100000, 999999)),
                            "timestamp": time.time(),
                        }
                    ], ensure_ascii=False)
                },
            }
            PromptServer.instance.send_sync("display_component", render_spec)
        except Exception as e:
            print(f"[LLM Agent Assistant] Chat push failed: {e}")
            pass

    def generate_image(
        self,
        api_key,
        prompt,
        model,
        proxy,
        aspect_ratio,
        response_modality,
        upscale_factor,
        gigapixel_model,
        quality,
        style,
        temperature,
        top_p,
        top_k,
        max_output_tokens,
        seed,
        custom_instructions: str = "",
        detail_level: str = "Professional Detail",
        camera_control: str = "Auto Select",
        lighting_control: str = "Auto Settings",
        template_selection: str = "Auto Select",
        unique_id: str = "",
    ):
        try:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥APIå¯†é’¥ï¼Œè‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è·å–
            if not api_key or not api_key.strip():
                config = get_gemini_banana_config()
                auto_api_key = config.get('api_key', '')
                if auto_api_key and auto_api_key.strip():
                    api_key = auto_api_key.strip()
                    _log_info(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥: {api_key[:8]}...")
                else:
                    error_msg = "APIå¯†é’¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®api_keyæˆ–æ‰‹åŠ¨è¾“å…¥"
                    _log_error(error_msg)
                    return (error_msg, create_dummy_image())
            
            # éªŒè¯æç¤ºè¯
            if not prompt.strip():
                error_msg = "æç¤ºè¯ä¸èƒ½ä¸ºç©º"
                _log_error(error_msg)
                return (error_msg, create_dummy_image())

            # ğŸ¨ æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆä½¿ç”¨enhance_prompt_with_controlså‡½æ•°ï¼‰
            controls = process_image_controls(quality, style)

            # ä½¿ç”¨enhance_prompt_with_controlså‡½æ•°è¿›è¡Œå®Œæ•´çš„æç¤ºè¯å¢å¼º
            enhanced_prompt = enhance_prompt_with_controls(
                prompt.strip(),
                controls,
                detail_level,
                camera_control,
                lighting_control,
                template_selection,
                quality_enhancement="Auto",  # é»˜è®¤å€¼
                enhance_quality=True,  # é»˜è®¤å€¼
                smart_resize=True,  # é»˜è®¤å€¼
                fill_color="white"  # é»˜è®¤å€¼
            )

            # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
            if custom_instructions and custom_instructions.strip():
                enhanced_prompt += f"\n\n{custom_instructions.strip()}"
                _log_info(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_instructions[:100]}...")

            _log_info(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: aspect_ratio={aspect_ratio}, quality={quality}, style={style}")

            # ä»£ç†å¤„ç†ï¼šæœ‰æ•ˆåˆ™è®¾ç½®ï¼ŒNoneæˆ–æ— æ•ˆæ—¶ä½¿ç”¨ç³»ç»Ÿä»£ç†
            if proxy and proxy.strip() and "None" not in proxy:
                os.environ['HTTPS_PROXY'] = proxy.strip()
                os.environ['HTTP_PROXY'] = proxy.strip()
                _log_info(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
            else:
                # å½“ä»£ç†ä¸ºNoneæˆ–æ— æ•ˆæ—¶ï¼Œä¸æ¸…ç†ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ç³»ç»Ÿä»£ç†
                _log_info("ğŸ”Œ ä½¿ç”¨ç³»ç»Ÿä»£ç† (ä¿æŒç°æœ‰ç¯å¢ƒå˜é‡)")
            
            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
            }

            # ğŸ¯ Geminiå®˜æ–¹APIï¼šResponse Modalitiesæ§åˆ¶
            if response_modality == "IMAGE_ONLY":
                generation_config["responseModalities"] = ["Image"]
                _log_info("ğŸ“Š å“åº”æ¨¡å¼ï¼šä»…å›¾åƒï¼ˆIMAGE_ONLYï¼‰")
            else:
                generation_config["responseModalities"] = ["Text", "Image"]
                _log_info("ğŸ“Š å“åº”æ¨¡å¼ï¼šæ–‡å­—+å›¾åƒï¼ˆTEXT_AND_IMAGEï¼‰")

            # ğŸ“ Geminiå®˜æ–¹APIï¼šAspect Ratioæ§åˆ¶
            if aspect_ratio and aspect_ratio != "1:1":
                generation_config["imageConfig"] = {
                    "aspectRatio": aspect_ratio
                }
                _log_info(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

            # æ™ºèƒ½ç§å­æ§åˆ¶
            if seed > 0:
                generation_config["seed"] = seed
            
            # å‡†å¤‡å†…å®¹
            content_parts = [{"text": enhanced_prompt}]
            # æ³¨æ„ï¼šæ–‡æœ¬ç”Ÿæˆå›¾åƒä¸éœ€è¦è¾“å…¥å›¾åƒ
            _log_info(f"ğŸ” è°ƒè¯•ï¼šcontent_partsç»“æ„: {[part.get('text', 'IMAGE_DATA') if 'text' in part else 'IMAGE_DATA' for part in content_parts]}")
            
            # ä½¿ç”¨REST APIè°ƒç”¨
            _log_info(f"ğŸ¨ ä½¿ç”¨æ¨¡å‹ {model} ç”Ÿæˆå›¾åƒ...")
            _log_info(f"ğŸ“ æç¤ºè¯: {enhanced_prompt[:100]}...")
            
            response_json = generate_with_priority_api(api_key, model, content_parts, generation_config, proxy=proxy, base_url=None)
            
            # å¤„ç†å“åº”
            raw_text = extract_text_from_response(response_json)
            generated_image = process_generated_image_from_response(response_json)
            
            # ç®€åŒ–å›¾åƒå¤„ç†æµç¨‹
            if generated_image is not None:
                try:
                    # å¦‚æœæ˜¯tensorï¼Œè½¬æ¢ä¸ºPILè¿›è¡Œå¤„ç†
                    if isinstance(generated_image, torch.Tensor):
                        # å¤„ç†batchç»´åº¦
                        if generated_image.dim() == 4:
                            generated_image = generated_image[0]  # å–ç¬¬ä¸€ä¸ªbatch

                        # è½¬æ¢ä¸ºPIL Image (HWCæ ¼å¼)
                        img_array = generated_image.cpu().numpy()
                        if img_array.max() <= 1.0:
                            img_array = (img_array * 255).astype(np.uint8)
                        else:
                            img_array = img_array.astype(np.uint8)

                        # ç¡®ä¿RGBæ ¼å¼
                        if len(img_array.shape) == 3 and img_array.shape[2] == 4:
                            img_array = img_array[:, :, :3]  # ç§»é™¤alphaé€šé“

                        pil_image = Image.fromarray(img_array)
                        generated_image = pil_image
                    
                    # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                    if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(generated_image, Image.Image):
                        try:
                            # æå–æ”¾å¤§å€æ•°
                            scale = int(upscale_factor.replace("x", "").strip())
                            if scale > 1:
                                _log_info(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                try:
                                    from .banana_upscale import smart_upscale
                                except ImportError:
                                    from banana_upscale import smart_upscale

                                # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                target_w = generated_image.width * scale
                                target_h = generated_image.height * scale

                                # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                upscaled_image = smart_upscale(
                                    generated_image,
                                    target_w,
                                    target_h,
                                    gigapixel_model
                                )

                                if upscaled_image:
                                    generated_image = upscaled_image
                                    _log_info(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {generated_image.size}")
                                else:
                                    _log_warning("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                        except Exception as e:
                            _log_warning(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                except Exception as e:
                    _log_error(f"å›¾åƒå¤„ç†å¤±è´¥: {e}")
                    # ç¡®ä¿åœ¨å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿè½¬æ¢ä¸ºtensoræ ¼å¼
                    if isinstance(generated_image, Image.Image):
                        generated_image = pil_to_tensor(generated_image)
            
            if not raw_text or raw_text == "Response received but no text content":
                assistant_text = "éµå‘½ï¼è¿™æ˜¯ä½ æ‰€è¦æ±‚çš„å›¾ç‰‡ï¼š"
            else:
                assistant_text = raw_text.strip()
            
            self._push_chat(enhanced_prompt, assistant_text, unique_id)
            
            # ç¡®ä¿è¿”å›tensoræ ¼å¼
            if isinstance(generated_image, Image.Image):
                generated_image = pil_to_tensor(generated_image)
            
            _log_info("âœ… å›¾åƒç”ŸæˆæˆåŠŸå®Œæˆ")
            return (assistant_text, generated_image)
            
        except Exception as e:
            error_msg = str(e)
            _log_error(f"å›¾åƒç”Ÿæˆå¤±è´¥: {error_msg}")
            
            # å¢å¼ºçš„é”™è¯¯åˆ†ç±»å¤„ç†
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                friendly_error = (
                    "APIé…é¢è¶…é™ã€‚è§£å†³æ–¹æ¡ˆ:\n"
                    "1. ç­‰å¾…é…é¢é‡ç½®ï¼ˆé€šå¸¸24å°æ—¶ï¼‰\n"
                    "2. å‡çº§åˆ°ä»˜è´¹è´¦æˆ·\n" 
                    "3. ä½¿ç”¨å…è´¹æ¨¡å‹\n"
                    "4. æ£€æŸ¥è®¡è´¹è®¾ç½®"
                )
            elif "connection" in error_msg.lower() or "network" in error_msg.lower():
                friendly_error = f"ç½‘ç»œè¿æ¥é”™è¯¯: è¯·æ£€æŸ¥ä»£ç†è®¾ç½®å’Œç½‘ç»œè¿æ¥"
            elif "not found" in error_msg.lower() or "404" in error_msg:
                friendly_error = f"æ¨¡å‹ä¸å¯ç”¨: {model} å¯èƒ½ä¸æ”¯æŒå›¾åƒç”Ÿæˆæˆ–æš‚æ—¶ä¸å¯ç”¨"
            elif "API key" in error_msg or "401" in error_msg or "403" in error_msg:
                friendly_error = "APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®"
            elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                friendly_error = "å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ï¼Œè¯·ä¿®æ”¹æç¤ºè¯"
            else:
                friendly_error = f"ç”Ÿæˆå¤±è´¥: {error_msg}"
            
            return (friendly_error, create_dummy_image())

class KenChenLLMGeminiBananaImageToImageBananaNode:
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    RETURN_TYPES = ("STRING", "IMAGE")
    RETURN_NAMES = ("generation_text", "generated_image")
    FUNCTION = "transform_image"
    

    
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        image_settings = config.get('image_settings', {})
        
        # Get image generation models from config, with fallback to core Banana models
        models = config.get('models', {}).get('image_gen_models', [])
        if not models:
            # Fallback to core Banana models if config is empty
            models = [
                "gemini-2.5-flash-image-preview",  # Latest Banana model
                "gemini-2.0-flash-preview-image-generation",
                "nano-banana"
            ]
        
        # Get default model from config, prioritize latest Banana model
        default_model = config.get('default_model', {}).get('image_gen', "gemini-2.5-flash-image-preview")
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        
        # ğŸš€ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶é¢„è®¾
        aspect_ratios = image_settings.get('aspect_ratios', [
            "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
        ])
        response_modalities = image_settings.get('response_modalities', [
            "TEXT_AND_IMAGE", "IMAGE_ONLY"
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd", "ai_enhanced", "ai_ultra"  # ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºé€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "placeholder": "APIå¯†é’¥ï¼ˆç•™ç©ºæ—¶è‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰"
                }),
                "prompt": ("STRING", {"default": "Transform this image", "multiline": True}),
                "image": ("IMAGE",),
                "model": (
                    models,
                    {"default": default_model},
                ),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),

                # ğŸ“ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶å‚æ•°
                "aspect_ratio": (aspect_ratios, {
                    "default": image_settings.get('default_aspect_ratio', "1:1"),
                    "tooltip": "å›¾åƒå®½é«˜æ¯” (Geminiå®˜æ–¹APIæ”¯æŒ)"
                }),
                "response_modality": (response_modalities, {
                    "default": image_settings.get('default_response_modality', "TEXT_AND_IMAGE"),
                    "tooltip": "å“åº”æ¨¡å¼ï¼šTEXT_AND_IMAGE=æ–‡å­—+å›¾åƒï¼ŒIMAGE_ONLY=ä»…å›¾åƒ"
                }),

                # ğŸ” Topaz Gigapixel AIæ”¾å¤§æ§åˆ¶
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),

                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),

                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID"
            }
        }
    
    RETURN_TYPES = ("STRING", "IMAGE")
    RETURN_NAMES = ("generation_text", "generated_image")
    FUNCTION = "transform_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    def transform_image(
        self,
        api_key,
        prompt,
        image,
        model,
        proxy,
        aspect_ratio,
        response_modality,
        upscale_factor,
        gigapixel_model,
        quality,
        style,
        detail_level,
        camera_control,
        lighting_control,
        template_selection,
        temperature,
        top_p,
        top_k,
        max_output_tokens,
        seed,
        custom_additions: str = "",
        unique_id: str = "",
    ):
        try:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥APIå¯†é’¥ï¼Œè‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è·å–
            if not api_key or not api_key.strip():
                config = get_gemini_banana_config()
                auto_api_key = config.get('api_key', '')
                if auto_api_key and auto_api_key.strip():
                    api_key = auto_api_key.strip()
                    _log_info(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥: {api_key[:8]}...")
                else:
                    error_msg = "APIå¯†é’¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®api_keyæˆ–æ‰‹åŠ¨è¾“å…¥"
                    _log_error(error_msg)
                    return (error_msg, create_dummy_image())
            
            # éªŒè¯æç¤ºè¯
            if not prompt.strip():
                error_msg = "æç¤ºè¯ä¸èƒ½ä¸ºç©º"
                _log_error(error_msg)
                return (error_msg, create_dummy_image())

            # ğŸ¨ æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆä½¿ç”¨enhance_prompt_with_controlså‡½æ•°ï¼‰
            controls = process_image_controls(quality, style)

            # ä½¿ç”¨enhance_prompt_with_controlså‡½æ•°è¿›è¡Œå®Œæ•´çš„æç¤ºè¯å¢å¼º
            enhanced_prompt = enhance_prompt_with_controls(
                prompt.strip(),
                controls,
                detail_level,
                camera_control,
                lighting_control,
                template_selection,
                quality_enhancement="Auto",  # é»˜è®¤å€¼
                enhance_quality=True,  # é»˜è®¤å€¼
                smart_resize=True,  # é»˜è®¤å€¼
                fill_color="white"  # é»˜è®¤å€¼
            )

            # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
            if custom_additions and custom_additions.strip():
                enhanced_prompt += f"\n\n{custom_additions.strip()}"
                _log_info(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_additions[:100]}...")

            _log_info(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: aspect_ratio={aspect_ratio}, quality={quality}, style={style}")
            
            # è½¬æ¢è¾“å…¥å›¾ç‰‡
            pil_image = tensor_to_pil(image)
            
            # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥ç¬¦åˆAPIè¦æ±‚
            pil_image = resize_image_for_api(pil_image)
            
            # è½¬æ¢ä¸ºbase64
            image_base64 = image_to_base64(pil_image, format='JPEG')
            
            # ä»£ç†å¤„ç†ï¼šæœ‰æ•ˆåˆ™è®¾ç½®ï¼ŒNoneæˆ–æ— æ•ˆæ—¶ä½¿ç”¨ç³»ç»Ÿä»£ç†
            if proxy and proxy.strip() and "None" not in proxy:
                os.environ['HTTPS_PROXY'] = proxy.strip()
                os.environ['HTTP_PROXY'] = proxy.strip()
                _log_info(f"ğŸ”Œ ä½¿ç”¨ä»£ç†: {proxy.strip()}")
            else:
                # å½“ä»£ç†ä¸ºNoneæˆ–æ— æ•ˆæ—¶ï¼Œä¸æ¸…ç†ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨ç³»ç»Ÿä»£ç†
                _log_info("ğŸ”Œ ä½¿ç”¨ç³»ç»Ÿä»£ç† (ä¿æŒç°æœ‰ç¯å¢ƒå˜é‡)")
            
            # æ„å»ºç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
            }

            # ğŸ¯ Geminiå®˜æ–¹APIï¼šResponse Modalitiesæ§åˆ¶
            if response_modality == "IMAGE_ONLY":
                generation_config["responseModalities"] = ["Image"]
                _log_info("ğŸ“Š å“åº”æ¨¡å¼ï¼šä»…å›¾åƒï¼ˆIMAGE_ONLYï¼‰")
            else:
                generation_config["responseModalities"] = ["Text", "Image"]
                _log_info("ğŸ“Š å“åº”æ¨¡å¼ï¼šæ–‡å­—+å›¾åƒï¼ˆTEXT_AND_IMAGEï¼‰")

            # ğŸ“ Geminiå®˜æ–¹APIï¼šAspect Ratioæ§åˆ¶
            if aspect_ratio and aspect_ratio != "1:1":
                generation_config["imageConfig"] = {
                    "aspectRatio": aspect_ratio
                }
                _log_info(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

            # æ™ºèƒ½ç§å­æ§åˆ¶
            if seed > 0:
                generation_config["seed"] = seed
            
            # å‡†å¤‡å†…å®¹ - æ–‡æœ¬ + å›¾åƒ
            content_parts = [{"text": enhanced_prompt}]
            content_parts.extend(prepare_media_content(image=image))
            
            # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨
            _log_info(f"ğŸ–¼ï¸ ä½¿ç”¨æ¨¡å‹ {model} è¿›è¡Œå›¾åƒè½¬æ¢...")
            _log_info(f"ğŸ“ è½¬æ¢æŒ‡ä»¤: {enhanced_prompt[:100]}...")
            
            response_json = generate_with_priority_api(api_key, model, content_parts, generation_config, proxy=proxy, base_url=None)
            
            # å¤„ç†å“åº”
            raw_text = extract_text_from_response(response_json)
            edited_image = process_generated_image_from_response(response_json)
            
            # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
            if edited_image is None:
                _log_warning("æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                edited_image = pil_image
                if not raw_text:
                    raw_text = "å›¾ç‰‡ç¼–è¾‘è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"

            # ç¡®ä¿edited_imageæ˜¯PIL Imageæ ¼å¼
            if isinstance(edited_image, torch.Tensor):
                edited_image = tensor_to_pil(edited_image)

            # ğŸ” æ™ºèƒ½AIæ”¾å¤§
            if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                try:
                    # æå–æ”¾å¤§å€æ•°
                    scale = int(upscale_factor.replace("x", "").strip().split()[0])
                    if scale > 1:
                        _log_info(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                        # å¯¼å…¥æ”¾å¤§å‡½æ•°
                        try:
                            from .banana_upscale import smart_upscale
                        except ImportError:
                            from banana_upscale import smart_upscale

                        # è®¡ç®—ç›®æ ‡å°ºå¯¸
                        target_w = edited_image.width * scale
                        target_h = edited_image.height * scale

                        # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                        upscaled_image = smart_upscale(
                            edited_image,
                            target_w,
                            target_h,
                            gigapixel_model
                        )

                        if upscaled_image:
                            edited_image = upscaled_image
                            _log_info(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                        else:
                            _log_warning("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                except Exception as e:
                    _log_warning(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

            # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
            if not raw_text:
                response_text = "å›¾ç‰‡ç¼–è¾‘å®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„ç¼–è¾‘æŒ‡ä»¤ä¿®æ”¹åçš„å›¾åƒã€‚"
                _log_info("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")
            else:
                response_text = raw_text.strip()
            
            # è½¬æ¢ä¸ºtensor
            if isinstance(edited_image, Image.Image):
                image_tensor = pil_to_tensor(edited_image)
            elif isinstance(edited_image, torch.Tensor):
                image_tensor = edited_image
            else:
                _log_error(f"æœªçŸ¥çš„å›¾åƒç±»å‹: {type(edited_image)}")
                # åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„é»‘è‰²å›¾åƒtensor
                image_tensor = torch.zeros((1, 512, 512, 3), dtype=torch.float32)
            
            _log_info("âœ… å›¾ç‰‡ç¼–è¾‘å®Œæˆ")
            _log_info(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
            _log_info(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")
            self._push_chat(enhanced_prompt, response_text or "", unique_id) # ä½¿ç”¨å¢å¼ºåçš„æç¤ºè¯
            
            return (response_text, image_tensor)
            
        except Exception as e:
            error_msg = str(e)
            _log_error(f"å›¾åƒè½¬æ¢å¤±è´¥: {error_msg}")
            
            # å¢å¼ºçš„é”™è¯¯åˆ†ç±»å¤„ç†
            if "API key" in error_msg or "401" in error_msg or "403" in error_msg:
                friendly_error = "APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®"
            elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                friendly_error = "å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ï¼Œè¯·ä¿®æ”¹æç¤ºè¯"
            else:
                friendly_error = f"è½¬æ¢å¤±è´¥: {error_msg}"
            
            return (friendly_error, create_dummy_image())
    
    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000
            
            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")
            
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt, 
                            "response": display_response, 
                            "response_id": str(random.randint(100000, 999999)), 
                            "timestamp": time.time()
                        }
                    ], ensure_ascii=False)
                },
            }
            PromptServer.instance.send_sync("display_component", render_spec)
        except Exception as e:
            _log_error(f"Chat push failed: {e}")
            pass

class KenChenLLMGeminiBananaMultimodalBananaNode:
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "analyze_multimodal"
    

    
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        
        # Get multimodal models from config, with fallback to latest models
        models = config.get('models', {}).get('multimodal_models', [])
        if not models:
            # Fallback to latest models if config is empty - only include true multimodal models
            models = [
                "gemini-2.5-pro-exp-03-25",  # Latest multimodal model - supports image+text
                "gemini-2.5-flash-preview-04-17",  # Supports image+text
                "gemini-2.5-pro-preview-05-06",  # Supports image+text
                "gemini-2.0-flash-exp-image-generation",  # Supports image+text
            ]
        
        # Get default model from config, prioritize latest models
        default_model = config.get('default_model', {}).get('multimodal', "gemini-2.5-pro-exp-03-25")
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        
        return {
            "required": {
                "api_key": ("STRING", {
                    "default": "", 
                    "multiline": False,
                    "placeholder": "APIå¯†é’¥ï¼ˆç•™ç©ºæ—¶è‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è¯»å–ï¼Œå»ºè®®ä¿æŒä¸ºç©ºä»¥ç¡®ä¿å®‰å…¨ï¼‰"
                }),
                "prompt": ("STRING", {"default": "Describe what you see", "multiline": True}),
                "model": (
                    models,
                    {"default": default_model},
                ),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),

                # ğŸ¨ åˆ†ææ§åˆ¶ç»„
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "analysis_mode": (["Auto Select", "Visual Analysis", "Audio Analysis", "Combined Analysis", "Detailed Description", "Summary Report"], {"default": "Auto Select"}),
                "output_format": (["Natural Language", "Structured Report", "Technical Analysis", "Creative Description", "Professional Summary"], {"default": "Natural Language"}),
                
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 8192}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                "image": ("IMAGE",),
                "audio": ("AUDIO",),
                
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰åˆ†æè¦æ±‚å’Œç‰¹æ®ŠæŒ‡ä»¤"
                }),
            },
        }
    

    
    def analyze_multimodal(
        self,
        api_key,
        prompt,
        model,
        proxy,
        detail_level,
        analysis_mode,
        output_format,
        temperature,
        top_p,
        top_k,
        max_output_tokens,
        seed,
        image=None,
        audio=None,
        custom_additions="",
    ):
        try:
            # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥APIå¯†é’¥ï¼Œè‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è·å–
            if not api_key or not api_key.strip():
                config = get_gemini_banana_config()
                auto_api_key = config.get('multimodal_api_key', '')
                if auto_api_key and auto_api_key.strip():
                    api_key = auto_api_key.strip()
                    _log_info(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥: {api_key[:8]}...")
                else:
                    error_msg = "APIå¯†é’¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®multimodal_api_keyæˆ–æ‰‹åŠ¨è¾“å…¥"
                    _log_error(error_msg)
                    return (error_msg,)
            
            # è®¾ç½®ä»£ç†
            # ä»£ç†å¤„ç†ï¼šä½¿ç”¨ proxies å‚æ•°ï¼Œä¸è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œé¿å…å†²çª
            if proxy and proxy.strip() and "None" not in proxy:
                # ä½¿ç”¨ proxies å‚æ•°ï¼Œä¸è®¾ç½®ç¯å¢ƒå˜é‡
                _log_info(f"ä½¿ç”¨ä»£ç†: {proxy.strip()}")
            else:
                _log_info("ğŸ”Œ ä½¿ç”¨ç³»ç»Ÿä»£ç†")
            
            # æ„å»ºç”Ÿæˆé…ç½®ï¼ˆå¤šæ¨¡æ€åˆ†æåªéœ€è¦TEXTè¾“å‡ºï¼‰
            generation_config = {
                "temperature": temperature,
                "topP": top_p,
                "topK": top_k,
                "maxOutputTokens": max_output_tokens,
                "responseModalities": ["TEXT"]  # åªéœ€è¦æ–‡æœ¬è¾“å‡º
            }
            
            if seed > 0:
                generation_config["seed"] = seed
            

            
            # å‡†å¤‡å†…å®¹ - æ–‡æœ¬ + å¤šåª’ä½“
            content_parts = [{"text": prompt.strip()}]
            content_parts.extend(prepare_media_content(image=image, audio=audio))
            
            # ä½¿ç”¨REST APIè°ƒç”¨
            _log_info(f"ğŸ” ä½¿ç”¨æ¨¡å‹ {model} è¿›è¡Œå¤šæ¨¡æ€åˆ†æ...")
            _log_info(f"ğŸ“ åˆ†ææç¤º: {prompt[:100]}...")
            
            response_json = generate_with_priority_api(api_key, model, content_parts, generation_config, proxy=proxy)
            
            # æå–æ–‡æœ¬å“åº”
            generated_text = extract_text_from_response(response_json)
            
            if not generated_text or generated_text == "Response received but no text content":
                generated_text = "æ¨¡å‹æœªè¿”å›æœ‰æ•ˆçš„åˆ†æç»“æœ"
            
            _log_info("âœ… å¤šæ¨¡æ€åˆ†ææˆåŠŸå®Œæˆ")
            return (generated_text,)
            
        except Exception as e:
            error_msg = str(e)
            _log_error(f"å¤šæ¨¡æ€åˆ†æå¤±è´¥: {error_msg}")
            
            # å¢å¼ºçš„é”™è¯¯åˆ†ç±»å¤„ç†
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                friendly_error = (
                    "APIé…é¢è¶…é™ã€‚è§£å†³æ–¹æ¡ˆ:\n"
                    "1. ç­‰å¾…é…é¢é‡ç½®ï¼ˆé€šå¸¸24å°æ—¶ï¼‰\n"
                    "2. å‡çº§åˆ°ä»˜è´¹è´¦æˆ·\n"
                    "3. ä½¿ç”¨å…è´¹æ¨¡å‹\n"
                    "4. æ£€æŸ¥è®¡è´¹è®¾ç½®"
                )
            elif "connection" in error_msg.lower() or "network" in error_msg.lower():
                friendly_error = f"ç½‘ç»œè¿æ¥é”™è¯¯: è¯·æ£€æŸ¥ä»£ç†è®¾ç½®å’Œç½‘ç»œè¿æ¥"
            elif "not found" in error_msg.lower() or "404" in error_msg:
                friendly_error = f"æ¨¡å‹ä¸å¯ç”¨: {model} å¯èƒ½ä¸æ”¯æŒå¤šæ¨¡æ€åˆ†ææˆ–æš‚æ—¶ä¸å¯ç”¨"
            elif "API key" in error_msg or "401" in error_msg or "403" in error_msg:
                friendly_error = "APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®"
            elif "safety" in error_msg.lower() or "blocked" in error_msg.lower():
                friendly_error = "å†…å®¹è¢«å®‰å…¨è¿‡æ»¤å™¨é˜»æ­¢ï¼Œè¯·ä¿®æ”¹æç¤ºè¯æˆ–åª’ä½“å†…å®¹"
            else:
                friendly_error = f"åˆ†æå¤±è´¥: {error_msg}"
            
            return (friendly_error,)

class GeminiBananaMultiImageEditNode:
    """
    Gemini Banana å¤šå›¾åƒç¼–è¾‘èŠ‚ç‚¹
    
    åŠŸèƒ½ç‰¹æ€§:
    - æ”¯æŒå¤šå¼ è¾“å…¥å›¾åƒï¼ˆæœ€å¤š4å¼ ï¼‰
    - ä¸“ä¸šçš„å›¾åƒç¼–è¾‘æç¤ºè¯
    - æ”¯æŒå°ºå¯¸ã€è´¨é‡ã€é£æ ¼æ§åˆ¶
    - æ™ºèƒ½å›¾åƒç»„åˆç¼–è¾‘
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        config = get_gemini_banana_config()
        default_params = config.get('default_params', {})
        image_settings = config.get('image_settings', {})
        
        # ğŸš€ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶é¢„è®¾
        aspect_ratios = image_settings.get('aspect_ratios', [
            "1:1", "2:3", "3:2", "3:4", "4:3", "4:5", "5:4", "9:16", "16:9", "21:9"
        ])
        response_modalities = image_settings.get('response_modalities', [
            "TEXT_AND_IMAGE", "IMAGE_ONLY"
        ])
        quality_presets = image_settings.get('quality_presets', [
            "standard", "hd", "ultra_hd", "ai_enhanced", "ai_ultra"  # ğŸš€ AIè¶…åˆ†è¾¨ç‡å¢å¼ºé€‰é¡¹
        ])
        style_presets = image_settings.get('style_presets', [
            "vivid", "natural", "artistic", "cinematic", "photographic"  # è¶…è¶Šå‚è€ƒé¡¹ç›®çš„é£æ ¼é€‰é¡¹
        ])
        
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "è¯·æ ¹æ®è¿™äº›å›¾ç‰‡è¿›è¡Œä¸“ä¸šçš„å›¾åƒç¼–è¾‘", "multiline": True}),
                "model": (["gemini-2.5-flash-image", "gemini-2.5-flash-image-preview", "gemini-2.0-flash"], {"default": "gemini-2.5-flash-image"}),

                # ğŸ“ Geminiå®˜æ–¹APIå›¾åƒæ§åˆ¶å‚æ•°
                "aspect_ratio": (aspect_ratios, {
                    "default": image_settings.get('default_aspect_ratio', "1:1"),
                    "tooltip": "å›¾åƒå®½é«˜æ¯” (Geminiå®˜æ–¹APIæ”¯æŒ)"
                }),
                "response_modality": (response_modalities, {
                    "default": image_settings.get('default_response_modality', "TEXT_AND_IMAGE"),
                    "tooltip": "å“åº”æ¨¡å¼ï¼šTEXT_AND_IMAGE=æ–‡å­—+å›¾åƒï¼ŒIMAGE_ONLY=ä»…å›¾åƒ"
                }),

                # ğŸ” Topaz Gigapixel AIæ”¾å¤§æ§åˆ¶
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                "quality": (quality_presets, {"default": image_settings.get('default_quality', "hd")}),
                "style": (style_presets, {"default": image_settings.get('default_style', "natural")}),

                # ğŸ¨ æ™ºèƒ½å›¾åƒæ§åˆ¶ç»„ï¼ˆæ”¾åœ¨styleä¸‹é¢ï¼‰
                "detail_level": (["Basic Detail", "Professional Detail", "Premium Quality", "Masterpiece Level"], {"default": "Professional Detail"}),
                "camera_control": (["Auto Select", "Wide-angle Lens", "Macro Shot", "Low-angle Perspective", "High-angle Shot", "Close-up Shot", "Medium Shot"], {"default": "Auto Select"}),
                "lighting_control": (["Auto Settings", "Natural Light", "Studio Lighting", "Dramatic Shadows", "Soft Glow", "Golden Hour", "Blue Hour"], {"default": "Auto Settings"}),
                "template_selection": (["Auto Select", "Professional Portrait", "Cinematic Landscape", "Product Photography", "Digital Concept Art", "Anime Style Art", "Photorealistic Render", "Classical Oil Painting", "Watercolor Painting", "Cyberpunk Future", "Vintage Film Photography", "Architectural Photography", "Gourmet Food Photography"], {"default": "Auto Select"}),

                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.95), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 8192), "min": 0, "max": 32768}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 999999}),
                "post_generation_control": (["randomize", "maintain_consistency", "enhance_creativity"], {"default": "randomize"}),
            },
            "optional": {
                "image1": ("IMAGE",),
                "image2": ("IMAGE",),
                "image3": ("IMAGE",),
                "image4": ("IMAGE",),
                
                # âœ¨ è‡ªå®šä¹‰æŒ‡ä»¤ç»„
                "custom_additions": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "è‡ªå®šä¹‰æ·»åŠ å’Œç‰¹æ®Šè¦æ±‚"
                }),
            },
            "hidden": {
                "unique_id": "UNIQUE_ID"
            }
        }
    
    RETURN_TYPES = ("IMAGE", "STRING")
    RETURN_NAMES = ("edited_image", "response_text")
    FUNCTION = "edit_multiple_images"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    def _push_chat(self, user_prompt: str, response_text: str, unique_id: str):
        if not PromptServer or not unique_id:
            return
        try:
            # é™åˆ¶æç¤ºè¯é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æœ¬å¯¼è‡´æ˜¾ç¤ºé—®é¢˜
            max_prompt_length = 500
            max_response_length = 1000
            
            # æˆªæ–­è¿‡é•¿çš„æç¤ºè¯å’Œå“åº”
            display_prompt = user_prompt[:max_prompt_length] + ("..." if len(user_prompt) > max_prompt_length else "")
            display_response = response_text[:max_response_length] + ("..." if len(response_text) > max_response_length else "")
            
            render_spec = {
                "node_id": unique_id,
                "component": "ChatHistoryWidget",
                "props": {
                    "history": json.dumps([
                        {
                            "prompt": display_prompt, 
                            "response": display_response, 
                            "response_id": str(random.randint(100000, 999999)), 
                            "timestamp": time.time()
                        }
                    ], ensure_ascii=False)
                },
            }
            PromptServer.instance.send_sync("display_component", render_spec)
        except Exception as e:
            _log_error(f"Chat push failed: {e}")
            pass

    def edit_multiple_images(self, api_key: str, prompt: str, model: str, aspect_ratio: str, response_modality: str,
                           upscale_factor: str, gigapixel_model: str, quality: str, style: str, detail_level: str,
                           camera_control: str, lighting_control: str, template_selection: str, temperature: float, top_p: float,
                           top_k: int, max_output_tokens: int, seed: int, post_generation_control: str,
                           image1=None, image2=None, image3=None, image4=None, custom_additions: str = "", unique_id: str = "") -> Tuple[torch.Tensor, str]:
        """ä½¿ç”¨ Gemini API è¿›è¡Œå¤šå›¾åƒç¼–è¾‘"""

        # å¦‚æœç”¨æˆ·æ²¡æœ‰è¾“å…¥APIå¯†é’¥ï¼Œè‡ªåŠ¨ä»é…ç½®æ–‡ä»¶è·å–
        if not api_key or not api_key.strip():
            config = get_gemini_banana_config()
            auto_api_key = config.get('api_key', '')
            if auto_api_key and auto_api_key.strip():
                api_key = auto_api_key.strip()
                print(f"ğŸ”‘ è‡ªåŠ¨ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„APIå¯†é’¥: {api_key[:8]}...")
            else:
                raise ValueError("APIå¯†é’¥ä¸èƒ½ä¸ºç©ºï¼Œè¯·åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½®api_keyæˆ–æ‰‹åŠ¨è¾“å…¥")

        # éªŒè¯APIå¯†é’¥
        if not validate_api_key(api_key):
            raise ValueError("API Keyæ ¼å¼æ— æ•ˆæˆ–ä¸ºç©º")
        
        # éªŒè¯æç¤ºè¯
        if not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")

        # ğŸ¨ æ„å»ºå¢å¼ºæç¤ºè¯ï¼ˆä½¿ç”¨enhance_prompt_with_controlså‡½æ•°ï¼‰
        controls = process_image_controls(quality, style)

        # ä½¿ç”¨enhance_prompt_with_controlså‡½æ•°è¿›è¡Œå®Œæ•´çš„æç¤ºè¯å¢å¼º
        enhanced_prompt = enhance_prompt_with_controls(
            prompt.strip(),
            controls,
            detail_level,
            camera_control,
            lighting_control,
            template_selection,
            quality_enhancement="Auto",  # é»˜è®¤å€¼
            enhance_quality=True,  # é»˜è®¤å€¼
            smart_resize=True,  # é»˜è®¤å€¼
            fill_color="white"  # é»˜è®¤å€¼
        )

        # å¤„ç†è‡ªå®šä¹‰æŒ‡ä»¤
        if custom_additions and custom_additions.strip():
            enhanced_prompt += f"\n\n{custom_additions.strip()}"
            print(f"ğŸ“ æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤: {custom_additions[:100]}...")

        print(f"ğŸ¨ å›¾åƒæ§åˆ¶å‚æ•°: aspect_ratio={aspect_ratio}, quality={quality}, style={style}")
        
        # æ”¶é›†æ‰€æœ‰è¾“å…¥çš„å›¾åƒ
        all_input_pils = []
        input_images = [image1, image2, image3, image4]
        
        for i, img_tensor in enumerate(input_images):
            if img_tensor is not None:
                try:
                    pil_image = tensor_to_pil(img_tensor)
                    if pil_image:
                        all_input_pils.append(pil_image)
                        print(f"ğŸ“¸ æ·»åŠ è¾“å…¥å›¾åƒ {i+1}: {pil_image.size}")
                except Exception as e:
                    print(f"âš ï¸ å›¾åƒ {i+1} å¤„ç†å¤±è´¥: {e}")
        
        if not all_input_pils:
            raise ValueError("é”™è¯¯ï¼šè¯·è¾“å…¥è‡³å°‘ä¸€å¼ è¦ç¼–è¾‘çš„å›¾åƒ")
        
        print(f"ğŸ–¼ï¸ æ€»å…±æ”¶é›†åˆ° {len(all_input_pils)} å¼ è¾“å…¥å›¾åƒ")
        
        # ğŸš€ åº”ç”¨çœŸæ­£çš„å›¾å½¢å¢å¼ºæŠ€æœ¯
        # 1. å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†å¢å¼º
        enhanced_input_pils = []
        for i, pil_image in enumerate(all_input_pils):
            print(f"ğŸ¨ å¤„ç†è¾“å…¥å›¾åƒ {i+1}...")
            enhanced_image = pil_image

            # è°ƒæ•´å›¾åƒå°ºå¯¸ä»¥ç¬¦åˆAPIè¦æ±‚
            enhanced_image = resize_image_for_api(enhanced_image)
            enhanced_input_pils.append(enhanced_image)

        # 2. æ„å»ºç»“æ„åŒ–å†…å®¹ï¼šå›¾ç‰‡æ ‡è¯† + å¢å¼ºåçš„å›¾ç‰‡æ•°æ® + æŒ‡ä»¤æ–‡æœ¬
        content = []

        # å…ˆæ·»åŠ å›¾ç‰‡æ ‡è¯†æ–‡æœ¬ï¼ˆå‚è€ƒé¡¹ç›®çš„æ ¸å¿ƒæŠ€æœ¯ï¼‰
        image_labels = ["Figure 1", "Figure 2", "Figure 3", "Figure 4"]
        for i, enhanced_image in enumerate(enhanced_input_pils):
            content.append({
                "type": "text",
                "text": f"[This is {image_labels[i]}]"
            })

        # å†æ·»åŠ å¢å¼ºåçš„å›¾ç‰‡æ•°æ®
        for enhanced_image in enhanced_input_pils:
            # è½¬æ¢ä¸ºbase64
            image_base64 = image_to_base64(enhanced_image, format='JPEG')
            content.append({
                "inline_data": {
                    "mime_type": "image/jpeg",
                    "data": image_base64
                }
            })
        
        # 3. æœ€åæ·»åŠ å¼ºåˆ¶å›¾åƒç”ŸæˆæŒ‡ä»¤ï¼ˆå‚è€ƒé¡¹ç›®çš„æ ¸å¿ƒæŠ€æœ¯ï¼‰
        if len(all_input_pils) == 1:
            # å•å›¾ç¼–è¾‘
            image_edit_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual image, not just text description.

Task: {prompt}

REQUIREMENTS:
1. GENERATE a new edited image based on my request
2. DO NOT just describe what the image should look like
3. RETURN the actual image file/data
4. The output MUST be a visual image, not text

Execute the image editing task now and return the generated image."""
        else:
            # å¤šå›¾ç¼–è¾‘
            image_edit_instruction = f"""CRITICAL INSTRUCTION: You MUST generate and return an actual image, not just text description.

Task: {enhanced_prompt}

Image References:
- When I mention "Figure 1", "ç¬¬ä¸€å¼ å›¾ç‰‡", "å·¦è¾¹å›¾ç‰‡", I mean the first image provided above
- When I mention "Figure 2", "ç¬¬äºŒå¼ å›¾ç‰‡", "å³è¾¹å›¾ç‰‡", I mean the second image provided above
- When I mention "Figure 3", "ç¬¬ä¸‰å¼ å›¾ç‰‡", I mean the third image provided above
- When I mention "Figure 4", "ç¬¬å››å¼ å›¾ç‰‡", I mean the fourth image provided above

REQUIREMENTS:
1. GENERATE a new image based on my request and the provided reference images
2. DO NOT just describe what the image should look like
3. RETURN the actual image file/data
4. The output MUST be a visual image, not text
5. Combine elements from the reference images as specified in the task
6. Maintain high quality and natural appearance

Execute the image editing task now and return the generated image."""
        
        content.append({"type": "text", "text": image_edit_instruction})
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        generation_config = {
            "temperature": temperature,
            "topP": top_p,
            "topK": top_k,
            "maxOutputTokens": max_output_tokens,
        }

        # ğŸ¯ Geminiå®˜æ–¹APIï¼šResponse Modalitiesæ§åˆ¶
        if response_modality == "IMAGE_ONLY":
            generation_config["responseModalities"] = ["Image"]
            print("ğŸ“Š å“åº”æ¨¡å¼ï¼šä»…å›¾åƒï¼ˆIMAGE_ONLYï¼‰")
        else:
            generation_config["responseModalities"] = ["Text", "Image"]
            print("ğŸ“Š å“åº”æ¨¡å¼ï¼šæ–‡å­—+å›¾åƒï¼ˆTEXT_AND_IMAGEï¼‰")

        # ğŸ“ Geminiå®˜æ–¹APIï¼šAspect Ratioæ§åˆ¶
        if aspect_ratio and aspect_ratio != "1:1":
            generation_config["imageConfig"] = {
                "aspectRatio": aspect_ratio
            }
            print(f"ğŸ“ è®¾ç½®å®½é«˜æ¯”: {aspect_ratio}")

        # æ™ºèƒ½ç§å­æ§åˆ¶
        if seed and seed > 0:
            generation_config["seed"] = seed

        request_data = {
            "contents": [{
                "parts": content
            }],
            "generationConfig": generation_config
        }
        
        # è®¾ç½®è¯·æ±‚å¤´
        headers = {
            "Content-Type": "application/json",
            "x-goog-api-key": api_key.strip()
        }
        
        # æ™ºèƒ½é‡è¯•æœºåˆ¶
        max_retries = 5
        timeout = 120
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ–¼ï¸ æ­£åœ¨ç¼–è¾‘å›¾ç‰‡... (å°è¯• {attempt + 1}/{max_retries})")
                print(f"ğŸ“ ç¼–è¾‘æŒ‡ä»¤: {enhanced_prompt[:100]}...")
                print(f"ğŸ”— ä½¿ç”¨æ¨¡å‹: {model}")
                
                # ä½¿ç”¨ä¼˜å…ˆAPIè°ƒç”¨
                result = generate_with_priority_api_direct(
                    api_key, 
                    model, 
                    request_data, 
                    max_retries=1,  # åœ¨é‡è¯•å¾ªç¯ä¸­åªå°è¯•ä¸€æ¬¡
                    proxy=None,
                    base_url=get_gemini_banana_config().get('base_url', 'https://generativelanguage.googleapis.com')
                )
                
                # æˆåŠŸå“åº”
                if result:
                    print(f"ğŸ“‹ APIå“åº”ç»“æ„: {list(result.keys())}")
                    
                    # æå–æ–‡æœ¬å“åº”å’Œç¼–è¾‘åçš„å›¾ç‰‡
                    response_text = ""
                    edited_image = None
                    
                    if "candidates" in result and result["candidates"]:
                        candidate = result["candidates"][0]
                        if "content" in candidate and "parts" in candidate["content"]:
                            for part in candidate["content"]["parts"]:
                                # æå–æ–‡æœ¬
                                if "text" in part:
                                    response_text += part["text"]
                                
                                # æå–ç¼–è¾‘åçš„å›¾ç‰‡
                                if "inline_data" in part or "inlineData" in part:
                                    inline_data = part.get("inline_data") or part.get("inlineData")
                                    if inline_data and "data" in inline_data:
                                        try:
                                            # è§£ç å›¾ç‰‡æ•°æ®
                                            image_data = inline_data["data"]
                                            image_bytes = base64.b64decode(image_data)
                                            edited_image = Image.open(io.BytesIO(image_bytes))
                                            print("âœ… æˆåŠŸæå–ç¼–è¾‘åçš„å›¾ç‰‡")
                                        except Exception as e:
                                            print(f"âš ï¸ è§£ç å›¾ç‰‡å¤±è´¥: {e}")
                    
                    # å¦‚æœæ²¡æœ‰ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡
                    if edited_image is None:
                        print("âš ï¸ æœªæ£€æµ‹åˆ°ç¼–è¾‘åçš„å›¾ç‰‡ï¼Œè¿”å›åŸå›¾ç‰‡")
                        edited_image = all_input_pils[0]  # è¿”å›ç¬¬ä¸€å¼ å›¾ç‰‡
                        if not response_text:
                            response_text = "å›¾ç‰‡ç¼–è¾‘è¯·æ±‚å·²å‘é€ï¼Œä½†æœªæ”¶åˆ°ç¼–è¾‘åçš„å›¾ç‰‡"

                    # ğŸ” æ™ºèƒ½AIæ”¾å¤§
                    if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)" and isinstance(edited_image, Image.Image):
                        try:
                            # æå–æ”¾å¤§å€æ•°
                            scale = int(upscale_factor.replace("x", "").strip().split()[0])
                            if scale > 1:
                                print(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                                # å¯¼å…¥æ”¾å¤§å‡½æ•°
                                try:
                                    from .banana_upscale import smart_upscale
                                except ImportError:
                                    from banana_upscale import smart_upscale

                                # è®¡ç®—ç›®æ ‡å°ºå¯¸
                                target_w = edited_image.width * scale
                                target_h = edited_image.height * scale

                                # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                                upscaled_image = smart_upscale(
                                    edited_image,
                                    target_w,
                                    target_h,
                                    gigapixel_model
                                )

                                if upscaled_image:
                                    edited_image = upscaled_image
                                    print(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {edited_image.size}")
                                else:
                                    print("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                        except Exception as e:
                            print(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                    # å¦‚æœæ²¡æœ‰å“åº”æ–‡æœ¬ï¼Œæä¾›é»˜è®¤æ–‡æœ¬
                    if not response_text:
                        response_text = "å¤šå›¾åƒç¼–è¾‘å®Œæˆï¼è¿™æ˜¯æ ¹æ®æ‚¨çš„æŒ‡ä»¤å’Œå‚è€ƒå›¾åƒç”Ÿæˆçš„ç¼–è¾‘ç»“æœã€‚"
                        print("ğŸ“ ä½¿ç”¨é»˜è®¤å“åº”æ–‡æœ¬")

                    # è½¬æ¢ä¸ºtensor
                    image_tensor = pil_to_tensor(edited_image)
                    
                    print("âœ… å¤šå›¾åƒç¼–è¾‘å®Œæˆ")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬é•¿åº¦: {len(response_text)}")
                    print(f"ğŸ“ å“åº”æ–‡æœ¬å†…å®¹: {response_text[:200]}...")
                    self._push_chat(enhanced_prompt, response_text or "", unique_id)
                    return (image_tensor, response_text)
                
                # å¤„ç†é”™è¯¯å“åº”
                else:
                    print(f"âŒ APIè°ƒç”¨å¤±è´¥: æœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
                    
                    # å¦‚æœæ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if attempt == max_retries - 1:
                        raise ValueError("APIè°ƒç”¨å¤±è´¥: æœªæ”¶åˆ°æœ‰æ•ˆå“åº”")
                    
                    # æ™ºèƒ½ç­‰å¾…
                    delay = smart_retry_delay(attempt, 500)  # ä½¿ç”¨é€šç”¨é”™è¯¯ç 
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)
                    
            except Exception as e:
                error_msg = format_error_message(e)
                print(f"âŒ è¯·æ±‚å¤±è´¥: {error_msg}")
                if attempt == max_retries - 1:
                    # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    print(f"âŒ æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥äº†ï¼Œå¤šå›¾åƒç¼–è¾‘å¤±è´¥: {error_msg}")
                    raise ValueError(f"å¤šå›¾åƒç¼–è¾‘å¤±è´¥: {error_msg}")
                else:
                    delay = smart_retry_delay(attempt)
                    print(f"ğŸ”„ ç­‰å¾… {delay:.1f} ç§’åé‡è¯•...")
                    time.sleep(delay)

# ç¿»è¯‘èŠ‚ç‚¹å·²ç§»åŠ¨åˆ°ç‹¬ç«‹æ¨¡å— gemini_banana_translation.py

# Node mappings
NODE_CLASS_MAPPINGS = {
    "KenChenLLMGeminiBananaTextToImageBananaNode": KenChenLLMGeminiBananaTextToImageBananaNode,
    "KenChenLLMGeminiBananaImageToImageBananaNode": KenChenLLMGeminiBananaImageToImageBananaNode,
    "KenChenLLMGeminiBananaMultimodalBananaNode": KenChenLLMGeminiBananaMultimodalBananaNode,
    "GeminiBananaMultiImageEdit": GeminiBananaMultiImageEditNode,
}

# é›†æˆç‹¬ç«‹ç¿»è¯‘æ¨¡å—
if TRANSLATION_MODULE_AVAILABLE:
    NODE_CLASS_MAPPINGS.update(TRANSLATION_NODE_MAPPINGS)

NODE_DISPLAY_NAME_MAPPINGS = {
    "KenChenLLMGeminiBananaTextToImageBananaNode": "Gemini Banana Text to Image Banana",
    "KenChenLLMGeminiBananaImageToImageBananaNode": "Gemini Banana Image to Image Banana",
    "KenChenLLMGeminiBananaMultimodalBananaNode": "Gemini Banana Multimodal Banana",
    "GeminiBananaMultiImageEdit": "Gemini Banana Multi Image Edit",
}

# é›†æˆç‹¬ç«‹ç¿»è¯‘æ¨¡å—æ˜¾ç¤ºåç§°
if TRANSLATION_MODULE_AVAILABLE:
    NODE_DISPLAY_NAME_MAPPINGS.update(TRANSLATION_DISPLAY_MAPPINGS)

