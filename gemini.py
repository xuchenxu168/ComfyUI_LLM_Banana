import os
import base64
import io
import json
import torch
import numpy as np
from PIL import Image
import requests
from io import BytesIO
import traceback
import tempfile
import time

# è§†é¢‘å¤„ç†ç›¸å…³å¯¼å…¥
try:
    import cv2
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("âš ï¸ opencv-python æœªå®‰è£…ï¼Œè§†é¢‘å¤„ç†åŠŸèƒ½å°†å—é™ã€‚è¯·è¿è¡Œ: pip install opencv-python")

# Gemini SDK å¯¼å…¥
try:
    from google import genai
    from google.genai import types
    GENAI_SDK_AVAILABLE = True
except ImportError:
    GENAI_SDK_AVAILABLE = False
    print("âš ï¸ google-genai SDK æœªå®‰è£…ï¼Œå°†ä½¿ç”¨ REST APIã€‚å»ºè®®è¿è¡Œ: pip install google-genai")

# å»¶è¿Ÿå¯¼å…¥google.genai (ä¿ç•™å…¼å®¹æ€§)
def get_google_genai():
    try:
        import google.genai
        return google.genai
    except ImportError:
        return None

def _log_info(message):
    print(f"[LLM Prompt] {message}")

def _log_warning(message):
    print(f"[LLM Prompt] WARNING: {message}")

def _log_error(message):
    print(f"[LLM Prompt] ERROR: {message}")

def load_gemini_config():
    """åŠ è½½Geminié…ç½®æ–‡ä»¶"""
    config_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "Gemini_config.json")
    try:
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            _log_info(f"æˆåŠŸåŠ è½½Geminié…ç½®æ–‡ä»¶: {config_path}")
            return config
        else:
            _log_warning(f"Geminié…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return {}
    except Exception as e:
        _log_error(f"åŠ è½½Geminié…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {}

def get_gemini_config():
    """è·å–Geminié…ç½®ï¼Œä¼˜å…ˆä»é…ç½®æ–‡ä»¶è¯»å–"""
    config = load_gemini_config()
    return config

def save_video_tensor_to_mp4(video_tensor, output_path, fps=30):
    """
    å°† ComfyUI çš„ VIDEO tensor ä¿å­˜ä¸º MP4 æ–‡ä»¶

    Args:
        video_tensor: torch.Tensor, shape [frames, channels, height, width]
        output_path: str, è¾“å‡º MP4 æ–‡ä»¶è·¯å¾„
        fps: int, å¸§ç‡ï¼ˆé»˜è®¤ 30ï¼‰

    Returns:
        str: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    if not CV2_AVAILABLE:
        _log_error("opencv-python æœªå®‰è£…ï¼Œæ— æ³•ä¿å­˜è§†é¢‘ä¸º MP4")
        return None

    try:
        frames_count = video_tensor.shape[0]
        height = video_tensor.shape[2]
        width = video_tensor.shape[3]

        _log_info(f"ğŸ“¹ ä¿å­˜è§†é¢‘: {frames_count} å¸§, {width}x{height}, {fps} FPS")

        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨ (ä½¿ç”¨ mp4v ç¼–ç å™¨)
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        if not out.isOpened():
            _log_error(f"æ— æ³•åˆ›å»ºè§†é¢‘å†™å…¥å™¨: {output_path}")
            return None

        for i in range(frames_count):
            frame = video_tensor[i]

            # è½¬æ¢ä¸º numpy æ•°ç»„ (HWC æ ¼å¼)
            if frame.shape[0] in [1, 3, 4]:  # CHW æ ¼å¼
                frame_np = frame.permute(1, 2, 0).cpu().numpy()
            else:  # HWC æ ¼å¼
                frame_np = frame.cpu().numpy()

            # è½¬æ¢ä¸º uint8
            if frame_np.max() <= 1.0:
                frame_np = (frame_np * 255).astype(np.uint8)
            else:
                frame_np = frame_np.astype(np.uint8)

            # å¤„ç†é€šé“æ•°
            if frame_np.shape[2] == 1:  # ç°åº¦
                frame_np = cv2.cvtColor(frame_np, cv2.COLOR_GRAY2BGR)
            elif frame_np.shape[2] == 3:  # RGB -> BGR
                frame_np = cv2.cvtColor(frame_np, cv2.COLOR_RGB2BGR)
            elif frame_np.shape[2] == 4:  # RGBA -> BGR
                frame_np = cv2.cvtColor(frame_np, cv2.COLOR_RGBA2BGR)

            out.write(frame_np)

        out.release()
        _log_info(f"âœ… è§†é¢‘ä¿å­˜æˆåŠŸ: {output_path}")
        return output_path

    except Exception as e:
        _log_error(f"ä¿å­˜è§†é¢‘å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def upload_video_to_gemini_file_api(video_path, api_key, display_name="video"):
    """
    ä½¿ç”¨ Gemini File API ä¸Šä¼ è§†é¢‘æ–‡ä»¶

    Args:
        video_path: str, è§†é¢‘æ–‡ä»¶è·¯å¾„
        api_key: str, Gemini API å¯†é’¥
        display_name: str, æ˜¾ç¤ºåç§°

    Returns:
        dict: åŒ…å« file_uri å’Œ mime_type çš„å­—å…¸ï¼Œå¦‚æœå¤±è´¥è¿”å› None
    """
    if not GENAI_SDK_AVAILABLE:
        _log_error("google-genai SDK æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨ File API")
        return None

    try:
        _log_info(f"ğŸ“¤ ä¸Šä¼ è§†é¢‘åˆ° Gemini File API: {video_path}")

        # åˆ›å»ºå®¢æˆ·ç«¯
        client = genai.Client(api_key=api_key)

        # ä¸Šä¼ æ–‡ä»¶
        uploaded_file = client.files.upload(file=video_path)

        _log_info(f"â³ ç­‰å¾…æ–‡ä»¶å¤„ç†... (æ–‡ä»¶å: {uploaded_file.name})")

        # ç­‰å¾…æ–‡ä»¶å¤„ç†å®Œæˆ
        max_wait = 60  # æœ€å¤šç­‰å¾… 60 ç§’
        wait_time = 0
        while uploaded_file.state.name == "PROCESSING":
            if wait_time >= max_wait:
                _log_error(f"æ–‡ä»¶å¤„ç†è¶…æ—¶ ({max_wait}ç§’)")
                return None

            time.sleep(1)
            wait_time += 1
            uploaded_file = client.files.get(name=uploaded_file.name)

        if uploaded_file.state.name == "FAILED":
            _log_error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {uploaded_file.name}")
            return None

        _log_info(f"âœ… è§†é¢‘ä¸Šä¼ æˆåŠŸ! URI: {uploaded_file.uri}")

        return {
            "file_uri": uploaded_file.uri,
            "mime_type": uploaded_file.mime_type,
            "file_name": uploaded_file.name
        }

    except Exception as e:
        _log_error(f"ä¸Šä¼ è§†é¢‘åˆ° File API å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

class KenChenLLMGeminiTextNode:
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_config()
        default_params = config.get('default_params', {})
        
        # æ·»åŠ é»˜è®¤æ¨¡å‹åˆ—è¡¨ï¼Œé˜²æ­¢é…ç½®åŠ è½½å¤±è´¥
        default_models = [
            "gemini-2.0-flash-lite",
            "gemini-1.5-pro",
            "gemini-1.5-flash-8b",
        ]
        
        models = config.get('models', {}).get('text_models', default_models)
        if not models:
            models = default_models
            print("[LLM Prompt] è­¦å‘Š: é…ç½®æ–‡ä»¶ä¸­çš„æ–‡æœ¬æ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨")
        
        default_model = config.get('default_model', {}).get('text_gen', "gemini-2.0-flash-lite")
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "", "multiline": True}),
                "model": (
                    models,
                    {"default": default_model},
                ),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 8192}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
            },
        }
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("text",)
    FUNCTION = "generate_text"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    
    def generate_text(
        self,
        api_key,
        prompt,
        model,
        proxy,
        temperature, 
        top_p, 
        top_k, 
        max_output_tokens,
        seed,
    ):
        try:
            genai = get_google_genai()
            if genai is None:
                return (f"é”™è¯¯: æœªå®‰è£…google-genaiåº“ï¼Œè¯·è¿è¡Œ: pip install google-genai",)
            
            config = get_gemini_config()
            final_api_key = api_key.strip() if api_key.strip() else config.get('api_key', '')
            final_proxy = proxy.strip() if proxy.strip() else config.get('proxy', '')
            if not final_api_key:
                return (f"é”™è¯¯: è¯·æä¾›Gemini API Key",)
            if final_proxy and final_proxy.strip() and "None" not in final_proxy:
                os.environ['HTTPS_PROXY'] = final_proxy
                os.environ['HTTP_PROXY'] = final_proxy
                _log_info(f"å·²è®¾ç½®ä»£ç†: {final_proxy}")
            elif final_proxy and "None" in final_proxy:
                _log_info("è·³è¿‡æ— æ•ˆä»£ç†è®¾ç½®")
            
            # ä½¿ç”¨æ–°çš„APIç»“æ„
            client = genai.Client(api_key=final_api_key)
            
            # å‡†å¤‡ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "max_output_tokens": max_output_tokens,
            }
            if seed > 0:
                generation_config["seed"] = seed
            
            # ç”Ÿæˆæ–‡æœ¬
            response = client.models.generate_content(
                model=model,
                contents=[{"parts": [{"text": prompt}]}],
                config=generation_config
            )
            
            if response.candidates and response.candidates[0].content.parts:
                text = response.candidates[0].content.parts[0].text
                _log_info(f"KenChen LLM Geminiæ–‡æœ¬ç”ŸæˆæˆåŠŸï¼Œæ¨¡å‹: {model}")
                return (text,)
            else:
                return (f"é”™è¯¯: æœªè·å¾—æœ‰æ•ˆå“åº”",)
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                _log_error(f"APIé…é¢é™åˆ¶: {error_msg}")
                return (f"é”™è¯¯: APIé…é¢é™åˆ¶ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š\n1. ä½¿ç”¨å…è´¹æ¨¡å‹ (å¦‚ gemini-2.0-flash-lite)\n2. å‡çº§åˆ°ä»˜è´¹è´¦æˆ·\n3. ç­‰å¾…é…é¢é‡ç½®\n\nè¯¦ç»†é”™è¯¯: {error_msg}",)
            else:
                error_msg = f"KenChen LLM Geminiæ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}"
                _log_error(error_msg)
                return (error_msg,)

class KenChenLLMGeminiMultimodalNode:
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_config()
        default_params = config.get('default_params', {})
        
        # æ·»åŠ é»˜è®¤æ¨¡å‹åˆ—è¡¨ï¼Œé˜²æ­¢é…ç½®åŠ è½½å¤±è´¥
        default_models = [
            "gemini-1.5-flash",
            "gemini-2.0-flash",
            "gemini-2.0-flash-exp-image-generation",
            "gemini-2.0-flash-thinking-exp-01-21",
            "gemini-2.5-pro-exp-03-25",
            "gemini-2.5-flash-preview-04-17",
            "gemini-2.5-pro-preview-05-06",
        ]
        
        models = config.get('models', {}).get('multimodal_models', default_models)
        if not models:
            models = default_models
            print("[LLM Prompt] è­¦å‘Š: é…ç½®æ–‡ä»¶ä¸­çš„å¤šæ¨¡æ€æ¨¡å‹åˆ—è¡¨ä¸ºç©ºï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨")
        
        default_model = config.get('default_model', {}).get('multimodal', "gemini-1.5-flash")
        default_proxy = config.get('proxy', "http://127.0.0.1:None")
        
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "", "multiline": True}),
                "model": (
                    models,
                    {"default": default_model},
                ),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "temperature": ("FLOAT", {"default": default_params.get('temperature', 0.9), "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": default_params.get('top_p', 0.9), "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": default_params.get('top_k', 40), "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": default_params.get('max_output_tokens', 2048), "min": 0, "max": 8192}),
                "seed": ("INT", {"default": default_params.get('seed', 0), "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                "image": ("IMAGE",),
                "audio": ("AUDIO",),
                "video": ("VIDEO",),
                "max_video_frames": ("INT", {
                    "default": 10,
                    "min": 1,
                    "max": 100,
                    "tooltip": "è§†é¢‘å¤„ç†æ—¶æœ€å¤šæå–çš„å¸§æ•°ã€‚å¢åŠ æ­¤å€¼ä¼šæé«˜è§†é¢‘åˆ†æè´¨é‡ï¼Œä½†ä¼šå¢åŠ APIæˆæœ¬å’Œå¤„ç†æ—¶é—´ã€‚å»ºè®®å€¼: 10-30"
                }),
            },
        }
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("multimodal_text",)  # æ”¹ä¸ºå”¯ä¸€çš„åç§°
    FUNCTION = "generate_multimodal"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    
    def generate_multimodal(
        self,
        api_key,
        prompt,
        model,
        proxy,
        temperature,
        top_p,
        top_k,
        max_output_tokens,
        seed,
        image=None,
        audio=None,
        video=None,
        max_video_frames=10,
    ):
        try:
            genai = get_google_genai()
            if genai is None:
                return (f"é”™è¯¯: æœªå®‰è£…google-genaiåº“ï¼Œè¯·è¿è¡Œ: pip install google-genai",)
            config = get_gemini_config()
            final_api_key = api_key.strip() if api_key.strip() else config.get('api_key', '')
            final_proxy = proxy.strip() if proxy.strip() else config.get('proxy', '')
            if not final_api_key:
                return (f"é”™è¯¯: è¯·æä¾›Gemini API Key",)
            if final_proxy and final_proxy.strip() and "None" not in final_proxy:
                os.environ['HTTPS_PROXY'] = final_proxy
                os.environ['HTTP_PROXY'] = final_proxy
                _log_info(f"å·²è®¾ç½®ä»£ç†: {final_proxy}")
            elif final_proxy and "None" in final_proxy:
                _log_info("è·³è¿‡æ— æ•ˆä»£ç†è®¾ç½®")
            
            # ä½¿ç”¨æ–°çš„APIç»“æ„
            client = genai.Client(api_key=final_api_key)
            
            # æ£€æŸ¥æ¨¡å‹èƒ½åŠ›
            _log_info(f"ä½¿ç”¨æ¨¡å‹: {model}")
            if "image-generation" in model.lower():
                _log_warning(f"è­¦å‘Š: {model} æ˜¯å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œå¯èƒ½ä¸æ”¯æŒéŸ³é¢‘å¤„ç†")
            elif "flash" in model.lower() and "exp" not in model.lower():
                _log_info(f"ä½¿ç”¨ Flash æ¨¡å‹: {model}ï¼Œæ”¯æŒå¤šæ¨¡æ€è¾“å…¥")
            else:
                _log_info(f"ä½¿ç”¨æ¨¡å‹: {model}")
            
            # å‡†å¤‡ç”Ÿæˆé…ç½®
            generation_config = {
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "max_output_tokens": max_output_tokens,
            }
            if seed > 0:
                generation_config["seed"] = seed
            
            # å‡†å¤‡å†…å®¹
            # å¦‚æœæœ‰è§†é¢‘è¾“å…¥ï¼Œéœ€è¦åœ¨æç¤ºè¯ä¸­æ·»åŠ è§†é¢‘ä¸Šä¸‹æ–‡
            video_context_added = False
            if video is not None:
                # æ£€æŸ¥æç¤ºè¯ä¸­æ˜¯å¦å·²ç»åŒ…å«è§†é¢‘ç›¸å…³çš„è¯´æ˜
                video_keywords = ['è§†é¢‘', 'video', 'å¸§', 'frame', 'åŠ¨æ€', 'è¿ç»­', 'åºåˆ—']
                has_video_context = any(keyword in prompt.lower() for keyword in video_keywords)

                if not has_video_context:
                    # è‡ªåŠ¨æ·»åŠ è§†é¢‘ä¸Šä¸‹æ–‡è¯´æ˜ï¼ˆæ›´å¼ºçš„æç¤ºï¼‰
                    video_prompt_prefix = (
                        "ã€é‡è¦æç¤º - è¿™æ˜¯è§†é¢‘åˆ†æä»»åŠ¡ã€‘\n"
                        "æ¥ä¸‹æ¥ä½ ä¼šçœ‹åˆ°å¤šå¼ å›¾ç‰‡ï¼Œè¿™äº›å›¾ç‰‡æ˜¯ä»åŒä¸€ä¸ªè§†é¢‘ä¸­æŒ‰æ—¶é—´é¡ºåºæå–çš„è¿ç»­å¸§ã€‚\n"
                        "è¯·åŠ¡å¿…ï¼š\n"
                        "1. å°†æ‰€æœ‰å›¾ç‰‡ä½œä¸ºä¸€ä¸ªå®Œæ•´çš„è§†é¢‘åºåˆ—æ¥åˆ†æ\n"
                        "2. åˆ†æè§†é¢‘ä¸­çš„åŠ¨ä½œã€åœºæ™¯å˜åŒ–ã€æ—¶é—´æµç¨‹\n"
                        "3. ä¸è¦åªæè¿°ç¬¬ä¸€å¼ å›¾ç‰‡ï¼Œè¦ç»¼åˆæ‰€æœ‰å¸§çš„ä¿¡æ¯\n"
                        "4. å¦‚æœçœ‹åˆ°åœºæ™¯æˆ–äººç‰©çš„å˜åŒ–ï¼Œè¯·æè¿°è¿™ä¸ªå˜åŒ–è¿‡ç¨‹\n\n"
                        "ç”¨æˆ·çš„é—®é¢˜ï¼š\n"
                    )
                    enhanced_prompt = video_prompt_prefix + prompt
                    content_parts = [{"text": enhanced_prompt}]
                    video_context_added = True
                    _log_info("ğŸ¬ è‡ªåŠ¨æ·»åŠ è§†é¢‘åˆ†æä¸Šä¸‹æ–‡åˆ°æç¤ºè¯")
                else:
                    content_parts = [{"text": prompt}]
            else:
                content_parts = [{"text": prompt}]

            # å¤„ç†å›¾åƒ
            if image is not None:
                if isinstance(image, torch.Tensor):
                    try:
                        _log_info(f"å¼€å§‹å¤„ç†è¾“å…¥å›¾åƒï¼ŒåŸå§‹å½¢çŠ¶: {image.shape}, æ•°æ®ç±»å‹: {image.dtype}")
                        
                        # æ£€æŸ¥å›¾åƒç»´åº¦
                        if image.dim() == 4:
                            image = image[0]  # å–ç¬¬ä¸€å¼ å›¾ç‰‡
                            _log_info(f"4Då›¾åƒï¼Œå–ç¬¬ä¸€å¼ ï¼Œæ–°å½¢çŠ¶: {image.shape}")
                        
                        # æ£€æŸ¥å›¾åƒå½¢çŠ¶
                        if image.dim() != 3:
                            _log_warning(f"å›¾åƒç»´åº¦ä¸æ­£ç¡®: {image.dim()}, æœŸæœ›3ç»´")
                            return (f"é”™è¯¯: å›¾åƒç»´åº¦ä¸æ­£ç¡®ï¼ŒæœŸæœ›3ç»´ï¼Œå®é™…{image.dim()}ç»´",)
                        
                        # æ£€æŸ¥å›¾åƒå°ºå¯¸å’Œé€šé“æ•°
                        if len(image.shape) == 3:
                            # åˆ¤æ–­æ˜¯CHWè¿˜æ˜¯HWCæ ¼å¼
                            if image.shape[0] in [1, 3, 4]:  # CHWæ ¼å¼
                                channels, height, width = image.shape
                                _log_info(f"CHWæ ¼å¼å›¾åƒ - é€šé“: {channels}, é«˜åº¦: {height}, å®½åº¦: {width}")
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯æ­£å¸¸çš„å›¾åƒæ ¼å¼
                                if channels not in [1, 3, 4]:
                                    _log_warning(f"é€šé“æ•°å¼‚å¸¸: {channels}, æœŸæœ›1,3,4")
                                    return (f"é”™è¯¯: é€šé“æ•°å¼‚å¸¸: {channels}, æœŸæœ›1,3,4",)
                                
                                if height < 10 or width < 10:
                                    _log_warning(f"å›¾åƒå°ºå¯¸å¤ªå°: {height}x{width}")
                                    return (f"é”™è¯¯: å›¾åƒå°ºå¯¸å¤ªå°: {height}x{width}",)
                                    
                            elif image.shape[2] in [1, 3, 4]:  # HWCæ ¼å¼
                                height, width, channels = image.shape
                                _log_info(f"HWCæ ¼å¼å›¾åƒ - é«˜åº¦: {height}, å®½åº¦: {width}, é€šé“: {channels}")
                                
                                # æ£€æŸ¥æ˜¯å¦æ˜¯æ­£å¸¸çš„å›¾åƒæ ¼å¼
                                if channels not in [1, 3, 4]:
                                    _log_warning(f"é€šé“æ•°å¼‚å¸¸: {channels}, æœŸæœ›1,3,4")
                                    return (f"é”™è¯¯: é€šé“æ•°å¼‚å¸¸: {channels}, æœŸæœ›1,3,4",)
                                
                                if height < 10 or width < 10:
                                    _log_warning(f"å›¾åƒå°ºå¯¸å¤ªå°: {height}x{width}")
                                    return (f"é”™è¯¯: å›¾åƒå°ºå¯¸å¤ªå°: {height}x{width}",)
                                    
                            else:
                                _log_warning(f"æ— æ³•è¯†åˆ«çš„å›¾åƒæ ¼å¼: {image.shape}")
                                return (f"é”™è¯¯: æ— æ³•è¯†åˆ«çš„å›¾åƒæ ¼å¼: {image.shape}",)
                        
                        # è½¬æ¢ä¸ºPILå›¾åƒ
                        try:
                            # ç¡®ä¿æ˜¯CHWæ ¼å¼
                            if image.shape[0] in [1, 3, 4]:  # é€šé“åœ¨ç¬¬ä¸€ä¸ªç»´åº¦
                                image_np = image.permute(1, 2, 0).cpu().numpy()
                            else:
                                # å¦‚æœå·²ç»æ˜¯HWCæ ¼å¼ï¼Œç›´æ¥è½¬æ¢
                                image_np = image.cpu().numpy()
                            
                            _log_info(f"è½¬æ¢ä¸ºnumpyæ•°ç»„ï¼Œå½¢çŠ¶: {image_np.shape}, æ•°æ®ç±»å‹: {image_np.dtype}")
                            
                            # æ£€æŸ¥æ•°æ®ç±»å‹å’ŒèŒƒå›´
                            if image_np.dtype != np.float32 and image_np.dtype != np.float64:
                                image_np = image_np.astype(np.float32)
                            
                            # ç¡®ä¿å€¼åœ¨0-1èŒƒå›´å†…
                            if image_np.max() > 1.0:
                                image_np = image_np / 255.0
                            
                            # è½¬æ¢ä¸ºuint8
                            image_np = (image_np * 255).astype(np.uint8)
                            
                            # å¤„ç†å•é€šé“å›¾åƒ
                            if image_np.shape[2] == 1:
                                image_np = np.repeat(image_np, 3, axis=2)
                                _log_info("å•é€šé“å›¾åƒè½¬æ¢ä¸ºRGB")
                            
                            # å¤„ç†RGBAå›¾åƒ
                            if image_np.shape[2] == 4:
                                image_np = image_np[:, :, :3]
                                _log_info("RGBAå›¾åƒè½¬æ¢ä¸ºRGB")
                            
                            pil_image = Image.fromarray(image_np)
                            _log_info(f"æˆåŠŸåˆ›å»ºPILå›¾åƒï¼Œå°ºå¯¸: {pil_image.size}, æ¨¡å¼: {pil_image.mode}")
                            
                        except Exception as permute_error:
                            _log_error(f"å›¾åƒè½¬æ¢å¤±è´¥: {permute_error}")
                            return (f"é”™è¯¯: å›¾åƒè½¬æ¢å¤±è´¥: {permute_error}",)
                        
                        # è½¬æ¢ä¸ºbase64
                        buffer = BytesIO()
                        pil_image.save(buffer, format='PNG')
                        img_base64 = base64.b64encode(buffer.getvalue()).decode()
                        content_parts.append({
                            "inline_data": {
                                "mime_type": "image/png",
                                "data": img_base64
                            }
                        })
                        _log_info(f"æˆåŠŸå¤„ç†è¾“å…¥å›¾åƒï¼Œæœ€ç»ˆå°ºå¯¸: {pil_image.size}")
                        
                    except Exception as e:
                        _log_error(f"å¤„ç†è¾“å…¥å›¾åƒæ—¶å‡ºé”™: {e}")
                        return (f"é”™è¯¯: å¤„ç†è¾“å…¥å›¾åƒå¤±è´¥: {e}",)
            
            # å¤„ç†éŸ³é¢‘
            if audio is not None:
                _log_info(f"æ£€æµ‹åˆ°éŸ³é¢‘è¾“å…¥ï¼Œç±»å‹: {type(audio)}")
                
                # ä¿å­˜åŸå§‹éŸ³é¢‘å­—å…¸ä»¥ä¾¿è·å–é‡‡æ ·ç‡
                original_audio_dict = None
                
                # å¤„ç†å­—å…¸æ ¼å¼çš„éŸ³é¢‘è¾“å…¥ï¼ˆComfyUIéŸ³é¢‘èŠ‚ç‚¹è¾“å‡ºï¼‰
                if isinstance(audio, dict):
                    _log_info(f"éŸ³é¢‘è¾“å…¥ä¸ºå­—å…¸æ ¼å¼ï¼Œé”®: {list(audio.keys())}")
                    original_audio_dict = audio  # ä¿å­˜åŸå§‹å­—å…¸
                    
                    # å°è¯•ä»å­—å…¸ä¸­æå–éŸ³é¢‘æ•°æ®
                    audio_tensor = None
                    if 'waveform' in audio:
                        audio_tensor = audio['waveform']
                        _log_info("ä» 'waveform' é”®æå–éŸ³é¢‘æ•°æ®")
                    elif 'samples' in audio:
                        audio_tensor = audio['samples']
                        _log_info("ä» 'samples' é”®æå–éŸ³é¢‘æ•°æ®")
                    elif 'audio' in audio:
                        audio_tensor = audio['audio']
                        _log_info("ä» 'audio' é”®æå–éŸ³é¢‘æ•°æ®")
                    elif 'data' in audio:
                        audio_tensor = audio['data']
                        _log_info("ä» 'data' é”®æå–éŸ³é¢‘æ•°æ®")
                    else:
                        _log_warning(f"æ— æ³•ä»éŸ³é¢‘å­—å…¸ä¸­æ‰¾åˆ°éŸ³é¢‘æ•°æ®ï¼Œå¯ç”¨é”®: {list(audio.keys())}")
                        return (f"é”™è¯¯: æ— æ³•ä»éŸ³é¢‘å­—å…¸ä¸­æ‰¾åˆ°éŸ³é¢‘æ•°æ®ï¼Œå¯ç”¨é”®: {list(audio.keys())}",)
                    
                    # å¦‚æœæå–çš„æ˜¯numpyæ•°ç»„ï¼Œè½¬æ¢ä¸ºtensor
                    if isinstance(audio_tensor, np.ndarray):
                        audio_tensor = torch.from_numpy(audio_tensor)
                        _log_info("å°†numpyæ•°ç»„è½¬æ¢ä¸ºtorch.Tensor")
                    
                    if not isinstance(audio_tensor, torch.Tensor):
                        _log_warning(f"æå–çš„éŸ³é¢‘æ•°æ®ç±»å‹ä¸æ­£ç¡®: {type(audio_tensor)}, æœŸæœ› torch.Tensor")
                        return (f"é”™è¯¯: æå–çš„éŸ³é¢‘æ•°æ®ç±»å‹ä¸æ­£ç¡®: {type(audio_tensor)}, æœŸæœ› torch.Tensor",)
                    
                    audio = audio_tensor  # æ›¿æ¢ä¸ºæå–çš„tensor
                
                # å¤„ç†torch.Tensoræ ¼å¼çš„éŸ³é¢‘
                if isinstance(audio, torch.Tensor):
                    try:
                        _log_info(f"å¼€å§‹å¤„ç†è¾“å…¥éŸ³é¢‘ï¼ŒåŸå§‹å½¢çŠ¶: {audio.shape}, æ•°æ®ç±»å‹: {audio.dtype}")
                        
                        # æ£€æŸ¥éŸ³é¢‘ç»´åº¦
                        if audio.dim() == 1:
                            # å•å£°é“éŸ³é¢‘
                            audio_np = audio.cpu().numpy()
                            _log_info(f"å•å£°é“éŸ³é¢‘ï¼Œé•¿åº¦: {len(audio_np)}")
                        elif audio.dim() == 2:
                            # ç«‹ä½“å£°éŸ³é¢‘ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
                            audio_np = audio[0].cpu().numpy()
                            _log_info(f"ç«‹ä½“å£°éŸ³é¢‘ï¼Œå–ç¬¬ä¸€é€šé“ï¼Œé•¿åº¦: {len(audio_np)}")
                        elif audio.dim() == 3:
                            # ComfyUI 3ç»´éŸ³é¢‘æ ¼å¼: [batch, channels, samples]
                            _log_info(f"3ç»´éŸ³é¢‘æ ¼å¼ï¼Œå½¢çŠ¶: {audio.shape}")
                            if audio.shape[0] == 1 and audio.shape[1] == 1:
                                # å•å£°é“éŸ³é¢‘ï¼Œå–ç¬¬ä¸€ä¸ªæ ·æœ¬
                                audio_np = audio[0, 0].cpu().numpy()
                                _log_info(f"æå–å•å£°é“éŸ³é¢‘ï¼Œé•¿åº¦: {len(audio_np)}")
                            elif audio.shape[0] == 1 and audio.shape[1] > 1:
                                # ç«‹ä½“å£°éŸ³é¢‘ï¼Œå–ç¬¬ä¸€ä¸ªé€šé“
                                audio_np = audio[0, 0].cpu().numpy()
                                _log_info(f"æå–ç«‹ä½“å£°ç¬¬ä¸€é€šé“ï¼Œé•¿åº¦: {len(audio_np)}")
                            else:
                                _log_warning(f"æ— æ³•å¤„ç†çš„3ç»´éŸ³é¢‘æ ¼å¼: {audio.shape}")
                                return (f"é”™è¯¯: æ— æ³•å¤„ç†çš„3ç»´éŸ³é¢‘æ ¼å¼: {audio.shape}",)
                        else:
                            _log_warning(f"éŸ³é¢‘ç»´åº¦ä¸æ­£ç¡®: {audio.dim()}, æœŸæœ›1ã€2æˆ–3ç»´")
                            return (f"é”™è¯¯: éŸ³é¢‘ç»´åº¦ä¸æ­£ç¡®ï¼ŒæœŸæœ›1ã€2æˆ–3ç»´ï¼Œå®é™…{audio.dim()}ç»´",)
                        
                        # æ£€æŸ¥éŸ³é¢‘é•¿åº¦
                        if len(audio_np) < 1000:
                            _log_warning(f"éŸ³é¢‘å¤ªçŸ­: {len(audio_np)} é‡‡æ ·ç‚¹")
                            return (f"é”™è¯¯: éŸ³é¢‘å¤ªçŸ­: {len(audio_np)} é‡‡æ ·ç‚¹",)
                        
                        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                        if audio_np.dtype != np.float32 and audio_np.dtype != np.float64:
                            audio_np = audio_np.astype(np.float32)
                        
                        # ç¡®ä¿å€¼åœ¨åˆç†èŒƒå›´å†…
                        if audio_np.max() > 1.0 or audio_np.min() < -1.0:
                            audio_np = np.clip(audio_np, -1.0, 1.0)
                            _log_info("éŸ³é¢‘å€¼å·²è£å‰ªåˆ° [-1, 1] èŒƒå›´")
                        
                        # è·å–é‡‡æ ·ç‡ï¼ˆå¦‚æœåŸå§‹éŸ³é¢‘å­—å…¸ä¸­æœ‰çš„è¯ï¼‰
                        sample_rate = 44100  # é»˜è®¤é‡‡æ ·ç‡
                        if original_audio_dict and 'sample_rate' in original_audio_dict:
                            sample_rate = original_audio_dict['sample_rate']
                            _log_info(f"ä½¿ç”¨åŸå§‹é‡‡æ ·ç‡: {sample_rate} Hz")
                        
                        # è½¬æ¢ä¸ºWAVå­—èŠ‚
                        buffer = BytesIO()
                        try:
                            import scipy.io.wavfile as wavfile
                            wavfile.write(buffer, sample_rate, audio_np)
                            audio_bytes = buffer.getvalue()
                            audio_base64 = base64.b64encode(audio_bytes).decode()
                            content_parts.append({
                                "inline_data": {
                                    "mime_type": "audio/wav",
                                    "data": audio_base64
                                }
                            })
                            _log_info(f"æˆåŠŸå¤„ç†è¾“å…¥éŸ³é¢‘ï¼Œé•¿åº¦: {len(audio_np)} é‡‡æ ·ç‚¹ï¼Œé‡‡æ ·ç‡: {sample_rate} Hz")
                        except ImportError:
                            _log_error("scipy åº“æœªå®‰è£…ï¼Œæ— æ³•å¤„ç†éŸ³é¢‘")
                            return (f"é”™è¯¯: scipy åº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install scipy",)
                        
                    except Exception as e:
                        _log_error(f"å¤„ç†è¾“å…¥éŸ³é¢‘æ—¶å‡ºé”™: {e}")
                        return (f"é”™è¯¯: å¤„ç†è¾“å…¥éŸ³é¢‘å¤±è´¥: {e}",)
                else:
                    _log_warning(f"éŸ³é¢‘è¾“å…¥ç±»å‹ä¸æ­£ç¡®: {type(audio)}, æœŸæœ› dict æˆ– torch.Tensor")
                    return (f"é”™è¯¯: éŸ³é¢‘è¾“å…¥ç±»å‹ä¸æ­£ç¡®: {type(audio)}, æœŸæœ› dict æˆ– torch.Tensor",)
            else:
                _log_info("æ²¡æœ‰éŸ³é¢‘è¾“å…¥")
            
            # ğŸ¬ å¤„ç†è§†é¢‘ - ä½¿ç”¨ File API
            uploaded_video_file = None
            temp_video_path = None

            if video is not None:
                try:
                    # æ£€æŸ¥è§†é¢‘ç±»å‹
                    if isinstance(video, str):
                        # Load_AF_Video è¿”å›è§†é¢‘æ–‡ä»¶è·¯å¾„ - ç›´æ¥ä½¿ç”¨
                        _log_info(f"ğŸ¬ æ£€æµ‹åˆ°è§†é¢‘æ–‡ä»¶è·¯å¾„: {video}")
                        temp_video_path = video
                        use_existing_file = True
                    elif isinstance(video, (dict, torch.Tensor)):
                        # éœ€è¦è½¬æ¢ä¸º MP4
                        video_frames = None

                        # æ”¯æŒå¤šç§ VIDEO ç±»å‹æ ¼å¼
                        if isinstance(video, dict):
                            if "video" in video:
                                video_frames = video["video"]
                            elif "frames" in video:
                                video_frames = video["frames"]
                            else:
                                for key, value in video.items():
                                    if isinstance(value, torch.Tensor):
                                        video_frames = value
                                        break
                        elif isinstance(video, torch.Tensor):
                            video_frames = video

                        if video_frames is not None and isinstance(video_frames, torch.Tensor):
                            _log_info(f"ğŸ¬ å¤„ç†è§†é¢‘ï¼Œå½¢çŠ¶: {video_frames.shape}, æ•°æ®ç±»å‹: {video_frames.dtype}")

                            if video_frames.dim() == 4:
                                frames_count = video_frames.shape[0]
                                _log_info(f"ğŸ“Š è§†é¢‘å¸§æ•°: {frames_count}")

                                # ä¿å­˜ä¸ºä¸´æ—¶ MP4 æ–‡ä»¶
                                temp_file = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False)
                                temp_video_path = temp_file.name
                                temp_file.close()

                                fps = 30
                                saved_path = save_video_tensor_to_mp4(video_frames, temp_video_path, fps=fps)

                                if not saved_path:
                                    _log_error("ä¿å­˜è§†é¢‘å¤±è´¥ï¼Œæ— æ³•ä½¿ç”¨ File API")
                                    temp_video_path = None
                                else:
                                    use_existing_file = False
                            else:
                                _log_warning(f"è§†é¢‘ç»´åº¦ä¸æ­£ç¡®: {video_frames.dim()}, æœŸæœ›4ç»´")
                                temp_video_path = None
                        else:
                            _log_warning("æ— æ³•ä» dict ä¸­æå–è§†é¢‘ tensor")
                            temp_video_path = None
                    else:
                        _log_warning(f"è§†é¢‘è¾“å…¥ç±»å‹ä¸æ”¯æŒ: {type(video)}")
                        temp_video_path = None

                    # å¦‚æœæœ‰è§†é¢‘æ–‡ä»¶ï¼Œä¸Šä¼ åˆ° File API
                    if temp_video_path:
                        _log_info("ğŸ¬ æ£€æµ‹åˆ°è§†é¢‘è¾“å…¥ï¼Œä½¿ç”¨ Gemini File API ä¸Šä¼ è§†é¢‘")

                        uploaded_video_file = upload_video_to_gemini_file_api(
                            temp_video_path,
                            final_api_key,
                            display_name="comfyui_video"
                        )

                        if not uploaded_video_file:
                            _log_error("è§†é¢‘ä¸Šä¼ å¤±è´¥ï¼Œå°†è·³è¿‡è§†é¢‘åˆ†æ")
                            temp_video_path = None

                except Exception as e:
                    _log_error(f"å¤„ç†è¾“å…¥è§†é¢‘æ—¶å‡ºé”™: {e}")
                    import traceback
                    traceback.print_exc()
                    temp_video_path = None
            
            # ğŸ¬ å¦‚æœæœ‰è§†é¢‘æ–‡ä»¶ï¼Œä½¿ç”¨ SDK è°ƒç”¨
            if uploaded_video_file and GENAI_SDK_AVAILABLE:
                _log_info(f"ğŸ” ä½¿ç”¨ Gemini SDK è¿›è¡Œè§†é¢‘åˆ†æ...")

                try:
                    # ä½¿ç”¨ SDK è°ƒç”¨
                    client_sdk = genai.Client(api_key=final_api_key)

                    # æ„å»ºå†…å®¹
                    sdk_contents = [
                        types.Part(text=prompt),
                        types.Part(
                            file_data=types.FileData(
                                file_uri=uploaded_video_file['file_uri'],
                                mime_type=uploaded_video_file['mime_type']
                            )
                        )
                    ]

                    # æ·»åŠ å…¶ä»–åª’ä½“ï¼ˆå¦‚æœæœ‰ï¼‰
                    for part in content_parts[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªæ–‡æœ¬éƒ¨åˆ†
                        if "inline_data" in part:
                            sdk_contents.append(
                                types.Part(
                                    inline_data=types.Blob(
                                        data=base64.b64decode(part["inline_data"]["data"]),
                                        mime_type=part["inline_data"]["mime_type"]
                                    )
                                )
                            )

                    # è°ƒç”¨ API
                    response = client_sdk.models.generate_content(
                        model=model,
                        contents=types.Content(parts=sdk_contents),
                        config=types.GenerateContentConfig(
                            temperature=temperature,
                            top_p=top_p,
                            top_k=top_k,
                            max_output_tokens=max_output_tokens
                        )
                    )

                    text = response.text

                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_video_path and not use_existing_file and os.path.exists(temp_video_path):
                        try:
                            os.unlink(temp_video_path)
                            _log_info("ğŸ—‘ï¸ ä¸´æ—¶è§†é¢‘æ–‡ä»¶å·²åˆ é™¤")
                        except:
                            pass

                    _log_info("âœ… è§†é¢‘åˆ†ææˆåŠŸå®Œæˆ")
                    return (text,)

                except Exception as sdk_error:
                    _log_error(f"SDK è°ƒç”¨å¤±è´¥: {sdk_error}")
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_video_path and not use_existing_file and os.path.exists(temp_video_path):
                        try:
                            os.unlink(temp_video_path)
                        except:
                            pass
                    raise

            # å¦‚æœæ²¡æœ‰è§†é¢‘æˆ– SDK ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹å¼
            _log_info(f"å‡†å¤‡å‘é€åˆ°APIçš„å†…å®¹éƒ¨åˆ†æ•°é‡: {len(content_parts)}")
            for i, part in enumerate(content_parts):
                if "text" in part:
                    _log_info(f"ç¬¬ {i+1} éƒ¨åˆ†: æ–‡æœ¬å†…å®¹ (é•¿åº¦: {len(part['text'])})")
                elif "inline_data" in part:
                    mime_type = part["inline_data"]["mime_type"]
                    data_length = len(part["inline_data"]["data"])
                    _log_info(f"ç¬¬ {i+1} éƒ¨åˆ†: {mime_type} æ•°æ® (é•¿åº¦: {data_length})")

            # æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©è°ƒç”¨æ–¹å¼
            has_audio = any("audio" in part.get("inline_data", {}).get("mime_type", "") for part in content_parts if "inline_data" in part)
            has_image = any("image" in part.get("inline_data", {}).get("mime_type", "") for part in content_parts if "inline_data" in part)

            if has_audio:
                _log_info("æ£€æµ‹åˆ°éŸ³é¢‘å†…å®¹ï¼Œä½¿ç”¨å¤šæ¨¡æ€è°ƒç”¨")
                response = client.models.generate_content(
                    model=model,
                    contents=[{"parts": content_parts}],
                    config=generation_config
                )
            elif has_image:
                _log_info("æ£€æµ‹åˆ°å›¾åƒå†…å®¹ï¼Œä½¿ç”¨å¤šæ¨¡æ€è°ƒç”¨")
                response = client.models.generate_content(
                    model=model,
                    contents=[{"parts": content_parts}],
                    config=generation_config
                )
            else:
                _log_info("ä½¿ç”¨æ ‡å‡†æ–‡æœ¬è°ƒç”¨")
                response = client.models.generate_content(
                    model=model,
                    contents=[{"parts": content_parts}],
                    config=generation_config
                )

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_video_path and not use_existing_file and os.path.exists(temp_video_path):
                try:
                    os.unlink(temp_video_path)
                    _log_info("ğŸ—‘ï¸ ä¸´æ—¶è§†é¢‘æ–‡ä»¶å·²åˆ é™¤")
                except:
                    pass
            
            if response.candidates and response.candidates[0].content.parts:
                text = response.candidates[0].content.parts[0].text
                _log_info(f"KenChen LLM Geminiå¤šæ¨¡æ€ç”ŸæˆæˆåŠŸï¼Œæ¨¡å‹: {model}")
                return (text,)
            else:
                return (f"é”™è¯¯: æœªè·å¾—æœ‰æ•ˆå“åº”",)
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg or "quota" in error_msg.lower():
                _log_error(f"APIé…é¢é™åˆ¶: {error_msg}")
                return (f"é”™è¯¯: APIé…é¢é™åˆ¶ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š\n1. ä½¿ç”¨å…è´¹æ¨¡å‹ (å¦‚ gemini-1.5-flash)\n2. å‡çº§åˆ°ä»˜è´¹è´¦æˆ·\n3. ç­‰å¾…é…é¢é‡ç½®\n\nè¯¦ç»†é”™è¯¯: {error_msg}",)
            else:
                error_msg = f"KenChen LLM Geminiå¤šæ¨¡æ€ç”Ÿæˆå¤±è´¥: {e}"
                _log_error(error_msg)
                return (error_msg,)

class KenChenLLMGeminiImageGenerationNode:
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_config()
        default_params = config.get('default_params', {})
        
        # å›¾åƒç”Ÿæˆä¸“ç”¨æ¨¡å‹
        default_models = [
            "gemini-2.0-flash-preview-image-generation",
            "gemini-2.0-flash-exp-image-generation",
        ]
        
        models = config.get('models', {}).get('image_gen_models', default_models)
        if not models:
            models = default_models
        
        default_model = "gemini-2.0-flash-preview-image-generation"
        default_proxy = "http://127.0.0.1:None"
        
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "", "multiline": True}),
                "model": (models, {"default": default_model}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "temperature": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": 40, "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 0, "max": 32768}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                "image": ("IMAGE",),
            },
        }
    RETURN_TYPES = ("STRING", "IMAGE",)
    RETURN_NAMES = ("generation_text", "generated_image",)
    FUNCTION = "generate_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    
    def generate_image(self, api_key, prompt, model, proxy, temperature, top_p, top_k, max_output_tokens, seed, image=None):
        try:
            genai = get_google_genai()
            if genai is None:
                dummy_image = torch.zeros(1, 512, 512, 3, dtype=torch.float32)
                return (f"é”™è¯¯: æœªå®‰è£…google-genaiåº“ï¼Œè¯·è¿è¡Œ: pip install google-genai", dummy_image)
            
            config = get_gemini_config()
            final_api_key = api_key.strip() if api_key.strip() else config.get('api_key', '')
            if not final_api_key:
                dummy_image = torch.zeros(1, 512, 512, 3, dtype=torch.float32)
                return (f"é”™è¯¯: è¯·æä¾›Gemini API Key", dummy_image)
            
            client = genai.Client(api_key=final_api_key)
            
            generation_config = {
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "max_output_tokens": max_output_tokens,
                "response_modalities": ["TEXT", "IMAGE"],  # å›¾åƒç”Ÿæˆå¿…é¡»æŒ‡å®š
            }
            if seed > 0:
                generation_config["seed"] = seed
            
            content_parts = [{"text": prompt}]
            
            # å¤„ç†è¾“å…¥å›¾åƒï¼ˆå›¾ç”Ÿå›¾åŠŸèƒ½ï¼‰
            if image is not None:
                if isinstance(image, torch.Tensor):
                    if image.dim() == 4:
                        image = image[0]
                    
                    if image.shape[0] in [1, 3, 4]:
                        image_np = image.permute(1, 2, 0).cpu().numpy()
                    else:
                        image_np = image.cpu().numpy()
                    
                    if image_np.dtype != np.float32:
                        image_np = image_np.astype(np.float32)
                    
                    if image_np.max() > 1.0:
                        image_np = image_np / 255.0
                    
                    image_np = (image_np * 255).astype(np.uint8)
                    
                    if image_np.shape[2] == 1:
                        image_np = np.repeat(image_np, 3, axis=2)
                    elif image_np.shape[2] == 4:
                        image_np = image_np[:, :, :3]
                    
                    pil_image = Image.fromarray(image_np)
                    buffer = BytesIO()
                    pil_image.save(buffer, format='PNG')
                    img_base64 = base64.b64encode(buffer.getvalue()).decode()
                    content_parts.append({
                        "inline_data": {
                            "mime_type": "image/png",
                            "data": img_base64
                        }
                    })
            
            response = client.models.generate_content(
                model=model,
                contents=[{"parts": content_parts}],
                config=generation_config
            )
            
            if response.candidates and response.candidates[0].content.parts:
                text = response.candidates[0].content.parts[0].text if response.candidates[0].content.parts[0].text else "å›¾åƒç”Ÿæˆå®Œæˆ"
                
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'inline_data') and part.inline_data and part.inline_data.mime_type.startswith('image/'):
                        try:
                            if isinstance(part.inline_data.data, bytes):
                                img_data = part.inline_data.data
                            else:
                                img_data = base64.b64decode(part.inline_data.data)
                            
                            pil_image = Image.open(BytesIO(img_data))
                            img_array = np.array(pil_image)
                            
                            if len(img_array.shape) == 3 and img_array.shape[2] == 4:
                                img_array = img_array[:, :, :3]
                            
                            img_tensor = torch.from_numpy(img_array).float() / 255.0
                            if len(img_tensor.shape) == 3 and img_tensor.shape[2] == 3:
                                img_tensor = img_tensor.unsqueeze(0)
                            else:
                                img_tensor = img_tensor.permute(1, 2, 0).unsqueeze(0)
                            
                            return (text, img_tensor)
                        except Exception as img_error:
                            _log_error(f"å¤„ç†ç”Ÿæˆçš„å›¾åƒæ—¶å‡ºé”™: {img_error}")
                            continue
                
                dummy_image = torch.zeros(1, 512, 512, 3, dtype=torch.float32)
                return (text, dummy_image)
            
            dummy_image = torch.zeros(1, 512, 512, 3, dtype=torch.float32)
            return (f"é”™è¯¯: æœªç”Ÿæˆæœ‰æ•ˆå›¾åƒ", dummy_image)
            
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                dummy_image = torch.zeros(1, 512, 512, 3, dtype=torch.float32)
                return (f"é”™è¯¯: APIé…é¢é™åˆ¶: {error_msg}", dummy_image)
            else:
                dummy_image = torch.zeros(1, 512, 512, 3, dtype=torch.float32)
                return (f"é”™è¯¯: {error_msg}", dummy_image)

class KenChenLLMGeminiImageAnalysisNode:
    @classmethod
    def INPUT_TYPES(s):
        config = get_gemini_config()
        default_params = config.get('default_params', {})
        
        # å›¾åƒåˆ†æä¸“ç”¨æ¨¡å‹
        default_models = [
            "gemini-1.5-flash",
            "gemini-2.0-flash",
            "gemini-1.5-pro",
        ]
        
        models = config.get('models', {}).get('multimodal_models', default_models)
        if not models:
            models = default_models
        
        default_model = "gemini-1.5-flash"
        default_proxy = "http://127.0.0.1:None"
        
        return {
            "required": {
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "prompt": ("STRING", {"default": "è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡çš„å†…å®¹", "multiline": True}),
                "model": (models, {"default": default_model}),
                "proxy": ("STRING", {"default": default_proxy, "multiline": False}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 1.5}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": 40, "min": 0, "max": 100}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 0, "max": 8192}),
                "seed": ("INT", {"default": 0, "min": 0, "max": 0xfffffff}),
            },
            "optional": {
                "image": ("IMAGE",),
            },
        }
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("analysis_text",)
    FUNCTION = "analyze_image"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"
    
    def analyze_image(self, api_key, prompt, model, proxy, temperature, top_p, top_k, max_output_tokens, seed, image=None):
        try:
            genai = get_google_genai()
            if genai is None:
                return (f"é”™è¯¯: æœªå®‰è£…google-genaiåº“ï¼Œè¯·è¿è¡Œ: pip install google-genai",)
            
            config = get_gemini_config()
            final_api_key = api_key.strip() if api_key.strip() else config.get('api_key', '')
            if not final_api_key:
                return (f"é”™è¯¯: è¯·æä¾›Gemini API Key",)
            
            if image is None:
                return (f"é”™è¯¯: è¯·æä¾›è¦åˆ†æçš„å›¾åƒ",)
            
            client = genai.Client(api_key=final_api_key)
            
            generation_config = {
                "temperature": temperature,
                "top_p": top_p,
                "top_k": top_k,
                "max_output_tokens": max_output_tokens,
            }
            if seed > 0:
                generation_config["seed"] = seed
            
            content_parts = [{"text": prompt}]
            
            # å¤„ç†è¾“å…¥å›¾åƒ
            if isinstance(image, torch.Tensor):
                if image.dim() == 4:
                    image = image[0]
                
                if image.shape[0] in [1, 3, 4]:
                    image_np = image.permute(1, 2, 0).cpu().numpy()
                else:
                    image_np = image.cpu().numpy()
                
                if image_np.dtype != np.float32:
                    image_np = image_np.astype(np.float32)
                
                if image_np.max() > 1.0:
                    image_np = image_np / 255.0
                
                image_np = (image_np * 255).astype(np.uint8)
                
                if image_np.shape[2] == 1:
                    image_np = np.repeat(image_np, 3, axis=2)
                elif image_np.shape[2] == 4:
                    image_np = image_np[:, :, :3]
                
                pil_image = Image.fromarray(image_np)
                buffer = BytesIO()
                pil_image.save(buffer, format='PNG')
                img_base64 = base64.b64encode(buffer.getvalue()).decode()
                content_parts.append({
                    "inline_data": {
                        "mime_type": "image/png",
                        "data": img_base64
                    }
                })
            
            response = client.models.generate_content(
                model=model,
                contents=[{"parts": content_parts}],
                config=generation_config
            )
            
            if response.candidates and response.candidates[0].content.parts:
                text = response.candidates[0].content.parts[0].text
                _log_info(f"KenChen LLM Geminiå›¾åƒåˆ†ææˆåŠŸï¼Œæ¨¡å‹: {model}")
                return (text,)
            else:
                return (f"é”™è¯¯: æœªè·å¾—æœ‰æ•ˆå“åº”",)
                
        except Exception as e:
            error_msg = str(e)
            if "429" in error_msg or "RESOURCE_EXHAUSTED" in error_msg:
                _log_error(f"APIé…é¢é™åˆ¶: {error_msg}")
                return (f"é”™è¯¯: APIé…é¢é™åˆ¶: {error_msg}",)
            else:
                error_msg = f"KenChen LLM Geminiå›¾åƒåˆ†æå¤±è´¥: {e}"
                _log_error(error_msg)
                return (error_msg,)

# èŠ‚ç‚¹æ˜ å°„
NODE_CLASS_MAPPINGS = {
    "KenChenLLMPromptGeminiText": KenChenLLMGeminiTextNode,
    "KenChenLLMPromptGeminiMultimodal": KenChenLLMGeminiMultimodalNode,
    "KenChenLLMPromptGeminiImageGeneration": KenChenLLMGeminiImageGenerationNode,
    "KenChenLLMPromptGeminiImageAnalysis": KenChenLLMGeminiImageAnalysisNode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "KenChenLLMPromptGeminiText": "Gemini-Text",
    "KenChenLLMPromptGeminiMultimodal": "Gemini-Multimodal",
    "KenChenLLMPromptGeminiImageGeneration": "Gemini-Image-Generation",
    "KenChenLLMPromptGeminiImageAnalysis": "Gemini-Image-Analysis",
}
