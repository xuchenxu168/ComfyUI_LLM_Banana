import os, json, base64, requests
import torch
import numpy as np
from PIL import Image
from io import BytesIO
import re

def _log(message):
    print(f"[NanoBanana-GeneralAPI] {message}")

# Nano Banana - General REST API (Gemini-compatible) node
# Goal: user provides api_key + base_url (+ model, version, auth_mode)
# Then call :generateContent and extract returned image automatically

def _b64_png_from_tensor(img: torch.Tensor) -> str:
    # Backward-compat helper. Always encodes PNG.
    return _b64_from_tensor(img, "image/png")


def _b64_from_tensor(img: torch.Tensor, mime: str = "image/png") -> str:
    if img is None:
        return None
    if isinstance(img, torch.Tensor):
        if img.dim() == 4:
            img = img[0]
        if img.shape[0] in [1, 3, 4]:
            img_np = img.permute(1, 2, 0).cpu().numpy()
        else:
            img_np = img.cpu().numpy()
        if img_np.dtype != np.float32:
            img_np = img_np.astype(np.float32)
        if img_np.max() <= 1.0:
            img_np = (img_np * 255.0).astype(np.uint8)
        if img_np.shape[2] == 1:
            img_np = np.repeat(img_np, 3, axis=2)
        if img_np.shape[2] == 4:
            img_np = img_np[:, :, :3]
        pil = Image.fromarray(img_np)
        buf = BytesIO()
        fmt = 'PNG'
        if mime == 'image/jpeg':
            fmt = 'JPEG'
        elif mime == 'image/webp':
            fmt = 'WEBP'
        pil.save(buf, format=fmt)
        return base64.b64encode(buf.getvalue()).decode()
    return None

def _auto_auth_headers(base_url: str, api_key: str, auth_mode: str):
    headers = {"Content-Type": "application/json"}
    mode = (auth_mode or "auto").lower()
    if mode == "google_xgoog" or (mode == "auto" and "generativelanguage.googleapis.com" in (base_url or "")):
        headers["x-goog-api-key"] = api_key
    else:
        headers["Authorization"] = f"Bearer {api_key}"
    return headers

def _build_endpoint(base_url: str, model: str, version: str):
    u = (base_url or "").rstrip('/')
    if "/models/" in u and ":generateContent" in u:
        return u

    # Check if base_url already contains a version path
    if u.endswith('/v1') or u.endswith('/v1beta') or u.endswith('/v1alpha'):
        return f"{u}/models/{model}:generateContent"

    ver = (version or "Auto").lower()
    if ver == "auto":
        ver = "v1beta" if "generativelanguage.googleapis.com" in u else "v1"

    return f"{u}/{ver}/models/{model}:generateContent"


def _deep_merge(dst: dict, src: dict):
    for k, v in (src or {}).items():
        if isinstance(v, dict) and isinstance(dst.get(k), dict):
            _deep_merge(dst[k], v)
        else:
            dst[k] = v
    return dst



# Redact big/base64 strings for logging to avoid noisy output
def _redact_for_log(obj, max_len=256):
    def is_base64_like(s: str) -> bool:
        try:
            return bool(re.fullmatch(r"[A-Za-z0-9+/=\n\r]+", s))
        except Exception:
            return False

    def walk(v):
        if isinstance(v, dict):
            out = {}
            for k, val in v.items():
                if k == "data" and isinstance(val, str) and len(val) > max_len:
                    out[k] = f"[redacted {len(val)} chars]"
                else:
                    out[k] = walk(val)
            return out
        if isinstance(v, list):
            return [walk(x) for x in v]
        if isinstance(v, str):
            if len(v) > max_len and is_base64_like(v):
                return f"[redacted {len(v)} chars]"
            if len(v) > 4096:
                return v[:1024] + f"... [truncated, total {len(v)} chars]"
            return v
        return v

    try:
        return walk(obj)
    except Exception:
        return obj

def _download_image(url: str, proxies=None, timeout=120):
    try:
        _log(f"Downloading image: {url}")
        r = requests.get(url, timeout=timeout, proxies=proxies)
        if r.status_code == 200:
            return r.content
        _log(f"Download failed: HTTP {r.status_code}")
    except Exception as e:
        _log(f"Error downloading image: {e}")
    return None

def _extract_first_image(resp_json, strict_native=False, proxies=None, timeout=120):
    # 1) Gemini style: candidates -> content.parts.inlineData/inline_data.image/*
    try:
        cands = resp_json.get("candidates") or []
        for cand in cands:
            parts = (cand.get("content") or {}).get("parts") or []
            for p in parts:
                # å°è¯•é©¼å³°å‘½åå’Œä¸‹åˆ’çº¿å‘½å
                data = p.get("inlineData") or p.get("inline_data") or {}
                mt = (data.get("mimeType") or data.get("mime_type") or "")
                if mt.startswith("image/"):
                    b64 = data.get("data")
                    if b64:
                        _log(f"Found inline image: mime={mt}, data_length={len(b64)}")
                        return base64.b64decode(b64)

                # æ£€æŸ¥æ˜¯å¦æœ‰ Markdown æ ¼å¼çš„å›¾åƒé“¾æ¥ï¼ˆéä¸¥æ ¼åŸç”Ÿæ¨¡å¼ï¼‰
                if not strict_native:
                    text = p.get("text", "")
                    if text:
                        _log(f"ğŸ” æ£€æŸ¥æ–‡æœ¬ä¸­çš„å›¾åƒURL: {text[:200]}")
                        # åŒ¹é… ![...](url) æˆ–ç›´æ¥çš„ http(s):// é“¾æ¥
                        md_match = re.search(r'!\[.*?\]\((https?://[^\)]+)\)', text)
                        if md_match:
                            url = md_match.group(1)
                            _log(f"âœ… ä»Markdownæ ¼å¼æå–åˆ°å›¾åƒURL: {url}")
                            img_data = _download_image(url, proxies=proxies, timeout=timeout)
                            if img_data:
                                _log(f"âœ… æˆåŠŸä¸‹è½½å›¾åƒï¼Œå¤§å°: {len(img_data)} bytes")
                                return img_data
                            else:
                                _log(f"âŒ ä¸‹è½½å›¾åƒå¤±è´¥: {url}")

                        # å°è¯•åŒ¹é…çº¯ URLï¼ˆæ‰©å±•æ”¯æŒæ›´å¤šæ ¼å¼ï¼‰
                        url_match = re.search(r'(https?://[^\s\)]+\.(?:png|jpg|jpeg|gif|webp|bmp))', text, re.IGNORECASE)
                        if url_match:
                            url = url_match.group(1)
                            _log(f"âœ… ä»æ–‡æœ¬ä¸­æå–åˆ°å›¾åƒURL: {url}")
                            img_data = _download_image(url, proxies=proxies, timeout=timeout)
                            if img_data:
                                _log(f"âœ… æˆåŠŸä¸‹è½½å›¾åƒï¼Œå¤§å°: {len(img_data)} bytes")
                                return img_data
                            else:
                                _log(f"âŒ ä¸‹è½½å›¾åƒå¤±è´¥: {url}")
    except Exception as e:
        _log(f"Error in Gemini-style image extraction: {e}")
        import traceback
        _log(traceback.format_exc())
        pass

    # 2) OpenAI/DALLÂ·E style
    try:
        d = resp_json.get("data")
        if isinstance(d, list) and d:
            b64 = d[0].get("b64_json")
            if b64:
                return base64.b64decode(b64)
    except Exception as e:
        _log(f"Error in OpenAI-style image extraction: {e}")
        pass

    # 3) Generic fallbacks
    try:
        for k in ["image", "images"]:
            v = resp_json.get(k)
            if isinstance(v, list) and v:
                b64 = v[0].get("base64") or v[0].get("b64")
                if b64:
                    return base64.b64decode(b64)
    except Exception as e:
        _log(f"Error in fallback image extraction: {e}")
        pass

    return None

class NanoBananaGeneralAPINode:
    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": ("STRING", {"default": "ç”Ÿæˆä¸€å¼ æ¸…æ™°çš„é¦™æ°´äº§å“å›¾", "multiline": True}),
                "api_key": ("STRING", {"default": "", "multiline": False}),
                "base_url": ("STRING", {"default": "https://generativelanguage.googleapis.com"}),
                "model": ("STRING", {"default": "gemini-3-pro-image-preview"}),
                "version": (["Auto", "v1", "v1alpha", "v1beta"], {"default": "Auto"}),
                "auth_mode": (["auto", "google_xgoog", "bearer"], {"default": "auto"}),
                "response_mode": (["TEXT_AND_IMAGE", "IMAGE_ONLY", "TEXT_ONLY"], {"default": "TEXT_AND_IMAGE"}),
                "aspect_ratio": (["Auto","1:1","16:9","9:16","4:3","3:4","3:2","2:3","5:4","4:5","21:9"], {"default": "Auto"}),
                "image_size": (["Auto","1K","2K","4K"], {"default": "Auto"}),

                # ğŸ” Topaz Gigapixel AIæ”¾å¤§æ§åˆ¶
                "upscale_factor": (["1x (ä¸æ”¾å¤§)", "2x", "4x", "6x"], {
                    "default": "1x (ä¸æ”¾å¤§)",
                    "tooltip": "ä½¿ç”¨Topaz Gigapixel AIè¿›è¡Œæ™ºèƒ½æ”¾å¤§"
                }),
                "gigapixel_model": (["High Fidelity", "Standard", "Art & CG", "Lines", "Very Compressed", "Low Resolution", "Text & Shapes", "Redefine", "Recover"], {
                    "default": "High Fidelity",
                    "tooltip": "Gigapixel AIæ”¾å¤§æ¨¡å‹"
                }),

                # æŒ‰é¡ºåºï¼štemperature -> top_p -> top_k -> max_output_tokens
                "temperature": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 2.0}),
                "top_p": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0}),
                "top_k": ("INT", {"default": 40, "min": 1, "max": 1000}),
                "max_output_tokens": ("INT", {"default": 2048, "min": 1, "max": 32768}),

                "seed": ("INT", {"default": 0, "min": 0, "max": 2147483647}),
                "strict_native": ("BOOLEAN", {"default": False}),
                "system_instruction": ("STRING", {"default": "", "multiline": True}),
                "image_mime": (["image/png","image/jpeg","image/webp"], {"default": "image/png"}),
                "timeout": ("INT", {"default": 300, "min": 30, "max": 600, "tooltip": "APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤300ç§’"}),

                # ğŸŒ ä»£ç†è®¾ç½®
                "use_system_proxy": ("BOOLEAN", {"default": True, "tooltip": "True=ä½¿ç”¨ç³»ç»Ÿä»£ç†, False=ç¦ç”¨ä»£ç†"}),
            },
            "optional": {
                "image": ("IMAGE",),
                "image2": ("IMAGE",),
                "image3": ("IMAGE",),
                "image4": ("IMAGE",),
                "extra_payload_json": ("STRING", {"default": "", "multiline": True}),
            },
        }

    RETURN_TYPES = ("STRING", "IMAGE")
    RETURN_NAMES = ("text", "image")
    FUNCTION = "call_api"
    CATEGORY = "Ken-Chen/LLM-Nano-Banana"

    def call_api(self, prompt, api_key, base_url, model, version, auth_mode,
                 response_mode, aspect_ratio, image_size, upscale_factor, gigapixel_model,
                 temperature, top_p, top_k, max_output_tokens, seed, strict_native,
                 system_instruction, image_mime, timeout, use_system_proxy,
                 image=None, image2=None, image3=None, image4=None, extra_payload_json=""):
        if not (api_key or "").strip():
            return ("é”™è¯¯: è¯·æä¾› API Key", torch.zeros(1, 512, 512, 3))
        endpoint = _build_endpoint(base_url, model, version)
        headers = _auto_auth_headers(base_url, api_key.strip(), auth_mode)

        # Build parts: prompt then up to 4 images
        parts = [{"text": prompt}]
        for img_tensor in [image, image2, image3, image4]:
            b64_img = _b64_from_tensor(img_tensor, image_mime or "image/png")
            if b64_img:
                # ä½¿ç”¨é©¼å³°å‘½åä»¥å…¼å®¹åŸç”Ÿ Gemini
                parts.append({"inlineData": {"mimeType": image_mime or "image/png", "data": b64_img}})

        # Base payload per Gemini docs
        payload = {
            "contents": [{"role": "user", "parts": parts}],
            "generationConfig": {
                "temperature": float(temperature),
                "topP": float(top_p),
                "topK": int(top_k),
                "maxOutputTokens": int(max_output_tokens),
            },
        }

        # responseModalities
        if response_mode == "IMAGE_ONLY":
            mods = ["IMAGE"]
        elif response_mode == "TEXT_ONLY":
            mods = ["TEXT"]
        else:
            mods = ["TEXT", "IMAGE"]
        # è®¾ç½®åœ¨ generationConfig ä¸‹ï¼ˆå®˜æ–¹æ ‡å‡†ä½ç½®ï¼‰
        payload.setdefault("generationConfig", {})["responseModalities"] = mods

        # imageConfig: aspectRatio + imageSize (1K/2K/4K)
        # åªåœ¨ generationConfig ä¸‹è®¾ç½®ï¼ˆå®˜æ–¹æ ‡å‡†ä½ç½®ï¼‰
        gen_cfg = payload.setdefault("generationConfig", {})

        if aspect_ratio and aspect_ratio != "Auto":
            gen_cfg.setdefault("imageConfig", {})["aspectRatio"] = aspect_ratio
        if image_size and image_size != "Auto":
            val = str(image_size).upper()
            gen_cfg.setdefault("imageConfig", {})["imageSize"] = val

        # seed (0 means no seed)
        try:
            if isinstance(seed, int) and seed > 0:
                payload.setdefault("generationConfig", {})["seed"] = int(seed)
        except Exception:
            pass

        # systemInstructionï¼ˆå®˜æ–¹æ ‡å‡†ä½ç½®ï¼šé¡¶å±‚ï¼‰
        if system_instruction and system_instruction.strip():
            payload["systemInstruction"] = {
                "role": "system",
                "parts": [{"text": system_instruction.strip()}]
            }

        # Merge extra JSON (allows official fields like safetySettings, tools, toolConfig, responseSchema, clientContext, etc.)
        if extra_payload_json and extra_payload_json.strip():
            try:
                user_extra = json.loads(extra_payload_json)
                payload = _deep_merge(payload, user_extra)
            except Exception as e:
                _log(f"extra_payload_json parse error: {e}")

        try:
            _log(f"Request URL: {endpoint}")
            logged_headers = headers.copy()
            if "Authorization" in logged_headers:
                logged_headers["Authorization"] = "Bearer sk-..."
            if "x-goog-api-key" in logged_headers:
                logged_headers["x-goog-api-key"] = "AIzaSy..."
            _log(f"Request Headers: {logged_headers}")
            _log(f"Request Payload: {json.dumps(_redact_for_log(payload), ensure_ascii=False, indent=2)}")

            # ğŸŒ é…ç½®ä»£ç†
            # use_system_proxy=True: ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼ˆrequestsé»˜è®¤è¡Œä¸ºï¼‰
            # use_system_proxy=False: æ˜¾å¼ç¦ç”¨ä»£ç†
            import os
            proxies = None if use_system_proxy else {'http': None, 'https': None}

            if not use_system_proxy:
                _log("ğŸš« Proxy disabled (use_system_proxy=False) - Direct connection")
                _log(f"   Target: {endpoint}")
            else:
                _log("ğŸŒ Using system proxy settings (use_system_proxy=True)")
                # æ˜¾ç¤ºç³»ç»Ÿä»£ç†ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœæœ‰ï¼‰
                http_proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
                https_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
                if http_proxy or https_proxy:
                    _log(f"   HTTP_PROXY: {http_proxy or 'Not set'}")
                    _log(f"   HTTPS_PROXY: {https_proxy or 'Not set'}")
                else:
                    _log("   No system proxy environment variables detected")

            resp = requests.post(endpoint, headers=headers, data=json.dumps(payload), timeout=timeout, proxies=proxies)

            _log(f"Response Status Code: {resp.status_code}")
            resp_json = resp.json() if resp.status_code == 200 else None
            _log(f"Response Body: {json.dumps(_redact_for_log(resp_json or resp.text), ensure_ascii=False, indent=2)}")

            if resp.status_code != 200:
                return (f"HTTP {resp.status_code}: {resp.text}", torch.zeros(1, 512, 512, 3))

            data = resp_json
            img_bytes = _extract_first_image(data, strict_native=strict_native, proxies=proxies, timeout=timeout)
            text = ""
            try:
                # Primary: candidates[].content.parts[].text
                if 'candidates' in data and data['candidates']:
                    for candidate in data['candidates']:
                        if 'content' in candidate and 'parts' in candidate['content']:
                            for part in candidate['content']['parts']:
                                if 'text' in part:
                                    text += part['text']
                # Fallback: response.parts (some proxies/SDKs flatten)
                if not text and 'parts' in data:
                    for part in data.get('parts') or []:
                        if 'text' in part:
                            text += part['text']
            except Exception as e:
                _log(f"Error extracting text: {e}")
                pass

            if img_bytes:
                _log("æˆåŠŸä»å“åº”ä¸­æå–å›¾åƒæ•°æ®ã€‚")
                try:
                    pil = Image.open(BytesIO(img_bytes))
                    _log(f"Decoded image mode={pil.mode} size={pil.size}")
                    pil = pil.convert("RGB")
                except Exception as e:
                    _log(f"PIL open/convert failed: {e}")
                    pil = Image.open(BytesIO(img_bytes)).convert("RGB")

                # ğŸ” Topaz Gigapixel AIæ™ºèƒ½æ”¾å¤§
                if upscale_factor and upscale_factor != "1x (ä¸æ”¾å¤§)":
                    try:
                        # æå–æ”¾å¤§å€æ•°
                        scale = int(upscale_factor.replace("x", "").strip().split()[0])
                        if scale > 1:
                            _log(f"ğŸ” ä½¿ç”¨æ™ºèƒ½AIæ”¾å¤§è¿›è¡Œ{scale}xæ”¾å¤§ï¼Œæ¨¡å‹: {gigapixel_model}")

                            # å¯¼å…¥æ”¾å¤§å‡½æ•°
                            try:
                                from .banana_upscale import smart_upscale
                            except ImportError:
                                from banana_upscale import smart_upscale

                            # è®¡ç®—ç›®æ ‡å°ºå¯¸
                            target_w = pil.width * scale
                            target_h = pil.height * scale

                            # ä½¿ç”¨æ™ºèƒ½æ”¾å¤§
                            upscaled_image = smart_upscale(
                                pil,
                                target_w,
                                target_h,
                                gigapixel_model
                            )

                            if upscaled_image:
                                pil = upscaled_image
                                _log(f"âœ… æ™ºèƒ½AIæ”¾å¤§å®Œæˆ: {pil.size}")
                            else:
                                _log("âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")
                    except Exception as e:
                        _log(f"âš ï¸ æ™ºèƒ½AIæ”¾å¤§å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å›¾åƒ")

                arr = np.array(pil)
                img_t = torch.from_numpy(arr).float() / 255.0
                return (text or "(å›¾åƒå·²ç”Ÿæˆ)", img_t.unsqueeze(0))

            _log("è­¦å‘Š: æœªèƒ½ä»å“åº”ä¸­æå–å›¾åƒæ•°æ®ã€‚")

            # ğŸ” è¯Šæ–­ç©ºå“åº”
            error_details = []
            if data.get('candidates'):
                candidate = data['candidates'][0]
                parts = candidate.get('content', {}).get('parts', [])
                finish_reason = candidate.get('finishReason')
                candidates_tokens = data.get('usageMetadata', {}).get('candidatesTokenCount', 0)

                if not parts or len(parts) == 0:
                    error_details.append("=" * 60)
                    error_details.append("âŒ APIè°ƒç”¨æˆåŠŸä½†æœªè¿”å›ä»»ä½•å†…å®¹")
                    error_details.append("=" * 60)
                    error_details.append(f"ğŸ“Š å“åº”çŠ¶æ€:")
                    error_details.append(f"   â€¢ finishReason: {finish_reason}")
                    error_details.append(f"   â€¢ candidatesTokenCount: {candidates_tokens}")
                    error_details.append(f"   â€¢ parts: [] (ç©ºæ•°ç»„)")

                    # æ£€æŸ¥è¯·æ±‚é…ç½®
                    error_details.append(f"\nğŸ”§ å½“å‰é…ç½®:")
                    error_details.append(f"   â€¢ æ¨¡å‹: {model}")
                    error_details.append(f"   â€¢ APIåœ°å€: {base_url}")
                    error_details.append(f"   â€¢ responseModalities: {mods}")
                    error_details.append(f"   â€¢ response_mode: {response_mode}")

                    if gen_cfg.get('imageConfig'):
                        error_details.append(f"   â€¢ imageConfig: {json.dumps(gen_cfg['imageConfig'])}")

                    error_details.append(f"\nğŸ“ Prompt (å‰100å­—ç¬¦):")
                    error_details.append(f"   {prompt[:100]}...")

                    # æ ¹æ®ä¸åŒæƒ…å†µç»™å‡ºå»ºè®®
                    error_details.append(f"\n" + "=" * 60)
                    error_details.append("ğŸ’¡ å¯èƒ½çš„åŸå› å’Œè§£å†³æ–¹æ¡ˆ:")
                    error_details.append("=" * 60)

                    if candidates_tokens == 0:
                        error_details.append("\n1ï¸âƒ£ æ¨¡å‹ä¸æ”¯æŒå›¾åƒç”Ÿæˆ")
                        error_details.append(f"   å½“å‰æ¨¡å‹ '{model}' å¯èƒ½æ˜¯:")
                        error_details.append("   â€¢ çº¯æ–‡æœ¬æ¨¡å‹ï¼ˆåªèƒ½ç”Ÿæˆæ–‡æœ¬ï¼‰")
                        error_details.append("   â€¢ è§†è§‰ç†è§£æ¨¡å‹ï¼ˆåªèƒ½ç†è§£å›¾åƒï¼Œä¸èƒ½ç”Ÿæˆï¼‰")
                        error_details.append("\n   âœ… è§£å†³æ–¹æ¡ˆ:")
                        error_details.append("   â€¢ ç¡®è®¤è¯¥APIæ˜¯å¦æ”¯æŒå›¾åƒç”ŸæˆåŠŸèƒ½")
                        error_details.append("   â€¢ æŸ¥çœ‹APIæ–‡æ¡£ï¼Œç¡®è®¤æ­£ç¡®çš„æ¨¡å‹åç§°")
                        error_details.append("   â€¢ è”ç³»APIæä¾›å•†ç¡®è®¤æ¨¡å‹èƒ½åŠ›")

                    if "IMAGE" in mods:
                        error_details.append("\n2ï¸âƒ£ APIç«¯ç‚¹å¯èƒ½ä¸æ­£ç¡®")
                        error_details.append(f"   å½“å‰ç«¯ç‚¹: {endpoint}")
                        error_details.append("\n   âœ… è§£å†³æ–¹æ¡ˆ:")
                        error_details.append("   â€¢ æ£€æŸ¥APIæ–‡æ¡£ï¼Œå›¾åƒç”Ÿæˆå¯èƒ½éœ€è¦ä¸åŒçš„ç«¯ç‚¹")
                        error_details.append("   â€¢ ä¾‹å¦‚: /generateImage è€Œä¸æ˜¯ /generateContent")
                        error_details.append("   â€¢ å°è¯•åœ¨ extra_payload_json ä¸­æ·»åŠ ç‰¹æ®Šå‚æ•°")

                    error_details.append("\n3ï¸âƒ£ éœ€è¦ç‰¹æ®Šé…ç½®")
                    error_details.append("   âœ… è§£å†³æ–¹æ¡ˆ:")
                    error_details.append("   â€¢ æŸ¥çœ‹APIæ–‡æ¡£ä¸­çš„å›¾åƒç”Ÿæˆç¤ºä¾‹")
                    error_details.append("   â€¢ å¯èƒ½éœ€è¦åœ¨ extra_payload_json ä¸­æ·»åŠ :")
                    error_details.append('     {"imageGenerationConfig": {...}}')
                    error_details.append("   â€¢ æˆ–å…¶ä»–ç‰¹å®šçš„é…ç½®å‚æ•°")

                    error_details.append("\n4ï¸âƒ£ Promptæ ¼å¼é—®é¢˜")
                    error_details.append("   âœ… è§£å†³æ–¹æ¡ˆ:")
                    error_details.append("   â€¢ æŸäº›APIéœ€è¦ç‰¹å®šçš„promptæ ¼å¼")
                    error_details.append("   â€¢ å°è¯•: 'ç”Ÿæˆä¸€å¼ [æè¿°]çš„å›¾ç‰‡'")
                    error_details.append("   â€¢ æˆ–: 'Generate an image of [description]'")

                    error_details.append("\n" + "=" * 60)
                    error_details.append("ğŸ“š å»ºè®®æ“ä½œ:")
                    error_details.append("=" * 60)
                    error_details.append("1. æŸ¥çœ‹ API æ–‡æ¡£ç¡®è®¤å›¾åƒç”Ÿæˆçš„æ­£ç¡®ç”¨æ³•")
                    error_details.append("2. ç¡®è®¤æ¨¡å‹åç§°å’Œç«¯ç‚¹æ˜¯å¦æ­£ç¡®")
                    error_details.append("3. æŸ¥çœ‹æ˜¯å¦éœ€è¦ç‰¹æ®Šçš„ extra_payload_json é…ç½®")
                    error_details.append("4. è”ç³» API æä¾›å•†è·å–å›¾åƒç”Ÿæˆç¤ºä¾‹")

            error_msg = "\n".join(error_details) if error_details else f"é”™è¯¯: å“åº”ä¸­æœªæ‰¾åˆ°å›¾åƒã€‚å“åº”å…¨æ–‡: {resp.text}"
            return (text or error_msg, torch.zeros(1, 512, 512, 3))

        except requests.exceptions.SSLError as e:
            error_msg = f"SSLè¿æ¥é”™è¯¯: {e}"
            _log(error_msg)

            # æä¾›è¯Šæ–­å»ºè®®
            suggestions = []
            if not use_system_proxy:
                suggestions.append("ğŸ’¡ å½“å‰å·²ç¦ç”¨ä»£ç†ï¼Œå¦‚æœAPIéœ€è¦ä»£ç†è®¿é—®ï¼Œè¯·å¯ç”¨ use_system_proxy")
            else:
                suggestions.append("ğŸ’¡ å½“å‰ä½¿ç”¨ç³»ç»Ÿä»£ç†ï¼Œå¦‚æœä»£ç†æœ‰é—®é¢˜ï¼Œå¯ä»¥å°è¯•ç¦ç”¨ use_system_proxy")

            suggestions.append("ğŸ’¡ æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
            suggestions.append(f"ğŸ’¡ ç¡®è®¤APIåœ°å€æ˜¯å¦æ­£ç¡®: {base_url}")
            suggestions.append("ğŸ’¡ å¦‚æœä½¿ç”¨è‡ªç­¾åè¯ä¹¦ï¼Œå¯èƒ½éœ€è¦é…ç½®SSLéªŒè¯")

            full_msg = f"{error_msg}\n\nè¯Šæ–­å»ºè®®:\n" + "\n".join(suggestions)
            return (full_msg, torch.zeros(1, 512, 512, 3))

        except requests.exceptions.ProxyError as e:
            error_msg = f"ä»£ç†è¿æ¥é”™è¯¯: {e}"
            _log(error_msg)

            suggestions = []
            if use_system_proxy:
                import os
                http_proxy = os.environ.get('HTTP_PROXY') or os.environ.get('http_proxy')
                https_proxy = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
                if http_proxy or https_proxy:
                    suggestions.append(f"ğŸ’¡ æ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†: HTTP={http_proxy}, HTTPS={https_proxy}")
                    suggestions.append("ğŸ’¡ è¯·ç¡®è®¤ä»£ç†æœåŠ¡å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ")
                else:
                    suggestions.append("ğŸ’¡ æœªæ£€æµ‹åˆ°ç³»ç»Ÿä»£ç†ç¯å¢ƒå˜é‡ï¼Œä½†å¯èƒ½é€šè¿‡å…¶ä»–æ–¹å¼é…ç½®äº†ä»£ç†")
                suggestions.append("ğŸ’¡ å°è¯•ç¦ç”¨ use_system_proxy æµ‹è¯•ç›´è¿")
            else:
                suggestions.append("ğŸ’¡ å½“å‰å·²ç¦ç”¨ä»£ç†ï¼Œä½†ä»ç„¶å‡ºç°ä»£ç†é”™è¯¯ï¼ˆå¯èƒ½æ˜¯ç³»ç»Ÿå¼ºåˆ¶ä»£ç†ï¼‰")

            full_msg = f"{error_msg}\n\nè¯Šæ–­å»ºè®®:\n" + "\n".join(suggestions)
            return (full_msg, torch.zeros(1, 512, 512, 3))

        except requests.exceptions.Timeout as e:
            error_msg = f"è¯·æ±‚è¶…æ—¶ (timeout={timeout}s): {e}"
            _log(error_msg)
            suggestions = [
                f"ğŸ’¡ å½“å‰è¶…æ—¶è®¾ç½®: {timeout}ç§’",
                "ğŸ’¡ å¯ä»¥å°è¯•å¢åŠ  timeout å‚æ•°",
                "ğŸ’¡ æ£€æŸ¥ç½‘ç»œè¿æ¥é€Ÿåº¦",
            ]
            full_msg = f"{error_msg}\n\nè¯Šæ–­å»ºè®®:\n" + "\n".join(suggestions)
            return (full_msg, torch.zeros(1, 512, 512, 3))

        except requests.exceptions.ConnectionError as e:
            error_msg = f"ç½‘ç»œè¿æ¥é”™è¯¯: {e}"
            _log(error_msg)

            suggestions = []
            if not use_system_proxy:
                suggestions.append("ğŸ’¡ å½“å‰ç¦ç”¨ä»£ç†ï¼Œç›´è¿å¤±è´¥")
                suggestions.append("ğŸ’¡ å¦‚æœAPIéœ€è¦ä»£ç†è®¿é—®ï¼Œè¯·å¯ç”¨ use_system_proxy")
                suggestions.append("ğŸ’¡ æ£€æŸ¥é˜²ç«å¢™è®¾ç½®")
            else:
                suggestions.append("ğŸ’¡ å½“å‰ä½¿ç”¨ç³»ç»Ÿä»£ç†")
                suggestions.append("ğŸ’¡ æ£€æŸ¥ä»£ç†æœåŠ¡å™¨æ˜¯å¦æ­£å¸¸")
                suggestions.append("ğŸ’¡ å°è¯•ç¦ç”¨ use_system_proxy æµ‹è¯•ç›´è¿")

            suggestions.append(f"ğŸ’¡ ç¡®è®¤APIåœ°å€æ˜¯å¦å¯è®¿é—®: {base_url}")

            full_msg = f"{error_msg}\n\nè¯Šæ–­å»ºè®®:\n" + "\n".join(suggestions)
            return (full_msg, torch.zeros(1, 512, 512, 3))

        except Exception as e:
            error_msg = f"è¯·æ±‚å¤±è´¥: {e}"
            _log(error_msg)
            import traceback
            _log(traceback.format_exc())
            return (error_msg, torch.zeros(1, 512, 512, 3))

NODE_CLASS_MAPPINGS = {
    "NanoBananaGeneralAPI": NanoBananaGeneralAPINode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "NanoBananaGeneralAPI": "NanoBanana-GeneralAPI",
}

